Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/h_ghazik/python_venv/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:03, 12.39s/it]  5%|▌         | 2/40 [00:23<07:23, 11.67s/it]  8%|▊         | 3/40 [00:35<07:12, 11.68s/it] 10%|█         | 4/40 [00:47<07:07, 11.88s/it] 12%|█▎        | 5/40 [00:59<06:58, 11.95s/it] 15%|█▌        | 6/40 [01:10<06:37, 11.69s/it] 18%|█▊        | 7/40 [01:22<06:21, 11.57s/it] 20%|██        | 8/40 [01:33<06:12, 11.63s/it] 22%|██▎       | 9/40 [01:54<07:26, 14.41s/it] 25%|██▌       | 10/40 [02:06<06:53, 13.78s/it] 28%|██▊       | 11/40 [02:18<06:21, 13.14s/it] 30%|███       | 12/40 [02:29<05:50, 12.50s/it] 32%|███▎      | 13/40 [02:41<05:32, 12.33s/it] 35%|███▌      | 14/40 [02:52<05:11, 11.97s/it] 38%|███▊      | 15/40 [03:03<04:51, 11.67s/it] 40%|████      | 16/40 [03:15<04:39, 11.64s/it] 42%|████▎     | 17/40 [03:35<05:30, 14.37s/it] 45%|████▌     | 18/40 [03:47<05:01, 13.71s/it] 48%|████▊     | 19/40 [03:59<04:32, 12.96s/it] 50%|█████     | 20/40 [04:10<04:12, 12.63s/it] 52%|█████▎    | 21/40 [04:22<03:52, 12.22s/it] 55%|█████▌    | 22/40 [04:34<03:37, 12.10s/it] 57%|█████▊    | 23/40 [04:45<03:19, 11.76s/it] 60%|██████    | 24/40 [04:56<03:04, 11.56s/it] 62%|██████▎   | 25/40 [05:15<03:30, 14.05s/it] 65%|██████▌   | 26/40 [05:27<03:04, 13.19s/it] 68%|██████▊   | 27/40 [05:39<02:46, 12.79s/it] 70%|███████   | 28/40 [05:49<02:26, 12.23s/it] 72%|███████▎  | 29/40 [06:01<02:12, 12.04s/it] 75%|███████▌  | 30/40 [06:13<01:59, 11.95s/it] 78%|███████▊  | 31/40 [06:24<01:44, 11.58s/it] 80%|████████  | 32/40 [06:36<01:34, 11.75s/it] 82%|████████▎ | 33/40 [06:55<01:39, 14.17s/it] 85%|████████▌ | 34/40 [07:07<01:21, 13.50s/it] 88%|████████▊ | 35/40 [07:19<01:04, 12.86s/it] 90%|█████████ | 36/40 [07:30<00:49, 12.44s/it] 92%|█████████▎| 37/40 [07:42<00:36, 12.22s/it] 95%|█████████▌| 38/40 [07:54<00:24, 12.05s/it] 98%|█████████▊| 39/40 [08:05<00:11, 11.76s/it]100%|██████████| 40/40 [08:16<00:00, 11.60s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:16<00:00, 11.60s/it]100%|██████████| 40/40 [08:16<00:00, 12.41s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 496.3943, 'train_samples_per_second': 5.651, 'train_steps_per_second': 0.081, 'train_loss': 0.7455397605895996, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:25, 11.44s/it]  5%|▌         | 2/40 [00:22<07:10, 11.33s/it]  8%|▊         | 3/40 [00:34<07:06, 11.52s/it] 10%|█         | 4/40 [00:46<07:04, 11.79s/it] 12%|█▎        | 5/40 [00:58<06:57, 11.93s/it] 15%|█▌        | 6/40 [01:10<06:43, 11.87s/it] 18%|█▊        | 7/40 [01:21<06:25, 11.69s/it] 20%|██        | 8/40 [01:33<06:15, 11.73s/it] 22%|██▎       | 9/40 [01:53<07:26, 14.39s/it] 25%|██▌       | 10/40 [02:06<06:55, 13.86s/it] 28%|██▊       | 11/40 [02:18<06:23, 13.22s/it] 30%|███       | 12/40 [02:29<05:52, 12.59s/it] 32%|███▎      | 13/40 [02:41<05:35, 12.42s/it] 35%|███▌      | 14/40 [02:52<05:14, 12.10s/it] 38%|███▊      | 15/40 [03:04<04:54, 11.80s/it] 40%|████      | 16/40 [03:15<04:43, 11.82s/it] 42%|████▎     | 17/40 [03:36<05:34, 14.55s/it] 45%|████▌     | 18/40 [03:48<05:03, 13.82s/it] 48%|████▊     | 19/40 [04:00<04:34, 13.06s/it] 50%|█████     | 20/40 [04:12<04:14, 12.73s/it] 52%|█████▎    | 21/40 [04:23<03:53, 12.30s/it] 55%|█████▌    | 22/40 [04:35<03:39, 12.19s/it] 57%|█████▊    | 23/40 [04:46<03:20, 11.80s/it] 60%|██████    | 24/40 [04:58<03:08, 11.81s/it] 62%|██████▎   | 25/40 [05:18<03:37, 14.47s/it] 65%|██████▌   | 26/40 [05:30<03:09, 13.54s/it] 68%|██████▊   | 27/40 [05:42<02:50, 13.11s/it] 70%|███████   | 28/40 [05:53<02:32, 12.68s/it] 72%|███████▎  | 29/40 [06:05<02:17, 12.49s/it] 75%|███████▌  | 30/40 [06:17<02:03, 12.31s/it] 78%|███████▊  | 31/40 [06:28<01:46, 11.87s/it] 80%|████████  | 32/40 [06:40<01:35, 11.99s/it] 82%|████████▎ | 33/40 [07:01<01:40, 14.42s/it] 85%|████████▌ | 34/40 [07:13<01:22, 13.70s/it] 88%|████████▊ | 35/40 [07:24<01:04, 12.97s/it] 90%|█████████ | 36/40 [07:35<00:49, 12.41s/it] 92%|█████████▎| 37/40 [07:47<00:36, 12.25s/it] 95%|█████████▌| 38/40 [07:59<00:24, 12.10s/it] 98%|█████████▊| 39/40 [08:10<00:11, 11.85s/it]100%|██████████| 40/40 [08:21<00:00, 11.68s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:21<00:00, 11.68s/it]100%|██████████| 40/40 [08:21<00:00, 12.54s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 501.6312, 'train_samples_per_second': 5.592, 'train_steps_per_second': 0.08, 'train_loss': 0.7508116722106933, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:16<10:51, 16.71s/it]  5%|▌         | 2/40 [00:32<10:18, 16.29s/it]  8%|▊         | 3/40 [00:49<10:14, 16.60s/it] 10%|█         | 4/40 [01:07<10:17, 17.16s/it] 12%|█▎        | 5/40 [01:25<10:08, 17.40s/it] 15%|█▌        | 6/40 [01:42<09:52, 17.41s/it] 18%|█▊        | 7/40 [01:59<09:25, 17.15s/it] 20%|██        | 8/40 [02:16<09:10, 17.20s/it] 22%|██▎       | 9/40 [02:45<10:46, 20.85s/it] 25%|██▌       | 10/40 [03:04<10:08, 20.29s/it] 28%|██▊       | 11/40 [03:21<09:18, 19.27s/it] 30%|███       | 12/40 [03:37<08:32, 18.30s/it] 32%|███▎      | 13/40 [03:55<08:06, 18.02s/it] 35%|███▌      | 14/40 [04:11<07:37, 17.61s/it] 38%|███▊      | 15/40 [04:27<07:08, 17.13s/it] 40%|████      | 16/40 [04:45<06:53, 17.24s/it] 42%|████▎     | 17/40 [05:15<08:08, 21.24s/it] 45%|████▌     | 18/40 [05:33<07:25, 20.25s/it] 48%|████▊     | 19/40 [05:50<06:43, 19.22s/it] 50%|█████     | 20/40 [06:08<06:13, 18.67s/it] 52%|█████▎    | 21/40 [06:24<05:41, 17.98s/it] 55%|█████▌    | 22/40 [06:41<05:18, 17.72s/it] 57%|█████▊    | 23/40 [06:57<04:53, 17.25s/it] 60%|██████    | 24/40 [07:14<04:35, 17.24s/it] 62%|██████▎   | 25/40 [07:45<05:16, 21.13s/it] 65%|██████▌   | 26/40 [08:01<04:36, 19.78s/it] 68%|██████▊   | 27/40 [08:19<04:08, 19.15s/it] 70%|███████   | 28/40 [08:35<03:39, 18.29s/it] 72%|███████▎  | 29/40 [08:53<03:18, 18.08s/it] 75%|███████▌  | 30/40 [09:10<02:59, 17.94s/it] 78%|███████▊  | 31/40 [09:25<02:33, 17.04s/it] 80%|████████  | 32/40 [09:43<02:18, 17.34s/it] 82%|████████▎ | 33/40 [10:12<02:25, 20.73s/it] 85%|████████▌ | 34/40 [10:30<01:59, 19.92s/it] 88%|████████▊ | 35/40 [10:47<01:34, 18.91s/it] 90%|█████████ | 36/40 [11:03<01:13, 18.25s/it] 92%|█████████▎| 37/40 [11:21<00:54, 18.06s/it] 95%|█████████▌| 38/40 [11:38<00:35, 17.74s/it] 98%|█████████▊| 39/40 [11:55<00:17, 17.40s/it]100%|██████████| 40/40 [12:11<00:00, 17.07s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:11<00:00, 17.07s/it]100%|██████████| 40/40 [12:11<00:00, 18.28s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 731.3542, 'train_samples_per_second': 3.835, 'train_steps_per_second': 0.055, 'train_loss': 0.7545728206634521, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:37, 17.90s/it]  5%|▌         | 2/40 [00:35<11:06, 17.54s/it]  8%|▊         | 3/40 [00:53<10:58, 17.79s/it] 10%|█         | 4/40 [01:12<10:58, 18.30s/it] 12%|█▎        | 5/40 [01:31<10:46, 18.47s/it] 15%|█▌        | 6/40 [01:49<10:28, 18.50s/it] 18%|█▊        | 7/40 [02:07<10:01, 18.24s/it] 20%|██        | 8/40 [02:25<09:45, 18.29s/it] 22%|██▎       | 9/40 [02:57<11:32, 22.35s/it] 25%|██▌       | 10/40 [03:17<10:48, 21.61s/it] 28%|██▊       | 11/40 [03:34<09:52, 20.42s/it] 30%|███       | 12/40 [03:52<09:06, 19.51s/it] 32%|███▎      | 13/40 [04:10<08:39, 19.24s/it] 35%|███▌      | 14/40 [04:27<08:00, 18.49s/it] 38%|███▊      | 15/40 [04:44<07:32, 18.10s/it] 40%|████      | 16/40 [05:03<07:18, 18.26s/it] 42%|████▎     | 17/40 [05:35<08:32, 22.29s/it] 45%|████▌     | 18/40 [05:54<07:49, 21.32s/it] 48%|████▊     | 19/40 [06:11<07:04, 20.22s/it] 50%|█████     | 20/40 [06:30<06:33, 19.67s/it] 52%|█████▎    | 21/40 [06:47<06:01, 19.05s/it] 55%|█████▌    | 22/40 [07:06<05:39, 18.87s/it] 57%|█████▊    | 23/40 [07:23<05:12, 18.37s/it] 60%|██████    | 24/40 [07:41<04:53, 18.35s/it] 62%|██████▎   | 25/40 [08:13<05:36, 22.41s/it] 65%|██████▌   | 26/40 [08:30<04:51, 20.83s/it] 68%|██████▊   | 27/40 [08:49<04:22, 20.21s/it] 70%|███████   | 28/40 [09:06<03:52, 19.38s/it] 72%|███████▎  | 29/40 [09:25<03:30, 19.12s/it] 75%|███████▌  | 30/40 [09:44<03:09, 18.97s/it] 78%|███████▊  | 31/40 [10:00<02:44, 18.32s/it] 80%|████████  | 32/40 [10:19<02:27, 18.48s/it] 82%|████████▎ | 33/40 [10:50<02:35, 22.15s/it] 85%|████████▌ | 34/40 [11:09<02:07, 21.18s/it] 88%|████████▊ | 35/40 [11:27<01:40, 20.20s/it] 90%|█████████ | 36/40 [11:45<01:18, 19.53s/it] 92%|█████████▎| 37/40 [12:03<00:57, 19.28s/it] 95%|█████████▌| 38/40 [12:21<00:37, 18.84s/it] 98%|█████████▊| 39/40 [12:38<00:18, 18.32s/it]100%|██████████| 40/40 [12:56<00:00, 17.99s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:56<00:00, 17.99s/it]100%|██████████| 40/40 [12:56<00:00, 19.40s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 776.0914, 'train_samples_per_second': 3.614, 'train_steps_per_second': 0.052, 'train_loss': 0.7799534797668457, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:23, 11.37s/it]  5%|▌         | 2/40 [00:22<07:15, 11.46s/it]  8%|▊         | 3/40 [00:34<07:11, 11.67s/it] 10%|█         | 4/40 [00:46<06:58, 11.61s/it] 12%|█▎        | 5/40 [00:59<07:02, 12.07s/it] 15%|█▌        | 6/40 [01:11<06:48, 12.01s/it] 18%|█▊        | 7/40 [01:22<06:31, 11.85s/it] 20%|██        | 8/40 [01:34<06:23, 11.99s/it] 22%|██▎       | 9/40 [01:54<07:29, 14.49s/it] 25%|██▌       | 10/40 [02:06<06:51, 13.73s/it] 28%|██▊       | 11/40 [02:18<06:18, 13.05s/it] 30%|███       | 12/40 [02:29<05:52, 12.59s/it] 32%|███▎      | 13/40 [02:41<05:33, 12.36s/it] 35%|███▌      | 14/40 [02:54<05:20, 12.34s/it] 38%|███▊      | 15/40 [03:05<04:58, 11.94s/it] 40%|████      | 16/40 [03:17<04:49, 12.05s/it] 42%|████▎     | 17/40 [03:38<05:40, 14.81s/it] 45%|████▌     | 18/40 [03:50<05:03, 13.81s/it] 48%|████▊     | 19/40 [04:01<04:36, 13.18s/it] 50%|█████     | 20/40 [04:13<04:16, 12.81s/it] 52%|█████▎    | 21/40 [04:25<03:57, 12.52s/it] 55%|█████▌    | 22/40 [04:37<03:41, 12.32s/it] 57%|█████▊    | 23/40 [04:49<03:25, 12.07s/it] 60%|██████    | 24/40 [05:00<03:09, 11.83s/it] 62%|██████▎   | 25/40 [05:22<03:42, 14.82s/it] 65%|██████▌   | 26/40 [05:33<03:13, 13.81s/it] 68%|██████▊   | 27/40 [05:44<02:49, 13.01s/it] 70%|███████   | 28/40 [05:56<02:32, 12.73s/it] 72%|███████▎  | 29/40 [06:08<02:16, 12.42s/it] 75%|███████▌  | 30/40 [06:20<02:02, 12.30s/it] 78%|███████▊  | 31/40 [06:31<01:46, 11.84s/it] 80%|████████  | 32/40 [06:42<01:33, 11.75s/it] 82%|████████▎ | 33/40 [07:02<01:39, 14.22s/it] 85%|████████▌ | 34/40 [07:15<01:22, 13.71s/it] 88%|████████▊ | 35/40 [07:26<01:05, 13.09s/it] 90%|█████████ | 36/40 [07:38<00:50, 12.66s/it] 92%|█████████▎| 37/40 [07:50<00:37, 12.35s/it] 95%|█████████▌| 38/40 [08:01<00:23, 11.99s/it] 98%|█████████▊| 39/40 [08:13<00:11, 11.97s/it]100%|██████████| 40/40 [08:24<00:00, 11.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:24<00:00, 11.82s/it]100%|██████████| 40/40 [08:24<00:00, 12.62s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 504.7102, 'train_samples_per_second': 5.558, 'train_steps_per_second': 0.079, 'train_loss': 0.7459011554718018, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:23, 11.36s/it]  5%|▌         | 2/40 [00:22<07:08, 11.27s/it]  8%|▊         | 3/40 [00:34<07:05, 11.51s/it] 10%|█         | 4/40 [00:45<06:54, 11.52s/it] 12%|█▎        | 5/40 [00:58<07:00, 12.01s/it] 15%|█▌        | 6/40 [01:10<06:40, 11.77s/it] 18%|█▊        | 7/40 [01:20<06:10, 11.23s/it] 20%|██        | 8/40 [01:30<05:54, 11.08s/it] 22%|██▎       | 9/40 [01:49<06:55, 13.42s/it] 25%|██▌       | 10/40 [02:01<06:29, 12.98s/it] 28%|██▊       | 11/40 [02:13<06:03, 12.53s/it] 30%|███       | 12/40 [02:24<05:41, 12.19s/it] 32%|███▎      | 13/40 [02:36<05:26, 12.09s/it] 35%|███▌      | 14/40 [02:48<05:13, 12.04s/it] 38%|███▊      | 15/40 [02:59<04:55, 11.82s/it] 40%|████      | 16/40 [03:11<04:46, 11.94s/it] 42%|████▎     | 17/40 [03:33<05:40, 14.79s/it] 45%|████▌     | 18/40 [03:44<05:03, 13.79s/it] 48%|████▊     | 19/40 [03:56<04:35, 13.13s/it] 50%|█████     | 20/40 [04:07<04:14, 12.71s/it] 52%|█████▎    | 21/40 [04:19<03:55, 12.37s/it] 55%|█████▌    | 22/40 [04:31<03:42, 12.35s/it] 57%|█████▊    | 23/40 [04:43<03:26, 12.14s/it] 60%|██████    | 24/40 [04:54<03:10, 11.91s/it] 62%|██████▎   | 25/40 [05:15<03:38, 14.57s/it] 65%|██████▌   | 26/40 [05:26<03:09, 13.57s/it] 68%|██████▊   | 27/40 [05:37<02:45, 12.70s/it] 70%|███████   | 28/40 [05:49<02:28, 12.38s/it] 72%|███████▎  | 29/40 [06:00<02:12, 12.06s/it] 75%|███████▌  | 30/40 [06:11<01:58, 11.86s/it] 78%|███████▊  | 31/40 [06:22<01:43, 11.55s/it] 80%|████████  | 32/40 [06:34<01:33, 11.65s/it] 82%|████████▎ | 33/40 [06:54<01:39, 14.16s/it] 85%|████████▌ | 34/40 [07:07<01:21, 13.64s/it] 88%|████████▊ | 35/40 [07:18<01:05, 13.11s/it] 90%|█████████ | 36/40 [07:30<00:50, 12.72s/it] 92%|█████████▎| 37/40 [07:42<00:37, 12.39s/it] 95%|█████████▌| 38/40 [07:53<00:23, 11.95s/it] 98%|█████████▊| 39/40 [08:04<00:11, 11.75s/it]100%|██████████| 40/40 [08:16<00:00, 11.72s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:16<00:00, 11.72s/it]100%|██████████| 40/40 [08:16<00:00, 12.40s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 496.1879, 'train_samples_per_second': 5.653, 'train_steps_per_second': 0.081, 'train_loss': 0.7504490852355957, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:16<10:39, 16.40s/it]  5%|▌         | 2/40 [00:32<10:27, 16.50s/it]  8%|▊         | 3/40 [00:50<10:25, 16.91s/it] 10%|█         | 4/40 [01:07<10:10, 16.96s/it] 12%|█▎        | 5/40 [01:26<10:22, 17.78s/it] 15%|█▌        | 6/40 [01:44<10:00, 17.66s/it] 18%|█▊        | 7/40 [02:00<09:33, 17.38s/it] 20%|██        | 8/40 [02:17<09:12, 17.27s/it] 22%|██▎       | 9/40 [02:46<10:44, 20.80s/it] 25%|██▌       | 10/40 [03:04<09:54, 19.82s/it] 28%|██▊       | 11/40 [03:20<09:03, 18.73s/it] 30%|███       | 12/40 [03:37<08:27, 18.13s/it] 32%|███▎      | 13/40 [03:54<08:03, 17.91s/it] 35%|███▌      | 14/40 [04:12<07:49, 18.04s/it] 38%|███▊      | 15/40 [04:29<07:18, 17.53s/it] 40%|████      | 16/40 [04:47<07:05, 17.75s/it] 42%|████▎     | 17/40 [05:18<08:21, 21.81s/it] 45%|████▌     | 18/40 [05:35<07:26, 20.30s/it] 48%|████▊     | 19/40 [05:52<06:45, 19.32s/it] 50%|█████     | 20/40 [06:09<06:13, 18.70s/it] 52%|█████▎    | 21/40 [06:27<05:47, 18.31s/it] 55%|█████▌    | 22/40 [06:45<05:27, 18.21s/it] 57%|█████▊    | 23/40 [07:02<05:03, 17.85s/it] 60%|██████    | 24/40 [07:18<04:36, 17.30s/it] 62%|██████▎   | 25/40 [07:50<05:25, 21.67s/it] 65%|██████▌   | 26/40 [08:07<04:44, 20.36s/it] 68%|██████▊   | 27/40 [08:23<04:07, 19.05s/it] 70%|███████   | 28/40 [08:41<03:43, 18.65s/it] 72%|███████▎  | 29/40 [08:58<03:20, 18.27s/it] 75%|███████▌  | 30/40 [09:16<03:01, 18.11s/it] 78%|███████▊  | 31/40 [09:32<02:37, 17.49s/it] 80%|████████  | 32/40 [09:49<02:20, 17.52s/it] 82%|████████▎ | 33/40 [10:18<02:26, 20.98s/it] 85%|████████▌ | 34/40 [10:37<02:02, 20.38s/it] 88%|████████▊ | 35/40 [10:54<01:36, 19.31s/it] 90%|█████████ | 36/40 [11:11<01:14, 18.55s/it] 92%|█████████▎| 37/40 [11:28<00:54, 18.12s/it] 95%|█████████▌| 38/40 [11:44<00:35, 17.53s/it] 98%|█████████▊| 39/40 [12:02<00:17, 17.47s/it]100%|██████████| 40/40 [12:18<00:00, 17.16s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:18<00:00, 17.16s/it]100%|██████████| 40/40 [12:18<00:00, 18.46s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 738.4891, 'train_samples_per_second': 3.798, 'train_steps_per_second': 0.054, 'train_loss': 0.7543065071105957, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:33, 17.77s/it]  5%|▌         | 2/40 [00:35<11:12, 17.71s/it]  8%|▊         | 3/40 [00:53<11:06, 18.00s/it] 10%|█         | 4/40 [01:11<10:47, 18.00s/it] 12%|█▎        | 5/40 [01:31<10:56, 18.76s/it] 15%|█▌        | 6/40 [01:50<10:33, 18.64s/it] 18%|█▊        | 7/40 [02:08<10:06, 18.37s/it] 20%|██        | 8/40 [02:26<09:49, 18.43s/it] 22%|██▎       | 9/40 [02:57<11:27, 22.18s/it] 25%|██▌       | 10/40 [03:15<10:30, 21.02s/it] 28%|██▊       | 11/40 [03:33<09:42, 20.09s/it] 30%|███       | 12/40 [03:51<09:00, 19.31s/it] 32%|███▎      | 13/40 [04:09<08:32, 18.97s/it] 35%|███▌      | 14/40 [04:28<08:15, 19.06s/it] 38%|███▊      | 15/40 [04:46<07:45, 18.61s/it] 40%|████      | 16/40 [05:05<07:30, 18.77s/it] 42%|████▎     | 17/40 [05:38<08:49, 23.01s/it] 45%|████▌     | 18/40 [05:56<07:53, 21.53s/it] 48%|████▊     | 19/40 [06:14<07:11, 20.56s/it] 50%|█████     | 20/40 [06:32<06:34, 19.71s/it] 52%|█████▎    | 21/40 [06:50<06:06, 19.31s/it] 55%|█████▌    | 22/40 [07:09<05:45, 19.20s/it] 57%|█████▊    | 23/40 [07:27<05:20, 18.86s/it] 60%|██████    | 24/40 [07:45<04:55, 18.47s/it] 62%|██████▎   | 25/40 [08:18<05:43, 22.91s/it] 65%|██████▌   | 26/40 [08:36<05:01, 21.53s/it] 68%|██████▊   | 27/40 [08:53<04:22, 20.22s/it] 70%|███████   | 28/40 [09:12<03:56, 19.73s/it] 72%|███████▎  | 29/40 [09:30<03:32, 19.29s/it] 75%|███████▌  | 30/40 [09:49<03:11, 19.12s/it] 78%|███████▊  | 31/40 [10:06<02:47, 18.61s/it] 80%|████████  | 32/40 [10:25<02:29, 18.68s/it] 82%|████████▎ | 33/40 [10:56<02:36, 22.30s/it] 85%|████████▌ | 34/40 [11:16<02:09, 21.56s/it] 88%|████████▊ | 35/40 [11:34<01:42, 20.46s/it] 90%|█████████ | 36/40 [11:51<01:18, 19.66s/it] 92%|█████████▎| 37/40 [12:09<00:57, 19.14s/it] 95%|█████████▌| 38/40 [12:26<00:37, 18.52s/it] 98%|█████████▊| 39/40 [12:45<00:18, 18.52s/it]100%|██████████| 40/40 [13:03<00:00, 18.43s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:03<00:00, 18.43s/it]100%|██████████| 40/40 [13:03<00:00, 19.59s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 783.6939, 'train_samples_per_second': 3.579, 'train_steps_per_second': 0.051, 'train_loss': 0.7796895980834961, 'epoch': 4.91}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:17, 12.76s/it]  5%|▌         | 2/40 [00:24<07:42, 12.18s/it]  8%|▊         | 3/40 [00:37<07:39, 12.43s/it] 10%|█         | 4/40 [00:50<07:34, 12.64s/it] 12%|█▎        | 5/40 [01:03<07:32, 12.92s/it] 15%|█▌        | 6/40 [01:15<07:06, 12.55s/it] 18%|█▊        | 7/40 [01:27<06:50, 12.43s/it] 20%|██        | 8/40 [01:39<06:26, 12.09s/it] 22%|██▎       | 9/40 [01:51<06:19, 12.25s/it] 25%|██▌       | 10/40 [02:02<05:57, 11.92s/it] 28%|██▊       | 11/40 [02:15<05:51, 12.11s/it] 30%|███       | 12/40 [02:28<05:44, 12.29s/it] 32%|███▎      | 13/40 [02:41<05:43, 12.71s/it] 35%|███▌      | 14/40 [02:54<05:28, 12.64s/it] 38%|███▊      | 15/40 [03:06<05:15, 12.61s/it] 40%|████      | 16/40 [03:18<04:57, 12.39s/it] 42%|████▎     | 17/40 [03:33<05:04, 13.26s/it] 45%|████▌     | 18/40 [03:46<04:50, 13.21s/it] 48%|████▊     | 19/40 [03:59<04:30, 12.89s/it] 50%|█████     | 20/40 [04:11<04:15, 12.80s/it] 52%|█████▎    | 21/40 [04:24<04:01, 12.73s/it] 55%|█████▌    | 22/40 [04:37<03:49, 12.74s/it] 57%|█████▊    | 23/40 [04:50<03:38, 12.85s/it] 60%|██████    | 24/40 [05:02<03:23, 12.73s/it] 62%|██████▎   | 25/40 [05:17<03:19, 13.30s/it] 65%|██████▌   | 26/40 [05:30<03:04, 13.15s/it] 68%|██████▊   | 27/40 [05:42<02:46, 12.80s/it] 70%|███████   | 28/40 [05:55<02:36, 13.00s/it] 72%|███████▎  | 29/40 [06:08<02:21, 12.89s/it] 75%|███████▌  | 30/40 [06:20<02:07, 12.70s/it] 78%|███████▊  | 31/40 [06:33<01:55, 12.87s/it] 80%|████████  | 32/40 [06:46<01:43, 12.89s/it] 82%|████████▎ | 33/40 [07:01<01:35, 13.59s/it] 85%|████████▌ | 34/40 [07:14<01:19, 13.32s/it] 88%|████████▊ | 35/40 [07:27<01:06, 13.33s/it] 90%|█████████ | 36/40 [07:40<00:52, 13.12s/it] 92%|█████████▎| 37/40 [07:53<00:39, 13.15s/it] 95%|█████████▌| 38/40 [08:06<00:26, 13.08s/it] 98%|█████████▊| 39/40 [08:19<00:12, 12.92s/it]100%|██████████| 40/40 [08:31<00:00, 12.75s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:31<00:00, 12.75s/it]100%|██████████| 40/40 [08:31<00:00, 12.79s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_iontransporters_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 511.5163, 'train_samples_per_second': 5.102, 'train_steps_per_second': 0.078, 'train_loss': 0.6997018814086914, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:20, 12.83s/it]  5%|▌         | 2/40 [00:24<07:51, 12.40s/it]  8%|▊         | 3/40 [00:37<07:46, 12.60s/it] 10%|█         | 4/40 [00:50<07:40, 12.78s/it] 12%|█▎        | 5/40 [01:04<07:36, 13.03s/it] 15%|█▌        | 6/40 [01:16<07:15, 12.82s/it] 18%|█▊        | 7/40 [01:29<07:05, 12.89s/it] 20%|██        | 8/40 [01:42<06:46, 12.69s/it] 22%|██▎       | 9/40 [01:55<06:45, 13.07s/it] 25%|██▌       | 10/40 [02:08<06:26, 12.89s/it] 28%|██▊       | 11/40 [02:21<06:13, 12.89s/it] 30%|███       | 12/40 [02:33<05:56, 12.74s/it] 32%|███▎      | 13/40 [02:47<05:49, 12.94s/it] 35%|███▌      | 14/40 [03:00<05:36, 12.94s/it] 38%|███▊      | 15/40 [03:13<05:26, 13.06s/it] 40%|████      | 16/40 [03:26<05:11, 12.99s/it] 42%|████▎     | 17/40 [03:41<05:14, 13.69s/it] 45%|████▌     | 18/40 [03:54<04:57, 13.52s/it] 48%|████▊     | 19/40 [04:06<04:35, 13.14s/it] 50%|█████     | 20/40 [04:19<04:19, 12.97s/it] 52%|█████▎    | 21/40 [04:32<04:04, 12.85s/it] 55%|█████▌    | 22/40 [04:44<03:50, 12.80s/it] 57%|█████▊    | 23/40 [04:57<03:37, 12.82s/it] 60%|██████    | 24/40 [05:09<03:22, 12.68s/it] 62%|██████▎   | 25/40 [05:24<03:18, 13.26s/it] 65%|██████▌   | 26/40 [05:37<03:03, 13.11s/it] 68%|██████▊   | 27/40 [05:49<02:45, 12.75s/it] 70%|███████   | 28/40 [06:02<02:35, 12.95s/it] 72%|███████▎  | 29/40 [06:15<02:21, 12.84s/it] 75%|███████▌  | 30/40 [06:27<02:06, 12.68s/it] 78%|███████▊  | 31/40 [06:40<01:56, 12.90s/it] 80%|████████  | 32/40 [06:53<01:43, 12.92s/it] 82%|████████▎ | 33/40 [07:09<01:35, 13.59s/it] 85%|████████▌ | 34/40 [07:21<01:19, 13.30s/it] 88%|████████▊ | 35/40 [07:35<01:06, 13.32s/it] 90%|█████████ | 36/40 [07:47<00:52, 13.02s/it] 92%|█████████▎| 37/40 [08:00<00:38, 12.97s/it] 95%|█████████▌| 38/40 [08:12<00:25, 12.83s/it] 98%|█████████▊| 39/40 [08:24<00:12, 12.62s/it]100%|██████████| 40/40 [08:36<00:00, 12.45s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:36<00:00, 12.45s/it]100%|██████████| 40/40 [08:36<00:00, 12.92s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_iontransporters_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 516.9139, 'train_samples_per_second': 5.049, 'train_steps_per_second': 0.077, 'train_loss': 0.7170424461364746, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<12:20, 18.99s/it]  5%|▌         | 2/40 [00:36<11:35, 18.30s/it]  8%|▊         | 3/40 [00:56<11:39, 18.90s/it] 10%|█         | 4/40 [01:16<11:35, 19.33s/it] 12%|█▎        | 5/40 [01:36<11:25, 19.59s/it] 15%|█▌        | 6/40 [01:54<10:52, 19.19s/it] 18%|█▊        | 7/40 [02:14<10:38, 19.34s/it] 20%|██        | 8/40 [02:32<10:09, 19.04s/it] 22%|██▎       | 9/40 [02:53<10:03, 19.48s/it] 25%|██▌       | 10/40 [03:11<09:34, 19.15s/it] 28%|██▊       | 11/40 [03:31<09:16, 19.19s/it] 30%|███       | 12/40 [03:49<08:54, 19.07s/it] 32%|███▎      | 13/40 [04:10<08:44, 19.41s/it] 35%|███▌      | 14/40 [04:29<08:24, 19.40s/it] 38%|███▊      | 15/40 [04:49<08:11, 19.67s/it] 40%|████      | 16/40 [05:08<07:49, 19.55s/it] 42%|████▎     | 17/40 [05:31<07:53, 20.59s/it] 45%|████▌     | 18/40 [05:51<07:26, 20.29s/it] 48%|████▊     | 19/40 [06:10<06:54, 19.74s/it] 50%|█████     | 20/40 [06:28<06:29, 19.48s/it] 52%|█████▎    | 21/40 [06:48<06:08, 19.38s/it] 55%|█████▌    | 22/40 [07:06<05:46, 19.24s/it] 57%|█████▊    | 23/40 [07:26<05:30, 19.42s/it] 60%|██████    | 24/40 [07:45<05:06, 19.18s/it] 62%|██████▎   | 25/40 [08:07<04:59, 19.95s/it] 65%|██████▌   | 26/40 [08:26<04:36, 19.78s/it] 68%|██████▊   | 27/40 [08:44<04:10, 19.25s/it] 70%|███████   | 28/40 [09:04<03:54, 19.53s/it] 72%|███████▎  | 29/40 [09:23<03:33, 19.43s/it] 75%|███████▌  | 30/40 [09:42<03:11, 19.17s/it] 78%|███████▊  | 31/40 [10:02<02:54, 19.38s/it] 80%|████████  | 32/40 [10:21<02:35, 19.44s/it] 82%|████████▎ | 33/40 [10:44<02:23, 20.47s/it] 85%|████████▌ | 34/40 [11:03<01:59, 19.96s/it] 88%|████████▊ | 35/40 [11:24<01:40, 20.13s/it] 90%|█████████ | 36/40 [11:43<01:19, 19.80s/it] 92%|█████████▎| 37/40 [12:02<00:59, 19.80s/it] 95%|█████████▌| 38/40 [12:22<00:39, 19.62s/it] 98%|█████████▊| 39/40 [12:40<00:19, 19.34s/it]100%|██████████| 40/40 [12:59<00:00, 19.03s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:59<00:00, 19.03s/it]100%|██████████| 40/40 [12:59<00:00, 19.48s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_iontransporters_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 779.1664, 'train_samples_per_second': 3.35, 'train_steps_per_second': 0.051, 'train_loss': 0.6926359176635742, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:20<13:06, 20.16s/it]  5%|▌         | 2/40 [00:39<12:21, 19.51s/it]  8%|▊         | 3/40 [00:59<12:17, 19.94s/it] 10%|█         | 4/40 [01:20<12:11, 20.32s/it] 12%|█▎        | 5/40 [01:41<12:00, 20.59s/it] 15%|█▌        | 6/40 [02:00<11:25, 20.17s/it] 18%|█▊        | 7/40 [02:21<11:10, 20.31s/it] 20%|██        | 8/40 [02:40<10:39, 19.99s/it] 22%|██▎       | 9/40 [03:02<10:32, 20.39s/it] 25%|██▌       | 10/40 [03:21<10:01, 20.04s/it] 28%|██▊       | 11/40 [03:41<09:41, 20.04s/it] 30%|███       | 12/40 [04:01<09:19, 19.98s/it] 32%|███▎      | 13/40 [04:22<09:08, 20.33s/it] 35%|███▌      | 14/40 [04:42<08:49, 20.37s/it] 38%|███▊      | 15/40 [05:04<08:38, 20.74s/it] 40%|████      | 16/40 [05:24<08:14, 20.59s/it] 42%|████▎     | 17/40 [05:48<08:16, 21.60s/it] 45%|████▌     | 18/40 [06:09<07:48, 21.28s/it] 48%|████▊     | 19/40 [06:28<07:14, 20.67s/it] 50%|█████     | 20/40 [06:48<06:47, 20.37s/it] 52%|█████▎    | 21/40 [07:08<06:24, 20.25s/it] 55%|█████▌    | 22/40 [07:27<06:02, 20.13s/it] 57%|█████▊    | 23/40 [07:48<05:46, 20.35s/it] 60%|██████    | 24/40 [08:08<05:21, 20.09s/it] 62%|██████▎   | 25/40 [08:31<05:13, 20.89s/it] 65%|██████▌   | 26/40 [08:51<04:49, 20.66s/it] 68%|██████▊   | 27/40 [09:10<04:21, 20.10s/it] 70%|███████   | 28/40 [09:31<04:05, 20.45s/it] 72%|███████▎  | 29/40 [09:50<03:42, 20.19s/it] 75%|███████▌  | 30/40 [10:10<03:19, 19.99s/it] 78%|███████▊  | 31/40 [10:31<03:02, 20.30s/it] 80%|████████  | 32/40 [10:51<02:42, 20.35s/it] 82%|████████▎ | 33/40 [11:15<02:30, 21.44s/it] 85%|████████▌ | 34/40 [11:35<02:06, 21.02s/it] 88%|████████▊ | 35/40 [11:57<01:45, 21.13s/it] 90%|█████████ | 36/40 [12:17<01:23, 20.79s/it] 92%|█████████▎| 37/40 [12:38<01:02, 20.78s/it] 95%|█████████▌| 38/40 [12:58<00:41, 20.59s/it] 98%|█████████▊| 39/40 [13:17<00:20, 20.26s/it]100%|██████████| 40/40 [13:36<00:00, 19.98s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:36<00:00, 19.98s/it]100%|██████████| 40/40 [13:36<00:00, 20.42s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_iontransporters_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'train_runtime': 816.9771, 'train_samples_per_second': 3.195, 'train_steps_per_second': 0.049, 'train_loss': 0.749912166595459, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:11<54:17, 11.47s/it]  1%|          | 2/285 [00:22<53:16, 11.29s/it]  1%|          | 3/285 [00:33<51:06, 10.87s/it]  1%|▏         | 4/285 [00:43<49:43, 10.62s/it]  2%|▏         | 5/285 [00:53<48:51, 10.47s/it]  2%|▏         | 6/285 [01:03<48:18, 10.39s/it]  2%|▏         | 7/285 [01:13<47:19, 10.21s/it]  3%|▎         | 8/285 [01:24<47:47, 10.35s/it]  3%|▎         | 9/285 [01:35<48:23, 10.52s/it]  4%|▎         | 10/285 [01:46<49:50, 10.87s/it]  4%|▍         | 11/285 [01:57<50:03, 10.96s/it]  4%|▍         | 12/285 [02:10<51:39, 11.35s/it]  5%|▍         | 13/285 [02:21<51:25, 11.34s/it]  5%|▍         | 14/285 [02:32<50:55, 11.28s/it]  5%|▌         | 15/285 [02:42<49:14, 10.94s/it]  6%|▌         | 16/285 [02:53<48:30, 10.82s/it]  6%|▌         | 17/285 [03:04<49:22, 11.05s/it]  6%|▋         | 18/285 [03:16<50:06, 11.26s/it]  7%|▋         | 19/285 [03:28<51:09, 11.54s/it]  7%|▋         | 20/285 [03:40<51:05, 11.57s/it]  7%|▋         | 21/285 [03:52<50:55, 11.57s/it]  8%|▊         | 22/285 [04:03<50:53, 11.61s/it]  8%|▊         | 23/285 [04:14<49:50, 11.41s/it]  8%|▊         | 24/285 [04:26<49:36, 11.41s/it]  9%|▉         | 25/285 [04:37<49:17, 11.38s/it]  9%|▉         | 26/285 [04:48<49:20, 11.43s/it]  9%|▉         | 27/285 [05:00<49:02, 11.40s/it] 10%|▉         | 28/285 [05:11<48:54, 11.42s/it] 10%|█         | 29/285 [05:22<48:20, 11.33s/it] 11%|█         | 30/285 [05:34<48:06, 11.32s/it] 11%|█         | 31/285 [05:45<47:46, 11.29s/it] 11%|█         | 32/285 [05:57<48:33, 11.51s/it] 12%|█▏        | 33/285 [06:08<47:24, 11.29s/it] 12%|█▏        | 34/285 [06:18<46:23, 11.09s/it] 12%|█▏        | 35/285 [06:30<46:30, 11.16s/it] 13%|█▎        | 36/285 [06:41<46:11, 11.13s/it] 13%|█▎        | 37/285 [06:52<46:49, 11.33s/it] 13%|█▎        | 38/285 [07:03<46:04, 11.19s/it] 14%|█▎        | 39/285 [07:14<45:39, 11.14s/it] 14%|█▍        | 40/285 [07:25<45:19, 11.10s/it] 14%|█▍        | 41/285 [07:36<44:45, 11.01s/it] 15%|█▍        | 42/285 [07:47<44:58, 11.11s/it] 15%|█▌        | 43/285 [07:59<45:36, 11.31s/it] 15%|█▌        | 44/285 [08:10<44:55, 11.19s/it] 16%|█▌        | 45/285 [08:21<44:18, 11.08s/it] 16%|█▌        | 46/285 [08:34<46:00, 11.55s/it] 16%|█▋        | 47/285 [08:44<44:03, 11.11s/it] 17%|█▋        | 48/285 [08:55<43:51, 11.10s/it] 17%|█▋        | 49/285 [09:05<42:26, 10.79s/it] 18%|█▊        | 50/285 [09:16<42:49, 10.93s/it] 18%|█▊        | 51/285 [09:28<43:59, 11.28s/it] 18%|█▊        | 52/285 [09:39<43:37, 11.24s/it] 19%|█▊        | 53/285 [09:51<43:39, 11.29s/it] 19%|█▉        | 54/285 [10:02<43:41, 11.35s/it] 19%|█▉        | 55/285 [10:14<44:04, 11.50s/it] 20%|█▉        | 56/285 [10:25<43:38, 11.43s/it] 20%|██        | 57/285 [10:37<43:30, 11.45s/it] 20%|██        | 58/285 [10:57<53:41, 14.19s/it] 21%|██        | 59/285 [11:09<50:42, 13.46s/it] 21%|██        | 60/285 [11:21<48:37, 12.97s/it] 21%|██▏       | 61/285 [11:33<46:59, 12.59s/it] 22%|██▏       | 62/285 [11:43<44:41, 12.03s/it] 22%|██▏       | 63/285 [11:55<44:17, 11.97s/it] 22%|██▏       | 64/285 [12:07<44:10, 11.99s/it] 23%|██▎       | 65/285 [12:19<43:51, 11.96s/it] 23%|██▎       | 66/285 [12:30<42:45, 11.71s/it] 24%|██▎       | 67/285 [12:42<42:36, 11.73s/it] 24%|██▍       | 68/285 [12:54<42:40, 11.80s/it] 24%|██▍       | 69/285 [13:05<41:08, 11.43s/it] 25%|██▍       | 70/285 [13:16<40:28, 11.29s/it] 25%|██▍       | 71/285 [13:27<40:32, 11.37s/it] 25%|██▌       | 72/285 [13:39<40:22, 11.38s/it] 26%|██▌       | 73/285 [13:49<39:29, 11.18s/it] 26%|██▌       | 74/285 [14:00<38:59, 11.09s/it] 26%|██▋       | 75/285 [14:12<39:22, 11.25s/it] 27%|██▋       | 76/285 [14:24<39:46, 11.42s/it] 27%|██▋       | 77/285 [14:35<39:07, 11.29s/it] 27%|██▋       | 78/285 [14:46<38:55, 11.28s/it] 28%|██▊       | 79/285 [14:58<39:24, 11.48s/it] 28%|██▊       | 80/285 [15:09<39:24, 11.54s/it] 28%|██▊       | 81/285 [15:21<38:42, 11.39s/it] 29%|██▉       | 82/285 [15:32<38:30, 11.38s/it] 29%|██▉       | 83/285 [15:43<38:06, 11.32s/it] 29%|██▉       | 84/285 [15:55<38:06, 11.37s/it] 30%|██▉       | 85/285 [16:05<37:08, 11.14s/it] 30%|███       | 86/285 [16:17<37:26, 11.29s/it] 31%|███       | 87/285 [16:28<36:50, 11.16s/it] 31%|███       | 88/285 [16:39<37:06, 11.30s/it] 31%|███       | 89/285 [16:51<36:53, 11.29s/it] 32%|███▏      | 90/285 [17:02<37:08, 11.43s/it] 32%|███▏      | 91/285 [17:14<37:00, 11.45s/it] 32%|███▏      | 92/285 [17:24<36:01, 11.20s/it] 33%|███▎      | 93/285 [17:36<35:48, 11.19s/it] 33%|███▎      | 94/285 [17:47<35:51, 11.26s/it] 33%|███▎      | 95/285 [17:58<35:29, 11.21s/it] 34%|███▎      | 96/285 [18:09<35:07, 11.15s/it] 34%|███▍      | 97/285 [18:21<35:16, 11.26s/it] 34%|███▍      | 98/285 [18:32<35:05, 11.26s/it] 35%|███▍      | 99/285 [18:43<34:23, 11.09s/it] 35%|███▌      | 100/285 [18:54<34:22, 11.15s/it] 35%|███▌      | 101/285 [19:06<35:12, 11.48s/it] 36%|███▌      | 102/285 [19:17<34:37, 11.35s/it] 36%|███▌      | 103/285 [19:28<34:01, 11.22s/it] 36%|███▋      | 104/285 [19:39<33:55, 11.24s/it] 37%|███▋      | 105/285 [19:50<33:31, 11.18s/it] 37%|███▋      | 106/285 [20:03<34:15, 11.48s/it] 38%|███▊      | 107/285 [20:15<34:38, 11.67s/it] 38%|███▊      | 108/285 [20:27<34:43, 11.77s/it] 38%|███▊      | 109/285 [20:38<34:22, 11.72s/it] 39%|███▊      | 110/285 [20:50<34:05, 11.69s/it] 39%|███▉      | 111/285 [21:01<33:28, 11.54s/it] 39%|███▉      | 112/285 [21:13<33:24, 11.59s/it] 40%|███▉      | 113/285 [21:24<32:34, 11.36s/it] 40%|████      | 114/285 [21:36<32:55, 11.55s/it] 40%|████      | 115/285 [21:56<40:19, 14.24s/it] 41%|████      | 116/285 [22:08<37:41, 13.38s/it] 41%|████      | 117/285 [22:19<35:54, 12.82s/it] 41%|████▏     | 118/285 [22:30<34:12, 12.29s/it] 42%|████▏     | 119/285 [22:40<31:54, 11.53s/it] 42%|████▏     | 120/285 [22:50<30:39, 11.15s/it] 42%|████▏     | 121/285 [22:59<28:49, 10.54s/it] 43%|████▎     | 122/285 [23:08<27:32, 10.14s/it] 43%|████▎     | 123/285 [23:19<27:35, 10.22s/it] 44%|████▎     | 124/285 [23:29<27:42, 10.32s/it] 44%|████▍     | 125/285 [23:41<28:33, 10.71s/it] 44%|████▍     | 126/285 [23:53<29:13, 11.03s/it] 45%|████▍     | 127/285 [24:04<29:20, 11.14s/it] 45%|████▍     | 128/285 [24:16<29:22, 11.23s/it] 45%|████▌     | 129/285 [24:27<29:00, 11.16s/it] 46%|████▌     | 130/285 [24:38<28:44, 11.13s/it] 46%|████▌     | 131/285 [24:49<28:59, 11.30s/it] 46%|████▋     | 132/285 [25:01<28:51, 11.32s/it] 47%|████▋     | 133/285 [25:12<28:42, 11.33s/it] 47%|████▋     | 134/285 [25:24<28:44, 11.42s/it] 47%|████▋     | 135/285 [25:34<27:46, 11.11s/it] 48%|████▊     | 136/285 [25:45<27:07, 10.93s/it] 48%|████▊     | 137/285 [25:55<26:53, 10.90s/it] 48%|████▊     | 138/285 [26:07<27:01, 11.03s/it] 49%|████▉     | 139/285 [26:18<26:39, 10.95s/it] 49%|████▉     | 140/285 [26:29<26:35, 11.00s/it] 49%|████▉     | 141/285 [26:40<26:40, 11.12s/it] 50%|████▉     | 142/285 [26:51<26:23, 11.07s/it] 50%|█████     | 143/285 [27:01<25:43, 10.87s/it] 51%|█████     | 144/285 [27:13<26:10, 11.14s/it] 51%|█████     | 145/285 [27:25<26:34, 11.39s/it] 51%|█████     | 146/285 [27:36<26:09, 11.29s/it] 52%|█████▏    | 147/285 [27:47<25:31, 11.10s/it] 52%|█████▏    | 148/285 [27:58<25:19, 11.09s/it] 52%|█████▏    | 149/285 [28:10<25:39, 11.32s/it] 53%|█████▎    | 150/285 [28:21<25:43, 11.43s/it] 53%|█████▎    | 151/285 [28:32<24:54, 11.16s/it] 53%|█████▎    | 152/285 [28:43<24:32, 11.07s/it] 54%|█████▎    | 153/285 [28:54<24:17, 11.05s/it] 54%|█████▍    | 154/285 [29:06<24:34, 11.25s/it] 54%|█████▍    | 155/285 [29:17<24:37, 11.37s/it] 55%|█████▍    | 156/285 [29:29<24:31, 11.41s/it] 55%|█████▌    | 157/285 [29:40<24:22, 11.43s/it] 55%|█████▌    | 158/285 [29:52<24:11, 11.43s/it] 56%|█████▌    | 159/285 [30:04<24:19, 11.59s/it] 56%|█████▌    | 160/285 [30:14<23:35, 11.32s/it] 56%|█████▋    | 161/285 [30:26<23:51, 11.54s/it] 57%|█████▋    | 162/285 [30:38<23:25, 11.43s/it] 57%|█████▋    | 163/285 [30:49<23:03, 11.34s/it] 58%|█████▊    | 164/285 [30:59<22:27, 11.14s/it] 58%|█████▊    | 165/285 [31:12<23:14, 11.62s/it] 58%|█████▊    | 166/285 [31:23<22:46, 11.48s/it] 59%|█████▊    | 167/285 [31:35<22:33, 11.47s/it] 59%|█████▉    | 168/285 [31:46<22:05, 11.33s/it] 59%|█████▉    | 169/285 [31:57<21:41, 11.22s/it] 60%|█████▉    | 170/285 [32:08<21:31, 11.23s/it] 60%|██████    | 171/285 [32:19<21:18, 11.21s/it] 60%|██████    | 172/285 [32:37<25:06, 13.33s/it] 61%|██████    | 173/285 [32:48<23:30, 12.60s/it] 61%|██████    | 174/285 [33:00<22:41, 12.27s/it] 61%|██████▏   | 175/285 [33:11<21:51, 11.93s/it] 62%|██████▏   | 176/285 [33:22<21:03, 11.59s/it] 62%|██████▏   | 177/285 [33:33<20:47, 11.55s/it] 62%|██████▏   | 178/285 [33:44<20:16, 11.37s/it] 63%|██████▎   | 179/285 [33:55<19:59, 11.32s/it] 63%|██████▎   | 180/285 [34:06<19:36, 11.20s/it] 64%|██████▎   | 181/285 [34:19<20:00, 11.55s/it] 64%|██████▍   | 182/285 [34:30<19:42, 11.48s/it] 64%|██████▍   | 183/285 [34:41<19:24, 11.42s/it] 65%|██████▍   | 184/285 [34:52<18:58, 11.27s/it] 65%|██████▍   | 185/285 [35:04<19:19, 11.59s/it] 65%|██████▌   | 186/285 [35:16<19:06, 11.58s/it] 66%|██████▌   | 187/285 [35:27<18:47, 11.50s/it] 66%|██████▌   | 188/285 [35:39<18:30, 11.45s/it] 66%|██████▋   | 189/285 [35:50<18:29, 11.56s/it] 67%|██████▋   | 190/285 [36:01<17:49, 11.26s/it] 67%|██████▋   | 191/285 [36:13<17:46, 11.34s/it] 67%|██████▋   | 192/285 [36:24<17:35, 11.35s/it] 68%|██████▊   | 193/285 [36:35<17:15, 11.25s/it] 68%|██████▊   | 194/285 [36:46<17:00, 11.21s/it] 68%|██████▊   | 195/285 [36:58<17:09, 11.44s/it] 69%|██████▉   | 196/285 [37:09<16:39, 11.23s/it] 69%|██████▉   | 197/285 [37:20<16:35, 11.31s/it] 69%|██████▉   | 198/285 [37:31<16:22, 11.29s/it] 70%|██████▉   | 199/285 [37:42<16:02, 11.19s/it] 70%|███████   | 200/285 [37:54<16:13, 11.45s/it]                                                  70%|███████   | 200/285 [37:54<16:13, 11.45s/it] 71%|███████   | 201/285 [38:07<16:16, 11.62s/it] 71%|███████   | 202/285 [38:18<15:50, 11.45s/it] 71%|███████   | 203/285 [38:29<15:47, 11.55s/it] 72%|███████▏  | 204/285 [38:41<15:49, 11.72s/it] 72%|███████▏  | 205/285 [38:53<15:37, 11.72s/it] 72%|███████▏  | 206/285 [39:03<14:39, 11.13s/it] 73%|███████▎  | 207/285 [39:13<14:04, 10.83s/it] 73%|███████▎  | 208/285 [39:22<13:08, 10.24s/it] 73%|███████▎  | 209/285 [39:31<12:39, 10.00s/it] 74%|███████▎  | 210/285 [39:42<12:54, 10.33s/it] 74%|███████▍  | 211/285 [39:54<13:04, 10.59s/it] 74%|███████▍  | 212/285 [40:05<12:59, 10.67s/it] 75%|███████▍  | 213/285 [40:16<13:12, 11.00s/it] 75%|███████▌  | 214/285 [40:27<13:01, 11.00s/it] 75%|███████▌  | 215/285 [40:39<13:02, 11.18s/it] 76%|███████▌  | 216/285 [40:50<12:58, 11.28s/it] 76%|███████▌  | 217/285 [41:02<12:54, 11.39s/it] 76%|███████▋  | 218/285 [41:13<12:29, 11.18s/it] 77%|███████▋  | 219/285 [41:24<12:14, 11.13s/it] 77%|███████▋  | 220/285 [41:36<12:19, 11.38s/it] 78%|███████▊  | 221/285 [41:48<12:26, 11.66s/it] 78%|███████▊  | 222/285 [41:59<11:55, 11.36s/it] 78%|███████▊  | 223/285 [42:10<11:41, 11.31s/it] 79%|███████▊  | 224/285 [42:22<11:42, 11.52s/it] 79%|███████▉  | 225/285 [42:33<11:23, 11.39s/it] 79%|███████▉  | 226/285 [42:45<11:19, 11.52s/it] 80%|███████▉  | 227/285 [42:55<10:47, 11.16s/it] 80%|████████  | 228/285 [43:05<10:09, 10.69s/it] 80%|████████  | 229/285 [43:21<11:39, 12.48s/it] 81%|████████  | 230/285 [43:32<10:50, 11.82s/it] 81%|████████  | 231/285 [43:42<10:14, 11.38s/it] 81%|████████▏ | 232/285 [43:51<09:21, 10.60s/it] 82%|████████▏ | 233/285 [44:00<08:49, 10.19s/it] 82%|████████▏ | 234/285 [44:11<08:44, 10.29s/it] 82%|████████▏ | 235/285 [44:21<08:37, 10.34s/it] 83%|████████▎ | 236/285 [44:31<08:19, 10.19s/it] 83%|████████▎ | 237/285 [44:40<07:56,  9.93s/it] 84%|████████▎ | 238/285 [44:51<07:54, 10.09s/it] 84%|████████▍ | 239/285 [45:02<07:55, 10.35s/it] 84%|████████▍ | 240/285 [45:13<08:03, 10.74s/it] 85%|████████▍ | 241/285 [45:25<08:02, 10.97s/it] 85%|████████▍ | 242/285 [45:36<07:52, 10.99s/it] 85%|████████▌ | 243/285 [45:46<07:34, 10.82s/it] 86%|████████▌ | 244/285 [45:57<07:22, 10.78s/it] 86%|████████▌ | 245/285 [46:09<07:21, 11.04s/it] 86%|████████▋ | 246/285 [46:19<07:04, 10.89s/it] 87%|████████▋ | 247/285 [46:31<07:06, 11.22s/it] 87%|████████▋ | 248/285 [46:42<06:57, 11.27s/it] 87%|████████▋ | 249/285 [46:54<06:53, 11.48s/it] 88%|████████▊ | 250/285 [47:06<06:40, 11.45s/it] 88%|████████▊ | 251/285 [47:17<06:22, 11.25s/it] 88%|████████▊ | 252/285 [47:29<06:19, 11.49s/it] 89%|████████▉ | 253/285 [47:40<06:06, 11.45s/it] 89%|████████▉ | 254/285 [47:52<05:56, 11.50s/it] 89%|████████▉ | 255/285 [48:03<05:44, 11.47s/it] 90%|████████▉ | 256/285 [48:13<05:17, 10.94s/it] 90%|█████████ | 257/285 [48:22<04:54, 10.53s/it] 91%|█████████ | 258/285 [48:32<04:40, 10.39s/it] 91%|█████████ | 259/285 [48:43<04:33, 10.53s/it] 91%|█████████ | 260/285 [48:54<04:28, 10.74s/it] 92%|█████████▏| 261/285 [49:07<04:27, 11.13s/it] 92%|█████████▏| 262/285 [49:18<04:16, 11.16s/it] 92%|█████████▏| 263/285 [49:29<04:08, 11.32s/it] 93%|█████████▎| 264/285 [49:41<04:00, 11.43s/it] 93%|█████████▎| 265/285 [49:52<03:45, 11.30s/it] 93%|█████████▎| 266/285 [50:04<03:35, 11.35s/it] 94%|█████████▎| 267/285 [50:15<03:25, 11.44s/it] 94%|█████████▍| 268/285 [50:27<03:13, 11.41s/it] 94%|█████████▍| 269/285 [50:38<03:04, 11.53s/it] 95%|█████████▍| 270/285 [50:50<02:51, 11.41s/it] 95%|█████████▌| 271/285 [51:01<02:41, 11.51s/it] 95%|█████████▌| 272/285 [51:13<02:29, 11.51s/it] 96%|█████████▌| 273/285 [51:24<02:17, 11.46s/it] 96%|█████████▌| 274/285 [51:35<02:05, 11.39s/it] 96%|█████████▋| 275/285 [51:46<01:52, 11.29s/it] 97%|█████████▋| 276/285 [51:57<01:40, 11.18s/it] 97%|█████████▋| 277/285 [52:09<01:29, 11.23s/it] 98%|█████████▊| 278/285 [52:20<01:18, 11.23s/it] 98%|█████████▊| 279/285 [52:31<01:07, 11.31s/it] 98%|█████████▊| 280/285 [52:43<00:57, 11.49s/it] 99%|█████████▊| 281/285 [52:54<00:45, 11.37s/it] 99%|█████████▉| 282/285 [53:06<00:34, 11.46s/it] 99%|█████████▉| 283/285 [53:18<00:23, 11.52s/it]100%|█████████▉| 284/285 [53:30<00:11, 11.65s/it]100%|██████████| 285/285 [53:41<00:00, 11.45s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [53:41<00:00, 11.45s/it]100%|██████████| 285/285 [53:41<00:00, 11.30s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'loss': 0.3607, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 3221.1674, 'train_samples_per_second': 5.734, 'train_steps_per_second': 0.088, 'train_loss': 0.2839670315123441, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:10<51:17, 10.84s/it]  1%|          | 2/285 [00:21<50:56, 10.80s/it]  1%|          | 3/285 [00:33<52:22, 11.14s/it]  1%|▏         | 4/285 [00:44<53:13, 11.37s/it]  2%|▏         | 5/285 [00:56<53:42, 11.51s/it]  2%|▏         | 6/285 [01:08<53:27, 11.50s/it]  2%|▏         | 7/285 [01:19<52:36, 11.35s/it]  3%|▎         | 8/285 [01:29<51:18, 11.11s/it]  3%|▎         | 9/285 [01:40<50:42, 11.02s/it]  4%|▎         | 10/285 [01:51<50:57, 11.12s/it]  4%|▍         | 11/285 [02:03<50:54, 11.15s/it]  4%|▍         | 12/285 [02:15<52:04, 11.44s/it]  5%|▍         | 13/285 [02:26<51:34, 11.38s/it]  5%|▍         | 14/285 [02:37<51:16, 11.35s/it]  5%|▌         | 15/285 [02:48<49:55, 11.10s/it]  6%|▌         | 16/285 [02:59<49:28, 11.04s/it]  6%|▌         | 17/285 [03:10<50:08, 11.23s/it]  6%|▋         | 18/285 [03:22<50:21, 11.32s/it]  7%|▋         | 19/285 [03:34<50:42, 11.44s/it]  7%|▋         | 20/285 [03:45<50:49, 11.51s/it]  7%|▋         | 21/285 [03:57<50:44, 11.53s/it]  8%|▊         | 22/285 [04:08<50:37, 11.55s/it]  8%|▊         | 23/285 [04:19<49:32, 11.35s/it]  8%|▊         | 24/285 [04:31<49:14, 11.32s/it]  9%|▉         | 25/285 [04:42<48:56, 11.30s/it]  9%|▉         | 26/285 [04:53<49:02, 11.36s/it]  9%|▉         | 27/285 [05:05<48:45, 11.34s/it] 10%|▉         | 28/285 [05:16<48:36, 11.35s/it] 10%|█         | 29/285 [05:27<48:07, 11.28s/it] 11%|█         | 30/285 [05:38<48:04, 11.31s/it] 11%|█         | 31/285 [05:49<46:44, 11.04s/it] 11%|█         | 32/285 [06:00<46:31, 11.03s/it] 12%|█▏        | 33/285 [06:10<44:34, 10.61s/it] 12%|█▏        | 34/285 [06:19<42:43, 10.21s/it] 12%|█▏        | 35/285 [06:29<42:35, 10.22s/it] 13%|█▎        | 36/285 [06:38<41:13,  9.93s/it] 13%|█▎        | 37/285 [06:49<41:38, 10.08s/it] 13%|█▎        | 38/285 [06:58<40:07,  9.75s/it] 14%|█▎        | 39/285 [07:07<39:24,  9.61s/it] 14%|█▍        | 40/285 [07:18<41:03, 10.06s/it] 14%|█▍        | 41/285 [07:29<41:46, 10.27s/it] 15%|█▍        | 42/285 [07:40<42:54, 10.59s/it] 15%|█▌        | 43/285 [07:52<44:24, 11.01s/it] 15%|█▌        | 44/285 [08:03<44:03, 10.97s/it] 16%|█▌        | 45/285 [08:14<43:46, 10.94s/it] 16%|█▌        | 46/285 [08:27<45:39, 11.46s/it] 16%|█▋        | 47/285 [08:38<45:00, 11.35s/it] 17%|█▋        | 48/285 [08:50<45:42, 11.57s/it] 17%|█▋        | 49/285 [09:01<45:15, 11.51s/it] 18%|█▊        | 50/285 [09:13<45:25, 11.60s/it] 18%|█▊        | 51/285 [09:25<45:55, 11.78s/it] 18%|█▊        | 52/285 [09:36<45:12, 11.64s/it] 19%|█▊        | 53/285 [09:48<44:46, 11.58s/it] 19%|█▉        | 54/285 [09:59<44:23, 11.53s/it] 19%|█▉        | 55/285 [10:11<44:21, 11.57s/it] 20%|█▉        | 56/285 [10:22<43:47, 11.47s/it] 20%|██        | 57/285 [10:34<43:31, 11.45s/it] 20%|██        | 58/285 [10:54<53:34, 14.16s/it] 21%|██        | 59/285 [11:06<50:32, 13.42s/it] 21%|██        | 60/285 [11:18<48:25, 12.91s/it] 21%|██▏       | 61/285 [11:29<46:35, 12.48s/it] 22%|██▏       | 62/285 [11:40<44:11, 11.89s/it] 22%|██▏       | 63/285 [11:51<43:44, 11.82s/it] 22%|██▏       | 64/285 [12:03<43:43, 11.87s/it] 23%|██▎       | 65/285 [12:15<43:24, 11.84s/it] 23%|██▎       | 66/285 [12:26<42:21, 11.61s/it] 24%|██▎       | 67/285 [12:38<42:14, 11.62s/it] 24%|██▍       | 68/285 [12:50<42:33, 11.77s/it] 24%|██▍       | 69/285 [13:00<40:57, 11.38s/it] 25%|██▍       | 70/285 [13:11<40:21, 11.26s/it] 25%|██▍       | 71/285 [13:22<39:53, 11.18s/it] 25%|██▌       | 72/285 [13:32<38:32, 10.86s/it] 26%|██▌       | 73/285 [13:41<36:27, 10.32s/it] 26%|██▌       | 74/285 [13:51<35:36, 10.13s/it] 26%|██▋       | 75/285 [14:03<36:49, 10.52s/it] 27%|██▋       | 76/285 [14:14<38:03, 10.93s/it] 27%|██▋       | 77/285 [14:25<37:57, 10.95s/it] 27%|██▋       | 78/285 [14:37<38:10, 11.06s/it] 28%|██▊       | 79/285 [14:49<38:57, 11.35s/it] 28%|██▊       | 80/285 [15:00<39:11, 11.47s/it] 28%|██▊       | 81/285 [15:12<38:35, 11.35s/it] 29%|██▉       | 82/285 [15:23<38:25, 11.36s/it] 29%|██▉       | 83/285 [15:34<37:54, 11.26s/it] 29%|██▉       | 84/285 [15:46<38:00, 11.35s/it] 30%|██▉       | 85/285 [15:56<37:06, 11.13s/it] 30%|███       | 86/285 [16:08<37:26, 11.29s/it] 31%|███       | 87/285 [16:19<36:44, 11.14s/it] 31%|███       | 88/285 [16:30<37:01, 11.28s/it] 31%|███       | 89/285 [16:41<36:42, 11.24s/it] 32%|███▏      | 90/285 [16:53<37:02, 11.40s/it] 32%|███▏      | 91/285 [17:05<36:57, 11.43s/it] 32%|███▏      | 92/285 [17:15<36:03, 11.21s/it] 33%|███▎      | 93/285 [17:26<35:50, 11.20s/it] 33%|███▎      | 94/285 [17:38<35:52, 11.27s/it] 33%|███▎      | 95/285 [17:49<35:31, 11.22s/it] 34%|███▎      | 96/285 [18:00<35:05, 11.14s/it] 34%|███▍      | 97/285 [18:11<34:59, 11.17s/it] 34%|███▍      | 98/285 [18:22<34:46, 11.16s/it] 35%|███▍      | 99/285 [18:33<34:03, 10.99s/it] 35%|███▌      | 100/285 [18:44<34:03, 11.04s/it] 35%|███▌      | 101/285 [18:56<34:39, 11.30s/it] 36%|███▌      | 102/285 [19:07<34:07, 11.19s/it] 36%|███▌      | 103/285 [19:18<33:47, 11.14s/it] 36%|███▋      | 104/285 [19:29<33:44, 11.19s/it] 37%|███▋      | 105/285 [19:40<33:30, 11.17s/it] 37%|███▋      | 106/285 [19:52<34:08, 11.44s/it] 38%|███▊      | 107/285 [20:04<34:08, 11.51s/it] 38%|███▊      | 108/285 [20:16<33:51, 11.48s/it] 38%|███▊      | 109/285 [20:27<33:21, 11.37s/it] 39%|███▊      | 110/285 [20:38<33:25, 11.46s/it] 39%|███▉      | 111/285 [20:50<33:20, 11.50s/it] 39%|███▉      | 112/285 [21:02<33:22, 11.58s/it] 40%|███▉      | 113/285 [21:13<32:38, 11.39s/it] 40%|████      | 114/285 [21:25<32:59, 11.57s/it] 40%|████      | 115/285 [21:45<40:07, 14.16s/it] 41%|████      | 116/285 [21:56<37:33, 13.34s/it] 41%|████      | 117/285 [22:08<35:43, 12.76s/it] 41%|████▏     | 118/285 [22:19<33:55, 12.19s/it] 42%|████▏     | 119/285 [22:30<32:46, 11.84s/it] 42%|████▏     | 120/285 [22:41<32:15, 11.73s/it] 42%|████▏     | 121/285 [22:52<31:26, 11.51s/it] 43%|████▎     | 122/285 [23:03<30:54, 11.38s/it] 43%|████▎     | 123/285 [23:15<31:03, 11.50s/it] 44%|████▎     | 124/285 [23:26<30:34, 11.40s/it] 44%|████▍     | 125/285 [23:38<30:32, 11.45s/it] 44%|████▍     | 126/285 [23:49<30:36, 11.55s/it] 45%|████▍     | 127/285 [24:01<30:24, 11.55s/it] 45%|████▍     | 128/285 [24:13<30:22, 11.61s/it] 45%|████▌     | 129/285 [24:24<29:49, 11.47s/it] 46%|████▌     | 130/285 [24:35<29:28, 11.41s/it] 46%|████▌     | 131/285 [24:47<29:43, 11.58s/it] 46%|████▋     | 132/285 [24:59<29:31, 11.58s/it] 47%|████▋     | 133/285 [25:10<29:22, 11.59s/it] 47%|████▋     | 134/285 [25:22<29:28, 11.71s/it] 47%|████▋     | 135/285 [25:33<28:44, 11.50s/it] 48%|████▊     | 136/285 [25:44<28:16, 11.39s/it] 48%|████▊     | 137/285 [25:56<28:07, 11.40s/it] 48%|████▊     | 138/285 [26:08<28:37, 11.69s/it] 49%|████▉     | 139/285 [26:19<27:54, 11.47s/it] 49%|████▉     | 140/285 [26:30<27:32, 11.40s/it] 49%|████▉     | 141/285 [26:42<27:26, 11.43s/it] 50%|████▉     | 142/285 [26:53<27:13, 11.42s/it] 50%|█████     | 143/285 [27:04<26:26, 11.17s/it] 51%|█████     | 144/285 [27:16<26:56, 11.46s/it] 51%|█████     | 145/285 [27:28<27:06, 11.62s/it] 51%|█████     | 146/285 [27:39<26:31, 11.45s/it] 52%|█████▏    | 147/285 [27:50<25:50, 11.23s/it] 52%|█████▏    | 148/285 [28:01<25:34, 11.20s/it] 52%|█████▏    | 149/285 [28:13<25:50, 11.40s/it] 53%|█████▎    | 150/285 [28:25<25:54, 11.51s/it] 53%|█████▎    | 151/285 [28:35<25:02, 11.21s/it] 53%|█████▎    | 152/285 [28:46<24:38, 11.12s/it] 54%|█████▎    | 153/285 [28:57<24:24, 11.09s/it] 54%|█████▍    | 154/285 [29:09<24:39, 11.29s/it] 54%|█████▍    | 155/285 [29:20<24:40, 11.39s/it] 55%|█████▍    | 156/285 [29:31<24:01, 11.18s/it] 55%|█████▌    | 157/285 [29:41<23:18, 10.93s/it] 55%|█████▌    | 158/285 [29:51<22:29, 10.62s/it] 56%|█████▌    | 159/285 [30:02<22:32, 10.73s/it] 56%|█████▌    | 160/285 [30:12<21:53, 10.51s/it] 56%|█████▋    | 161/285 [30:23<21:48, 10.55s/it] 57%|█████▋    | 162/285 [30:32<20:49, 10.16s/it] 57%|█████▋    | 163/285 [30:42<20:14,  9.96s/it] 58%|█████▊    | 164/285 [30:51<19:24,  9.63s/it] 58%|█████▊    | 165/285 [31:03<21:10, 10.59s/it] 58%|█████▊    | 166/285 [31:15<21:25, 10.80s/it] 59%|█████▊    | 167/285 [31:26<21:37, 10.99s/it] 59%|█████▉    | 168/285 [31:37<21:19, 10.94s/it] 59%|█████▉    | 169/285 [31:48<21:07, 10.93s/it] 60%|█████▉    | 170/285 [31:59<21:10, 11.05s/it] 60%|██████    | 171/285 [32:10<21:05, 11.10s/it] 60%|██████    | 172/285 [32:29<24:58, 13.26s/it] 61%|██████    | 173/285 [32:40<23:32, 12.62s/it] 61%|██████    | 174/285 [32:51<22:43, 12.28s/it] 61%|██████▏   | 175/285 [33:03<21:57, 11.98s/it] 62%|██████▏   | 176/285 [33:14<21:13, 11.69s/it] 62%|██████▏   | 177/285 [33:25<20:57, 11.64s/it] 62%|██████▏   | 178/285 [33:36<20:22, 11.43s/it] 63%|██████▎   | 179/285 [33:47<20:02, 11.34s/it] 63%|██████▎   | 180/285 [33:58<19:41, 11.25s/it] 64%|██████▎   | 181/285 [34:11<20:02, 11.57s/it] 64%|██████▍   | 182/285 [34:22<19:40, 11.46s/it] 64%|██████▍   | 183/285 [34:32<18:58, 11.16s/it] 65%|██████▍   | 184/285 [34:42<17:55, 10.65s/it] 65%|██████▍   | 185/285 [34:53<17:58, 10.79s/it] 65%|██████▌   | 186/285 [35:04<17:48, 10.79s/it] 66%|██████▌   | 187/285 [35:15<17:55, 10.97s/it] 66%|██████▌   | 188/285 [35:26<17:55, 11.08s/it] 66%|██████▋   | 189/285 [35:38<18:05, 11.31s/it] 67%|██████▋   | 190/285 [35:49<17:43, 11.20s/it] 67%|██████▋   | 191/285 [36:01<17:39, 11.27s/it] 67%|██████▋   | 192/285 [36:12<17:30, 11.30s/it] 68%|██████▊   | 193/285 [36:23<17:02, 11.11s/it] 68%|██████▊   | 194/285 [36:33<16:25, 10.83s/it] 68%|██████▊   | 195/285 [36:45<16:47, 11.19s/it] 69%|██████▉   | 196/285 [36:55<16:16, 10.97s/it] 69%|██████▉   | 197/285 [37:07<16:14, 11.08s/it] 69%|██████▉   | 198/285 [37:18<16:09, 11.15s/it] 70%|██████▉   | 199/285 [37:29<15:55, 11.11s/it] 70%|███████   | 200/285 [37:41<16:11, 11.43s/it]                                                  70%|███████   | 200/285 [37:41<16:11, 11.43s/it] 71%|███████   | 201/285 [37:53<16:14, 11.61s/it] 71%|███████   | 202/285 [38:04<15:48, 11.43s/it] 71%|███████   | 203/285 [38:16<15:46, 11.54s/it] 72%|███████▏  | 204/285 [38:28<15:49, 11.72s/it] 72%|███████▏  | 205/285 [38:40<15:38, 11.73s/it] 72%|███████▏  | 206/285 [38:51<15:08, 11.49s/it] 73%|███████▎  | 207/285 [39:02<14:55, 11.49s/it] 73%|███████▎  | 208/285 [39:13<14:24, 11.22s/it] 73%|███████▎  | 209/285 [39:24<14:05, 11.12s/it] 74%|███████▎  | 210/285 [39:35<13:57, 11.17s/it] 74%|███████▍  | 211/285 [39:46<13:50, 11.22s/it] 74%|███████▍  | 212/285 [39:57<13:33, 11.14s/it] 75%|███████▍  | 213/285 [40:09<13:36, 11.34s/it] 75%|███████▌  | 214/285 [40:20<13:22, 11.30s/it] 75%|███████▌  | 215/285 [40:32<13:16, 11.38s/it] 76%|███████▌  | 216/285 [40:44<13:11, 11.47s/it] 76%|███████▌  | 217/285 [40:55<13:00, 11.48s/it] 76%|███████▋  | 218/285 [41:06<12:39, 11.33s/it] 77%|███████▋  | 219/285 [41:17<12:24, 11.28s/it] 77%|███████▋  | 220/285 [41:29<12:28, 11.52s/it] 78%|███████▊  | 221/285 [41:42<12:32, 11.76s/it] 78%|███████▊  | 222/285 [41:52<11:59, 11.43s/it] 78%|███████▊  | 223/285 [42:04<11:46, 11.39s/it] 79%|███████▊  | 224/285 [42:16<11:46, 11.57s/it] 79%|███████▉  | 225/285 [42:27<11:25, 11.42s/it] 79%|███████▉  | 226/285 [42:38<11:18, 11.51s/it] 80%|███████▉  | 227/285 [42:50<11:04, 11.46s/it] 80%|████████  | 228/285 [43:00<10:32, 11.10s/it] 80%|████████  | 229/285 [43:17<12:06, 12.97s/it] 81%|████████  | 230/285 [43:28<11:15, 12.28s/it] 81%|████████  | 231/285 [43:38<10:31, 11.70s/it] 81%|████████▏ | 232/285 [43:47<09:34, 10.84s/it] 82%|████████▏ | 233/285 [43:57<09:11, 10.61s/it] 82%|████████▏ | 234/285 [44:08<09:08, 10.76s/it] 82%|████████▏ | 235/285 [44:19<09:02, 10.86s/it] 83%|████████▎ | 236/285 [44:30<08:55, 10.92s/it] 83%|████████▎ | 237/285 [44:41<08:39, 10.83s/it] 84%|████████▎ | 238/285 [44:53<08:44, 11.16s/it] 84%|████████▍ | 239/285 [45:04<08:32, 11.14s/it] 84%|████████▍ | 240/285 [45:16<08:27, 11.28s/it] 85%|████████▍ | 241/285 [45:27<08:18, 11.34s/it] 85%|████████▍ | 242/285 [45:38<08:04, 11.26s/it] 85%|████████▌ | 243/285 [45:49<07:43, 11.04s/it] 86%|████████▌ | 244/285 [45:59<07:28, 10.94s/it] 86%|████████▌ | 245/285 [46:11<07:30, 11.26s/it] 86%|████████▋ | 246/285 [46:22<07:12, 11.09s/it] 87%|████████▋ | 247/285 [46:34<07:11, 11.36s/it] 87%|████████▋ | 248/285 [46:45<06:56, 11.27s/it] 87%|████████▋ | 249/285 [46:56<06:41, 11.16s/it] 88%|████████▊ | 250/285 [47:06<06:20, 10.88s/it] 88%|████████▊ | 251/285 [47:16<05:56, 10.48s/it] 88%|████████▊ | 252/285 [47:28<05:58, 10.86s/it] 89%|████████▉ | 253/285 [47:39<05:51, 10.98s/it] 89%|████████▉ | 254/285 [47:51<05:48, 11.23s/it] 89%|████████▉ | 255/285 [48:03<05:42, 11.41s/it] 90%|████████▉ | 256/285 [48:14<05:27, 11.31s/it] 90%|█████████ | 257/285 [48:25<05:15, 11.27s/it] 91%|█████████ | 258/285 [48:36<05:06, 11.34s/it] 91%|█████████ | 259/285 [48:48<04:56, 11.40s/it] 91%|█████████ | 260/285 [48:59<04:42, 11.29s/it] 92%|█████████▏| 261/285 [49:11<04:36, 11.52s/it] 92%|█████████▏| 262/285 [49:22<04:22, 11.42s/it] 92%|█████████▏| 263/285 [49:34<04:12, 11.49s/it] 93%|█████████▎| 264/285 [49:45<04:02, 11.55s/it] 93%|█████████▎| 265/285 [49:56<03:47, 11.39s/it] 93%|█████████▎| 266/285 [50:08<03:36, 11.38s/it] 94%|█████████▎| 267/285 [50:19<03:26, 11.45s/it] 94%|█████████▍| 268/285 [50:31<03:14, 11.45s/it] 94%|█████████▍| 269/285 [50:43<03:05, 11.58s/it] 95%|█████████▍| 270/285 [50:54<02:51, 11.41s/it] 95%|█████████▌| 271/285 [51:06<02:41, 11.51s/it] 95%|█████████▌| 272/285 [51:17<02:30, 11.60s/it] 96%|█████████▌| 273/285 [51:29<02:18, 11.57s/it] 96%|█████████▌| 274/285 [51:39<02:02, 11.14s/it] 96%|█████████▋| 275/285 [51:49<01:47, 10.78s/it] 97%|█████████▋| 276/285 [51:58<01:33, 10.36s/it] 97%|█████████▋| 277/285 [52:09<01:23, 10.43s/it] 98%|█████████▊| 278/285 [52:20<01:14, 10.64s/it] 98%|█████████▊| 279/285 [52:31<01:04, 10.78s/it] 98%|█████████▊| 280/285 [52:43<00:55, 11.11s/it] 99%|█████████▊| 281/285 [52:54<00:44, 11.11s/it] 99%|█████████▉| 282/285 [53:06<00:33, 11.30s/it] 99%|█████████▉| 283/285 [53:17<00:22, 11.39s/it]100%|█████████▉| 284/285 [53:30<00:11, 11.58s/it]100%|██████████| 285/285 [53:40<00:00, 11.38s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [53:40<00:00, 11.38s/it]100%|██████████| 285/285 [53:40<00:00, 11.30s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652359063
{'loss': 0.4602, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 3220.9005, 'train_samples_per_second': 5.734, 'train_steps_per_second': 0.088, 'train_loss': 0.34643863878752057, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:16<1:16:41, 16.20s/it]  1%|          | 2/285 [00:31<1:15:15, 15.95s/it]  1%|          | 3/285 [00:48<1:16:23, 16.25s/it]  1%|▏         | 4/285 [01:05<1:17:26, 16.54s/it]  2%|▏         | 5/285 [01:22<1:17:58, 16.71s/it]  2%|▏         | 6/285 [01:39<1:18:06, 16.80s/it]  2%|▏         | 7/285 [01:55<1:16:26, 16.50s/it]  3%|▎         | 8/285 [02:10<1:13:42, 15.97s/it]  3%|▎         | 9/285 [02:24<1:11:35, 15.56s/it]  4%|▎         | 10/285 [02:41<1:12:59, 15.92s/it]  4%|▍         | 11/285 [02:57<1:12:49, 15.95s/it]  4%|▍         | 12/285 [03:15<1:15:16, 16.54s/it]  5%|▍         | 13/285 [03:31<1:14:41, 16.48s/it]  5%|▍         | 14/285 [03:48<1:14:11, 16.43s/it]  5%|▌         | 15/285 [04:03<1:11:57, 15.99s/it]  6%|▌         | 16/285 [04:18<1:10:44, 15.78s/it]  6%|▌         | 17/285 [04:35<1:12:23, 16.21s/it]  6%|▋         | 18/285 [04:52<1:12:51, 16.37s/it]  7%|▋         | 19/285 [05:09<1:13:31, 16.58s/it]  7%|▋         | 20/285 [05:26<1:13:34, 16.66s/it]  7%|▋         | 21/285 [05:43<1:13:36, 16.73s/it]  8%|▊         | 22/285 [05:59<1:13:07, 16.68s/it]  8%|▊         | 23/285 [06:15<1:11:15, 16.32s/it]  8%|▊         | 24/285 [06:31<1:10:25, 16.19s/it]  9%|▉         | 25/285 [06:47<1:10:02, 16.16s/it]  9%|▉         | 26/285 [07:03<1:10:18, 16.29s/it]  9%|▉         | 27/285 [07:20<1:10:03, 16.29s/it] 10%|▉         | 28/285 [07:36<1:10:10, 16.38s/it] 10%|█         | 29/285 [07:52<1:09:26, 16.27s/it] 11%|█         | 30/285 [08:09<1:09:30, 16.35s/it] 11%|█         | 31/285 [08:25<1:08:55, 16.28s/it] 11%|█         | 32/285 [08:43<1:10:42, 16.77s/it] 12%|█▏        | 33/285 [08:58<1:08:57, 16.42s/it] 12%|█▏        | 34/285 [09:13<1:06:50, 15.98s/it] 12%|█▏        | 35/285 [09:30<1:07:25, 16.18s/it] 13%|█▎        | 36/285 [09:46<1:06:28, 16.02s/it] 13%|█▎        | 37/285 [10:03<1:07:42, 16.38s/it] 13%|█▎        | 38/285 [10:18<1:06:16, 16.10s/it] 14%|█▎        | 39/285 [10:34<1:05:52, 16.07s/it] 14%|█▍        | 40/285 [10:50<1:05:38, 16.08s/it] 14%|█▍        | 41/285 [11:06<1:04:31, 15.87s/it] 15%|█▍        | 42/285 [11:22<1:04:56, 16.04s/it] 15%|█▌        | 43/285 [11:39<1:05:58, 16.36s/it] 15%|█▌        | 44/285 [11:55<1:05:01, 16.19s/it] 16%|█▌        | 45/285 [12:11<1:03:54, 15.98s/it] 16%|█▌        | 46/285 [12:29<1:06:57, 16.81s/it] 16%|█▋        | 47/285 [12:46<1:05:51, 16.61s/it] 17%|█▋        | 48/285 [13:04<1:07:19, 17.04s/it] 17%|█▋        | 49/285 [13:20<1:06:36, 16.93s/it] 18%|█▊        | 50/285 [13:37<1:06:36, 17.01s/it] 18%|█▊        | 51/285 [13:55<1:07:33, 17.32s/it] 18%|█▊        | 52/285 [14:12<1:06:18, 17.07s/it] 19%|█▊        | 53/285 [14:28<1:05:13, 16.87s/it] 19%|█▉        | 54/285 [14:44<1:03:51, 16.59s/it] 19%|█▉        | 55/285 [15:01<1:03:23, 16.54s/it] 20%|█▉        | 56/285 [15:17<1:02:31, 16.38s/it] 20%|██        | 57/285 [15:33<1:01:45, 16.25s/it] 20%|██        | 58/285 [16:02<1:16:52, 20.32s/it] 21%|██        | 59/285 [16:20<1:12:59, 19.38s/it] 21%|██        | 60/285 [16:37<1:09:50, 18.63s/it] 21%|██▏       | 61/285 [16:53<1:07:15, 18.02s/it] 22%|██▏       | 62/285 [17:08<1:03:40, 17.13s/it] 22%|██▏       | 63/285 [17:25<1:03:11, 17.08s/it] 22%|██▏       | 64/285 [17:42<1:02:55, 17.09s/it] 23%|██▎       | 65/285 [17:59<1:02:23, 17.02s/it] 23%|██▎       | 66/285 [18:15<1:00:54, 16.69s/it] 24%|██▎       | 67/285 [18:32<1:00:46, 16.73s/it] 24%|██▍       | 68/285 [18:50<1:01:30, 17.01s/it] 24%|██▍       | 69/285 [19:04<58:49, 16.34s/it]   25%|██▍       | 70/285 [19:20<58:04, 16.21s/it] 25%|██▍       | 71/285 [19:37<58:33, 16.42s/it] 25%|██▌       | 72/285 [19:53<58:14, 16.41s/it] 26%|██▌       | 73/285 [20:09<56:38, 16.03s/it] 26%|██▌       | 74/285 [20:24<56:11, 15.98s/it] 26%|██▋       | 75/285 [20:41<56:44, 16.21s/it] 27%|██▋       | 76/285 [20:59<57:55, 16.63s/it] 27%|██▋       | 77/285 [21:14<56:32, 16.31s/it] 27%|██▋       | 78/285 [21:30<55:58, 16.22s/it] 28%|██▊       | 79/285 [21:48<56:48, 16.54s/it] 28%|██▊       | 80/285 [22:05<56:57, 16.67s/it] 28%|██▊       | 81/285 [22:20<55:42, 16.39s/it] 29%|██▉       | 82/285 [22:37<55:34, 16.42s/it] 29%|██▉       | 83/285 [22:53<55:24, 16.46s/it] 29%|██▉       | 84/285 [23:10<55:17, 16.50s/it] 30%|██▉       | 85/285 [23:25<53:27, 16.04s/it] 30%|███       | 86/285 [23:42<54:18, 16.38s/it] 31%|███       | 87/285 [23:58<53:35, 16.24s/it] 31%|███       | 88/285 [24:15<54:09, 16.49s/it] 31%|███       | 89/285 [24:31<53:22, 16.34s/it] 32%|███▏      | 90/285 [24:48<53:51, 16.57s/it] 32%|███▏      | 91/285 [25:05<53:44, 16.62s/it] 32%|███▏      | 92/285 [25:20<52:03, 16.18s/it] 33%|███▎      | 93/285 [25:36<51:39, 16.14s/it] 33%|███▎      | 94/285 [25:53<51:49, 16.28s/it] 33%|███▎      | 95/285 [26:08<50:09, 15.84s/it] 34%|███▎      | 96/285 [26:23<49:18, 15.65s/it] 34%|███▍      | 97/285 [26:39<49:31, 15.81s/it] 34%|███▍      | 98/285 [26:55<49:07, 15.76s/it] 35%|███▍      | 99/285 [27:09<47:46, 15.41s/it] 35%|███▌      | 100/285 [27:25<47:39, 15.46s/it] 35%|███▌      | 101/285 [27:42<49:23, 16.10s/it] 36%|███▌      | 102/285 [27:58<48:37, 15.95s/it] 36%|███▌      | 103/285 [28:14<48:11, 15.89s/it] 36%|███▋      | 104/285 [28:31<48:43, 16.15s/it] 37%|███▋      | 105/285 [28:47<48:25, 16.14s/it] 37%|███▋      | 106/285 [29:04<49:36, 16.63s/it] 38%|███▊      | 107/285 [29:21<49:37, 16.73s/it] 38%|███▊      | 108/285 [29:38<49:12, 16.68s/it] 38%|███▊      | 109/285 [29:54<48:08, 16.41s/it] 39%|███▊      | 110/285 [30:10<48:05, 16.49s/it] 39%|███▉      | 111/285 [30:27<47:59, 16.55s/it] 39%|███▉      | 112/285 [30:44<48:14, 16.73s/it] 40%|███▉      | 113/285 [31:00<46:54, 16.36s/it] 40%|████      | 114/285 [31:16<46:45, 16.41s/it] 40%|████      | 115/285 [31:45<56:58, 20.11s/it] 41%|████      | 116/285 [32:02<53:35, 19.03s/it] 41%|████      | 117/285 [32:18<51:09, 18.27s/it] 41%|████▏     | 118/285 [32:34<48:44, 17.51s/it] 42%|████▏     | 119/285 [32:50<47:08, 17.04s/it] 42%|████▏     | 120/285 [33:07<46:56, 17.07s/it] 42%|████▏     | 121/285 [33:23<45:32, 16.66s/it] 43%|████▎     | 122/285 [33:38<44:37, 16.42s/it] 43%|████▎     | 123/285 [33:56<44:59, 16.66s/it] 44%|████▎     | 124/285 [34:12<44:08, 16.45s/it] 44%|████▍     | 125/285 [34:28<44:11, 16.57s/it] 44%|████▍     | 126/285 [34:46<44:26, 16.77s/it] 45%|████▍     | 127/285 [35:02<44:10, 16.78s/it] 45%|████▍     | 128/285 [35:19<43:54, 16.78s/it] 45%|████▌     | 129/285 [35:35<42:58, 16.53s/it] 46%|████▌     | 130/285 [35:51<42:19, 16.38s/it] 46%|████▌     | 131/285 [36:09<42:55, 16.72s/it] 46%|████▋     | 132/285 [36:26<42:41, 16.74s/it] 47%|████▋     | 133/285 [36:42<42:13, 16.67s/it] 47%|████▋     | 134/285 [36:59<42:31, 16.90s/it] 47%|████▋     | 135/285 [37:15<41:02, 16.42s/it] 48%|████▊     | 136/285 [37:31<40:14, 16.21s/it] 48%|████▊     | 137/285 [37:47<40:20, 16.35s/it] 48%|████▊     | 138/285 [38:04<40:28, 16.52s/it] 49%|████▉     | 139/285 [38:20<39:41, 16.31s/it] 49%|████▉     | 140/285 [38:36<39:22, 16.29s/it] 49%|████▉     | 141/285 [38:53<39:28, 16.44s/it] 50%|████▉     | 142/285 [39:10<39:21, 16.52s/it] 50%|█████     | 143/285 [39:25<37:57, 16.04s/it] 51%|█████     | 144/285 [39:42<38:53, 16.55s/it] 51%|█████     | 145/285 [40:00<39:16, 16.83s/it] 51%|█████     | 146/285 [40:16<38:22, 16.57s/it] 52%|█████▏    | 147/285 [40:31<37:19, 16.23s/it] 52%|█████▏    | 148/285 [40:47<36:52, 16.15s/it] 52%|█████▏    | 149/285 [41:04<37:23, 16.50s/it] 53%|█████▎    | 150/285 [41:21<37:26, 16.64s/it] 53%|█████▎    | 151/285 [41:36<35:57, 16.10s/it] 53%|█████▎    | 152/285 [41:52<35:23, 15.96s/it] 54%|█████▎    | 153/285 [42:07<34:45, 15.80s/it] 54%|█████▍    | 154/285 [42:24<35:07, 16.09s/it] 54%|█████▍    | 155/285 [42:40<34:45, 16.04s/it] 55%|█████▍    | 156/285 [42:56<34:39, 16.12s/it] 55%|█████▌    | 157/285 [43:13<34:47, 16.31s/it] 55%|█████▌    | 158/285 [43:29<34:22, 16.24s/it] 56%|█████▌    | 159/285 [43:47<34:49, 16.59s/it] 56%|█████▌    | 160/285 [44:03<34:09, 16.40s/it] 56%|█████▋    | 161/285 [44:20<34:35, 16.74s/it] 57%|█████▋    | 162/285 [44:36<33:42, 16.44s/it] 57%|█████▋    | 163/285 [44:52<33:05, 16.28s/it] 58%|█████▊    | 164/285 [45:07<32:08, 15.94s/it] 58%|█████▊    | 165/285 [45:26<33:48, 16.91s/it] 58%|█████▊    | 166/285 [45:42<32:52, 16.57s/it] 59%|█████▊    | 167/285 [45:58<32:33, 16.56s/it] 59%|█████▉    | 168/285 [46:14<31:39, 16.24s/it] 59%|█████▉    | 169/285 [46:30<31:08, 16.11s/it] 60%|█████▉    | 170/285 [46:46<31:03, 16.20s/it] 60%|██████    | 171/285 [47:02<30:39, 16.14s/it] 60%|██████    | 172/285 [47:28<35:48, 19.01s/it] 61%|██████    | 173/285 [47:44<33:48, 18.11s/it] 61%|██████    | 174/285 [48:01<32:45, 17.71s/it] 61%|██████▏   | 175/285 [48:17<31:45, 17.32s/it] 62%|██████▏   | 176/285 [48:33<30:31, 16.80s/it] 62%|██████▏   | 177/285 [48:49<30:16, 16.82s/it] 62%|██████▏   | 178/285 [49:05<29:21, 16.46s/it] 63%|██████▎   | 179/285 [49:21<28:52, 16.34s/it] 63%|██████▎   | 180/285 [49:37<28:20, 16.20s/it] 64%|██████▎   | 181/285 [49:55<29:01, 16.75s/it] 64%|██████▍   | 182/285 [50:11<28:21, 16.51s/it] 64%|██████▍   | 183/285 [50:27<27:53, 16.41s/it] 65%|██████▍   | 184/285 [50:43<27:10, 16.15s/it] 65%|██████▍   | 185/285 [51:00<27:39, 16.60s/it] 65%|██████▌   | 186/285 [51:18<27:41, 16.78s/it] 66%|██████▌   | 187/285 [51:34<27:21, 16.75s/it] 66%|██████▌   | 188/285 [51:51<26:56, 16.67s/it] 66%|██████▋   | 189/285 [52:08<26:53, 16.81s/it] 67%|██████▋   | 190/285 [52:23<25:52, 16.34s/it] 67%|██████▋   | 191/285 [52:40<25:49, 16.49s/it] 67%|██████▋   | 192/285 [52:56<25:32, 16.48s/it] 68%|██████▊   | 193/285 [53:12<24:56, 16.26s/it] 68%|██████▊   | 194/285 [53:28<24:25, 16.10s/it] 68%|██████▊   | 195/285 [53:46<24:52, 16.58s/it] 69%|██████▉   | 196/285 [54:01<24:02, 16.21s/it] 69%|██████▉   | 197/285 [54:18<23:59, 16.35s/it] 69%|██████▉   | 198/285 [54:34<23:34, 16.26s/it] 70%|██████▉   | 199/285 [54:49<22:57, 16.02s/it] 70%|███████   | 200/285 [55:07<23:26, 16.54s/it]                                                  70%|███████   | 200/285 [55:07<23:26, 16.54s/it] 71%|███████   | 201/285 [55:24<23:37, 16.88s/it] 71%|███████   | 202/285 [55:40<22:45, 16.45s/it] 71%|███████   | 203/285 [55:57<22:43, 16.62s/it] 72%|███████▏  | 204/285 [56:15<22:53, 16.96s/it] 72%|███████▏  | 205/285 [56:32<22:37, 16.97s/it] 72%|███████▏  | 206/285 [56:47<21:42, 16.49s/it] 73%|███████▎  | 207/285 [57:04<21:27, 16.51s/it] 73%|███████▎  | 208/285 [57:18<20:30, 15.98s/it] 73%|███████▎  | 209/285 [57:34<19:58, 15.78s/it] 74%|███████▎  | 210/285 [57:50<19:48, 15.84s/it] 74%|███████▍  | 211/285 [58:06<19:46, 16.03s/it] 74%|███████▍  | 212/285 [58:22<19:19, 15.89s/it] 75%|███████▍  | 213/285 [58:38<19:22, 16.14s/it] 75%|███████▌  | 214/285 [58:54<19:04, 16.12s/it] 75%|███████▌  | 215/285 [59:11<18:57, 16.26s/it] 76%|███████▌  | 216/285 [59:27<18:40, 16.24s/it] 76%|███████▌  | 217/285 [59:44<18:32, 16.37s/it] 76%|███████▋  | 218/285 [1:00:00<18:05, 16.19s/it] 77%|███████▋  | 219/285 [1:00:15<17:37, 16.02s/it] 77%|███████▋  | 220/285 [1:00:33<17:47, 16.43s/it] 78%|███████▊  | 221/285 [1:00:50<17:56, 16.83s/it] 78%|███████▊  | 222/285 [1:01:06<17:06, 16.30s/it] 78%|███████▊  | 223/285 [1:01:22<16:53, 16.34s/it] 79%|███████▊  | 224/285 [1:01:39<16:53, 16.62s/it] 79%|███████▉  | 225/285 [1:01:55<16:25, 16.42s/it] 79%|███████▉  | 226/285 [1:02:12<16:18, 16.58s/it] 80%|███████▉  | 227/285 [1:02:29<16:02, 16.59s/it] 80%|████████  | 228/285 [1:02:45<15:39, 16.48s/it] 80%|████████  | 229/285 [1:03:13<18:41, 20.02s/it] 81%|████████  | 230/285 [1:03:30<17:33, 19.15s/it] 81%|████████  | 231/285 [1:03:47<16:41, 18.54s/it] 81%|████████▏ | 232/285 [1:04:03<15:28, 17.53s/it] 82%|████████▏ | 233/285 [1:04:18<14:42, 16.97s/it] 82%|████████▏ | 234/285 [1:04:35<14:14, 16.75s/it] 82%|████████▏ | 235/285 [1:04:51<13:46, 16.54s/it] 83%|████████▎ | 236/285 [1:05:06<13:19, 16.32s/it] 83%|████████▎ | 237/285 [1:05:22<12:49, 16.03s/it] 84%|████████▎ | 238/285 [1:05:39<12:47, 16.32s/it] 84%|████████▍ | 239/285 [1:05:55<12:25, 16.21s/it] 84%|████████▍ | 240/285 [1:06:11<12:15, 16.35s/it] 85%|████████▍ | 241/285 [1:06:28<12:04, 16.47s/it] 85%|████████▍ | 242/285 [1:06:44<11:39, 16.26s/it] 85%|████████▌ | 243/285 [1:06:59<11:04, 15.83s/it] 86%|████████▌ | 244/285 [1:07:14<10:43, 15.69s/it] 86%|████████▌ | 245/285 [1:07:30<10:34, 15.85s/it] 86%|████████▋ | 246/285 [1:07:46<10:10, 15.65s/it] 87%|████████▋ | 247/285 [1:08:03<10:13, 16.15s/it] 87%|████████▋ | 248/285 [1:08:19<09:57, 16.15s/it] 87%|████████▋ | 249/285 [1:08:37<09:57, 16.60s/it] 88%|████████▊ | 250/285 [1:08:53<09:41, 16.61s/it] 88%|████████▊ | 251/285 [1:09:09<09:13, 16.27s/it] 88%|████████▊ | 252/285 [1:09:26<09:09, 16.64s/it] 89%|████████▉ | 253/285 [1:09:42<08:44, 16.40s/it] 89%|████████▉ | 254/285 [1:09:59<08:37, 16.70s/it] 89%|████████▉ | 255/285 [1:10:17<08:24, 16.81s/it] 90%|████████▉ | 256/285 [1:10:33<08:00, 16.58s/it] 90%|█████████ | 257/285 [1:10:48<07:35, 16.27s/it] 91%|█████████ | 258/285 [1:11:04<07:19, 16.27s/it] 91%|█████████ | 259/285 [1:11:21<07:06, 16.39s/it] 91%|█████████ | 260/285 [1:11:37<06:48, 16.33s/it] 92%|█████████▏| 261/285 [1:11:55<06:40, 16.67s/it] 92%|█████████▏| 262/285 [1:12:11<06:19, 16.48s/it] 92%|█████████▏| 263/285 [1:12:28<06:05, 16.61s/it] 93%|█████████▎| 264/285 [1:12:45<05:50, 16.68s/it] 93%|█████████▎| 265/285 [1:13:00<05:26, 16.32s/it] 93%|█████████▎| 266/285 [1:13:16<05:10, 16.36s/it] 94%|█████████▎| 267/285 [1:13:33<04:55, 16.44s/it] 94%|█████████▍| 268/285 [1:13:49<04:38, 16.38s/it] 94%|█████████▍| 269/285 [1:14:06<04:25, 16.59s/it] 95%|█████████▍| 270/285 [1:14:22<04:05, 16.37s/it] 95%|█████████▌| 271/285 [1:14:39<03:51, 16.52s/it] 95%|█████████▌| 272/285 [1:14:56<03:35, 16.60s/it] 96%|█████████▌| 273/285 [1:15:12<03:18, 16.55s/it] 96%|█████████▌| 274/285 [1:15:29<03:01, 16.54s/it] 96%|█████████▋| 275/285 [1:15:45<02:43, 16.39s/it] 97%|█████████▋| 276/285 [1:16:00<02:24, 16.11s/it] 97%|█████████▋| 277/285 [1:16:17<02:09, 16.20s/it] 98%|█████████▊| 278/285 [1:16:33<01:53, 16.21s/it] 98%|█████████▊| 279/285 [1:16:49<01:37, 16.28s/it] 98%|█████████▊| 280/285 [1:17:07<01:23, 16.68s/it] 99%|█████████▊| 281/285 [1:17:23<01:06, 16.51s/it] 99%|█████████▉| 282/285 [1:17:40<00:49, 16.60s/it] 99%|█████████▉| 283/285 [1:17:57<00:33, 16.70s/it]100%|█████████▉| 284/285 [1:18:15<00:16, 16.96s/it]100%|██████████| 285/285 [1:18:30<00:00, 16.50s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 285/285 [1:18:30<00:00, 16.50s/it]100%|██████████| 285/285 [1:18:30<00:00, 16.53s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652356503
{'loss': 0.2729, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 4710.4372, 'train_samples_per_second': 3.921, 'train_steps_per_second': 0.061, 'train_loss': 0.19814495496582565, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:17<1:21:42, 17.26s/it]  1%|          | 2/285 [00:34<1:20:57, 17.17s/it]  1%|          | 3/285 [00:52<1:22:06, 17.47s/it]  1%|▏         | 4/285 [01:10<1:22:39, 17.65s/it]  2%|▏         | 5/285 [01:28<1:22:45, 17.73s/it]  2%|▏         | 6/285 [01:45<1:22:44, 17.79s/it]  2%|▏         | 7/285 [02:02<1:20:17, 17.33s/it]  3%|▎         | 8/285 [02:18<1:18:57, 17.10s/it]  3%|▎         | 9/285 [02:35<1:18:09, 16.99s/it]  4%|▎         | 10/285 [02:53<1:19:23, 17.32s/it]  4%|▍         | 11/285 [03:10<1:18:39, 17.23s/it]  4%|▍         | 12/285 [03:29<1:20:38, 17.72s/it]  5%|▍         | 13/285 [03:47<1:19:58, 17.64s/it]  5%|▍         | 14/285 [04:04<1:19:26, 17.59s/it]  5%|▌         | 15/285 [04:19<1:15:17, 16.73s/it]  6%|▌         | 16/285 [04:34<1:12:39, 16.21s/it]  6%|▌         | 17/285 [04:51<1:14:17, 16.63s/it]  6%|▋         | 18/285 [05:08<1:13:37, 16.55s/it]  7%|▋         | 19/285 [05:24<1:13:24, 16.56s/it]  7%|▋         | 20/285 [05:41<1:12:59, 16.53s/it]  7%|▋         | 21/285 [05:57<1:12:34, 16.50s/it]  8%|▊         | 22/285 [06:13<1:11:27, 16.30s/it]  8%|▊         | 23/285 [06:28<1:09:15, 15.86s/it]  8%|▊         | 24/285 [06:43<1:07:46, 15.58s/it]  9%|▉         | 25/285 [06:58<1:07:41, 15.62s/it]  9%|▉         | 26/285 [07:15<1:08:26, 15.85s/it]  9%|▉         | 27/285 [07:31<1:08:31, 15.94s/it] 10%|▉         | 28/285 [07:47<1:08:39, 16.03s/it] 10%|█         | 29/285 [08:03<1:07:35, 15.84s/it] 11%|█         | 30/285 [08:19<1:07:57, 15.99s/it] 11%|█         | 31/285 [08:35<1:07:53, 16.04s/it] 11%|█         | 32/285 [08:54<1:11:17, 16.91s/it] 12%|█▏        | 33/285 [09:11<1:10:29, 16.78s/it] 12%|█▏        | 34/285 [09:27<1:09:31, 16.62s/it] 12%|█▏        | 35/285 [09:44<1:10:33, 16.93s/it] 13%|█▎        | 36/285 [10:00<1:08:10, 16.43s/it] 13%|█▎        | 37/285 [10:17<1:08:28, 16.57s/it] 13%|█▎        | 38/285 [10:31<1:06:07, 16.06s/it] 14%|█▎        | 39/285 [10:48<1:06:11, 16.14s/it] 14%|█▍        | 40/285 [11:05<1:07:19, 16.49s/it] 14%|█▍        | 41/285 [11:22<1:07:10, 16.52s/it] 15%|█▍        | 42/285 [11:39<1:08:14, 16.85s/it] 15%|█▌        | 43/285 [11:58<1:09:49, 17.31s/it] 15%|█▌        | 44/285 [12:15<1:09:08, 17.21s/it] 16%|█▌        | 45/285 [12:31<1:08:06, 17.03s/it] 16%|█▌        | 46/285 [12:51<1:11:03, 17.84s/it] 16%|█▋        | 47/285 [13:08<1:10:07, 17.68s/it] 17%|█▋        | 48/285 [13:27<1:11:30, 18.10s/it] 17%|█▋        | 49/285 [13:45<1:10:45, 17.99s/it] 18%|█▊        | 50/285 [14:03<1:10:47, 18.08s/it] 18%|█▊        | 51/285 [14:22<1:11:39, 18.37s/it] 18%|█▊        | 52/285 [14:40<1:10:27, 18.15s/it] 19%|█▊        | 53/285 [14:58<1:09:24, 17.95s/it] 19%|█▉        | 54/285 [15:15<1:08:34, 17.81s/it] 19%|█▉        | 55/285 [15:33<1:08:18, 17.82s/it] 20%|█▉        | 56/285 [15:50<1:07:22, 17.65s/it] 20%|██        | 57/285 [16:07<1:06:35, 17.53s/it] 20%|██        | 58/285 [16:39<1:22:27, 21.79s/it] 21%|██        | 59/285 [16:57<1:18:01, 20.71s/it] 21%|██        | 60/285 [17:15<1:14:35, 19.89s/it] 21%|██▏       | 61/285 [17:33<1:11:32, 19.16s/it] 22%|██▏       | 62/285 [17:50<1:08:43, 18.49s/it] 22%|██▏       | 63/285 [18:08<1:07:45, 18.31s/it] 22%|██▏       | 64/285 [18:26<1:07:36, 18.35s/it] 23%|██▎       | 65/285 [18:44<1:07:01, 18.28s/it] 23%|██▎       | 66/285 [19:01<1:05:26, 17.93s/it] 24%|██▎       | 67/285 [19:19<1:05:13, 17.95s/it] 24%|██▍       | 68/285 [19:38<1:05:56, 18.23s/it] 24%|██▍       | 69/285 [19:54<1:03:13, 17.56s/it] 25%|██▍       | 70/285 [20:11<1:02:23, 17.41s/it] 25%|██▍       | 71/285 [20:29<1:02:42, 17.58s/it] 25%|██▌       | 72/285 [20:46<1:01:52, 17.43s/it] 26%|██▌       | 73/285 [21:01<58:51, 16.66s/it]   26%|██▌       | 74/285 [21:17<58:03, 16.51s/it] 26%|██▋       | 75/285 [21:35<59:15, 16.93s/it] 27%|██▋       | 76/285 [21:54<1:00:39, 17.41s/it] 27%|██▋       | 77/285 [22:11<59:41, 17.22s/it]   27%|██▋       | 78/285 [22:28<59:15, 17.18s/it] 28%|██▊       | 79/285 [22:46<1:00:15, 17.55s/it] 28%|██▊       | 80/285 [23:04<1:00:27, 17.69s/it] 28%|██▊       | 81/285 [23:21<59:15, 17.43s/it]   29%|██▉       | 82/285 [23:38<58:55, 17.42s/it] 29%|██▉       | 83/285 [23:56<58:28, 17.37s/it] 29%|██▉       | 84/285 [24:13<58:15, 17.39s/it] 30%|██▉       | 85/285 [24:29<56:12, 16.86s/it] 30%|███       | 86/285 [24:47<57:00, 17.19s/it] 31%|███       | 87/285 [25:03<55:52, 16.93s/it] 31%|███       | 88/285 [25:21<56:25, 17.18s/it] 31%|███       | 89/285 [25:38<56:00, 17.15s/it] 32%|███▏      | 90/285 [25:56<56:49, 17.49s/it] 32%|███▏      | 91/285 [26:14<56:42, 17.54s/it] 32%|███▏      | 92/285 [26:30<55:19, 17.20s/it] 33%|███▎      | 93/285 [26:47<55:07, 17.23s/it] 33%|███▎      | 94/285 [27:05<55:21, 17.39s/it] 33%|███▎      | 95/285 [27:22<54:32, 17.22s/it] 34%|███▎      | 96/285 [27:39<54:09, 17.20s/it] 34%|███▍      | 97/285 [27:57<54:16, 17.32s/it] 34%|███▍      | 98/285 [28:14<53:51, 17.28s/it] 35%|███▍      | 99/285 [28:30<52:31, 16.94s/it] 35%|███▌      | 100/285 [28:47<52:11, 16.92s/it] 35%|███▌      | 101/285 [29:06<53:37, 17.49s/it] 36%|███▌      | 102/285 [29:22<52:40, 17.27s/it] 36%|███▌      | 103/285 [29:39<52:06, 17.18s/it] 36%|███▋      | 104/285 [29:57<52:21, 17.36s/it] 37%|███▋      | 105/285 [30:14<51:46, 17.26s/it] 37%|███▋      | 106/285 [30:33<52:33, 17.62s/it] 38%|███▊      | 107/285 [30:51<52:42, 17.77s/it] 38%|███▊      | 108/285 [31:08<52:18, 17.73s/it] 38%|███▊      | 109/285 [31:26<51:25, 17.53s/it] 39%|███▊      | 110/285 [31:43<51:20, 17.61s/it] 39%|███▉      | 111/285 [32:01<51:16, 17.68s/it] 39%|███▉      | 112/285 [32:19<51:25, 17.84s/it] 40%|███▉      | 113/285 [32:36<50:13, 17.52s/it] 40%|████      | 114/285 [32:55<50:46, 17.81s/it] 40%|████      | 115/285 [33:25<1:01:25, 21.68s/it] 41%|████      | 116/285 [33:42<56:36, 20.10s/it]   41%|████      | 117/285 [33:58<52:55, 18.90s/it] 41%|████▏     | 118/285 [34:13<49:20, 17.73s/it] 42%|████▏     | 119/285 [34:28<47:04, 17.01s/it] 42%|████▏     | 120/285 [34:45<46:56, 17.07s/it] 42%|████▏     | 121/285 [35:03<46:50, 17.14s/it] 43%|████▎     | 122/285 [35:20<46:18, 17.04s/it] 43%|████▎     | 123/285 [35:38<47:08, 17.46s/it] 44%|████▎     | 124/285 [35:55<46:34, 17.36s/it] 44%|████▍     | 125/285 [36:13<46:44, 17.53s/it] 44%|████▍     | 126/285 [36:31<47:04, 17.77s/it] 45%|████▍     | 127/285 [36:49<46:50, 17.79s/it] 45%|████▍     | 128/285 [37:07<46:44, 17.87s/it] 45%|████▌     | 129/285 [37:24<45:56, 17.67s/it] 46%|████▌     | 130/285 [37:42<45:12, 17.50s/it] 46%|████▌     | 131/285 [38:00<45:42, 17.81s/it] 46%|████▋     | 132/285 [38:18<45:22, 17.79s/it] 47%|████▋     | 133/285 [38:35<44:54, 17.73s/it] 47%|████▋     | 134/285 [38:54<45:10, 17.95s/it] 47%|████▋     | 135/285 [39:11<43:58, 17.59s/it] 48%|████▊     | 136/285 [39:28<43:14, 17.41s/it] 48%|████▊     | 137/285 [39:45<43:17, 17.55s/it] 48%|████▊     | 138/285 [40:04<43:26, 17.73s/it] 49%|████▉     | 139/285 [40:21<42:46, 17.58s/it] 49%|████▉     | 140/285 [40:38<42:29, 17.58s/it] 49%|████▉     | 141/285 [40:56<42:26, 17.68s/it] 50%|████▉     | 142/285 [41:14<42:10, 17.69s/it] 50%|█████     | 143/285 [41:30<40:45, 17.22s/it] 51%|█████     | 144/285 [41:49<41:35, 17.70s/it] 51%|█████     | 145/285 [42:07<41:50, 17.93s/it] 51%|█████     | 146/285 [42:24<40:52, 17.65s/it] 52%|█████▏    | 147/285 [42:41<39:44, 17.28s/it] 52%|█████▏    | 148/285 [42:58<39:20, 17.23s/it] 52%|█████▏    | 149/285 [43:16<39:48, 17.57s/it] 53%|█████▎    | 150/285 [43:35<39:56, 17.75s/it] 53%|█████▎    | 151/285 [43:50<38:26, 17.21s/it] 53%|█████▎    | 152/285 [44:07<37:54, 17.10s/it] 54%|█████▎    | 153/285 [44:24<37:34, 17.08s/it] 54%|█████▍    | 154/285 [44:43<38:02, 17.42s/it] 54%|█████▍    | 155/285 [45:01<38:05, 17.58s/it] 55%|█████▍    | 156/285 [45:18<37:45, 17.56s/it] 55%|█████▌    | 157/285 [45:36<37:37, 17.64s/it] 55%|█████▌    | 158/285 [45:53<37:12, 17.58s/it] 56%|█████▌    | 159/285 [46:12<37:37, 17.92s/it] 56%|█████▌    | 160/285 [46:29<36:43, 17.63s/it] 56%|█████▋    | 161/285 [46:47<36:32, 17.68s/it] 57%|█████▋    | 162/285 [47:03<35:04, 17.11s/it] 57%|█████▋    | 163/285 [47:20<34:56, 17.18s/it] 58%|█████▊    | 164/285 [47:36<34:12, 16.96s/it] 58%|█████▊    | 165/285 [47:56<35:45, 17.88s/it] 58%|█████▊    | 166/285 [48:13<34:50, 17.56s/it] 59%|█████▊    | 167/285 [48:31<34:45, 17.67s/it] 59%|█████▉    | 168/285 [48:48<33:55, 17.40s/it] 59%|█████▉    | 169/285 [49:05<33:28, 17.31s/it] 60%|█████▉    | 170/285 [49:23<33:22, 17.41s/it] 60%|██████    | 171/285 [49:40<32:55, 17.33s/it] 60%|██████    | 172/285 [50:08<38:39, 20.53s/it] 61%|██████    | 173/285 [50:25<36:27, 19.53s/it] 61%|██████    | 174/285 [50:43<35:17, 19.08s/it] 61%|██████▏   | 175/285 [51:01<34:07, 18.62s/it] 62%|██████▏   | 176/285 [51:17<32:53, 18.11s/it] 62%|██████▏   | 177/285 [51:35<32:30, 18.06s/it] 62%|██████▏   | 178/285 [51:52<31:35, 17.72s/it] 63%|██████▎   | 179/285 [52:10<31:07, 17.61s/it] 63%|██████▎   | 180/285 [52:27<30:34, 17.47s/it] 64%|██████▎   | 181/285 [52:46<31:11, 18.00s/it] 64%|██████▍   | 182/285 [53:04<30:38, 17.85s/it] 64%|██████▍   | 183/285 [53:21<30:11, 17.76s/it] 65%|██████▍   | 184/285 [53:38<29:18, 17.41s/it] 65%|██████▍   | 185/285 [53:57<29:45, 17.85s/it] 65%|██████▌   | 186/285 [54:15<29:41, 17.99s/it] 66%|██████▌   | 187/285 [54:33<29:18, 17.95s/it] 66%|██████▌   | 188/285 [54:50<28:50, 17.84s/it] 66%|██████▋   | 189/285 [55:08<28:41, 17.93s/it] 67%|██████▋   | 190/285 [55:25<27:46, 17.54s/it] 67%|██████▋   | 191/285 [55:43<27:40, 17.66s/it] 67%|██████▋   | 192/285 [56:01<27:21, 17.65s/it] 68%|██████▊   | 193/285 [56:17<26:38, 17.37s/it] 68%|██████▊   | 194/285 [56:34<26:05, 17.20s/it] 68%|██████▊   | 195/285 [56:53<26:28, 17.65s/it] 69%|██████▉   | 196/285 [57:09<25:42, 17.33s/it] 69%|██████▉   | 197/285 [57:27<25:39, 17.49s/it] 69%|██████▉   | 198/285 [57:45<25:17, 17.44s/it] 70%|██████▉   | 199/285 [58:02<24:45, 17.27s/it] 70%|███████   | 200/285 [58:20<24:54, 17.58s/it]                                                  70%|███████   | 200/285 [58:20<24:54, 17.58s/it] 71%|███████   | 201/285 [58:38<24:43, 17.67s/it] 71%|███████   | 202/285 [58:54<23:45, 17.18s/it] 71%|███████   | 203/285 [59:12<23:50, 17.44s/it] 72%|███████▏  | 204/285 [59:31<24:05, 17.84s/it] 72%|███████▏  | 205/285 [59:49<23:51, 17.89s/it] 72%|███████▏  | 206/285 [1:00:05<23:01, 17.49s/it] 73%|███████▎  | 207/285 [1:00:23<22:47, 17.54s/it] 73%|███████▎  | 208/285 [1:00:39<21:59, 17.14s/it] 73%|███████▎  | 209/285 [1:00:56<21:36, 17.06s/it] 74%|███████▎  | 210/285 [1:01:13<21:13, 16.98s/it] 74%|███████▍  | 211/285 [1:01:29<20:43, 16.80s/it] 74%|███████▍  | 212/285 [1:01:45<20:05, 16.51s/it] 75%|███████▍  | 213/285 [1:02:03<20:27, 17.04s/it] 75%|███████▌  | 214/285 [1:02:20<20:12, 17.08s/it] 75%|███████▌  | 215/285 [1:02:38<20:10, 17.29s/it] 76%|███████▌  | 216/285 [1:02:55<19:44, 17.17s/it] 76%|███████▌  | 217/285 [1:03:13<19:40, 17.36s/it] 76%|███████▋  | 218/285 [1:03:30<19:13, 17.22s/it] 77%|███████▋  | 219/285 [1:03:47<18:49, 17.12s/it] 77%|███████▋  | 220/285 [1:04:05<19:00, 17.55s/it] 78%|███████▊  | 221/285 [1:04:24<19:11, 17.99s/it] 78%|███████▊  | 222/285 [1:04:41<18:22, 17.50s/it] 78%|███████▊  | 223/285 [1:04:58<18:05, 17.51s/it] 79%|███████▊  | 224/285 [1:05:16<18:04, 17.77s/it] 79%|███████▉  | 225/285 [1:05:33<17:33, 17.55s/it] 79%|███████▉  | 226/285 [1:05:52<17:24, 17.70s/it] 80%|███████▉  | 227/285 [1:06:09<17:05, 17.68s/it] 80%|████████  | 228/285 [1:06:26<16:37, 17.49s/it] 80%|████████  | 229/285 [1:06:56<19:52, 21.29s/it] 81%|████████  | 230/285 [1:07:15<18:42, 20.42s/it] 81%|████████  | 231/285 [1:07:32<17:34, 19.53s/it] 81%|████████▏ | 232/285 [1:07:47<16:03, 18.18s/it] 82%|████████▏ | 233/285 [1:08:03<15:12, 17.56s/it] 82%|████████▏ | 234/285 [1:08:21<14:52, 17.50s/it] 82%|████████▏ | 235/285 [1:08:38<14:34, 17.49s/it] 83%|████████▎ | 236/285 [1:08:55<14:09, 17.34s/it] 83%|████████▎ | 237/285 [1:09:12<13:39, 17.06s/it] 84%|████████▎ | 238/285 [1:09:30<13:39, 17.44s/it] 84%|████████▍ | 239/285 [1:09:47<13:17, 17.35s/it] 84%|████████▍ | 240/285 [1:10:05<13:07, 17.50s/it] 85%|████████▍ | 241/285 [1:10:23<12:52, 17.56s/it] 85%|████████▍ | 242/285 [1:10:39<12:23, 17.29s/it] 85%|████████▌ | 243/285 [1:10:55<11:51, 16.94s/it] 86%|████████▌ | 244/285 [1:11:12<11:34, 16.93s/it] 86%|████████▌ | 245/285 [1:11:31<11:35, 17.38s/it] 86%|████████▋ | 246/285 [1:11:47<11:04, 17.03s/it] 87%|████████▋ | 247/285 [1:12:05<11:04, 17.47s/it] 87%|████████▋ | 248/285 [1:12:23<10:44, 17.41s/it] 87%|████████▋ | 249/285 [1:12:42<10:42, 17.84s/it] 88%|████████▊ | 250/285 [1:13:00<10:28, 17.94s/it] 88%|████████▊ | 251/285 [1:13:17<09:58, 17.59s/it] 88%|████████▊ | 252/285 [1:13:35<09:52, 17.94s/it] 89%|████████▉ | 253/285 [1:13:53<09:29, 17.78s/it] 89%|████████▉ | 254/285 [1:14:11<09:18, 18.02s/it] 89%|████████▉ | 255/285 [1:14:29<09:02, 18.08s/it] 90%|████████▉ | 256/285 [1:14:47<08:37, 17.85s/it] 90%|█████████ | 257/285 [1:15:04<08:11, 17.56s/it] 91%|█████████ | 258/285 [1:15:21<07:54, 17.56s/it] 91%|█████████ | 259/285 [1:15:39<07:38, 17.64s/it] 91%|█████████ | 260/285 [1:15:56<07:18, 17.54s/it] 92%|█████████▏| 261/285 [1:16:15<07:10, 17.93s/it] 92%|█████████▏| 262/285 [1:16:33<06:50, 17.87s/it] 92%|█████████▏| 263/285 [1:16:51<06:35, 17.98s/it] 93%|█████████▎| 264/285 [1:17:09<06:17, 17.96s/it] 93%|█████████▎| 265/285 [1:17:26<05:50, 17.54s/it] 93%|█████████▎| 266/285 [1:17:43<05:32, 17.50s/it] 94%|█████████▎| 267/285 [1:18:01<05:17, 17.66s/it] 94%|█████████▍| 268/285 [1:18:19<04:59, 17.61s/it] 94%|█████████▍| 269/285 [1:18:37<04:44, 17.78s/it] 95%|█████████▍| 270/285 [1:18:54<04:25, 17.68s/it] 95%|█████████▌| 271/285 [1:19:13<04:10, 17.90s/it] 95%|█████████▌| 272/285 [1:19:31<03:54, 18.05s/it] 96%|█████████▌| 273/285 [1:19:49<03:37, 18.11s/it] 96%|█████████▌| 274/285 [1:20:07<03:18, 18.08s/it] 96%|█████████▋| 275/285 [1:20:25<02:58, 17.83s/it] 97%|█████████▋| 276/285 [1:20:41<02:37, 17.51s/it] 97%|█████████▋| 277/285 [1:20:59<02:20, 17.53s/it] 98%|█████████▊| 278/285 [1:21:16<02:02, 17.43s/it] 98%|█████████▊| 279/285 [1:21:34<01:44, 17.46s/it] 98%|█████████▊| 280/285 [1:21:52<01:29, 17.82s/it] 99%|█████████▊| 281/285 [1:22:10<01:10, 17.67s/it] 99%|█████████▉| 282/285 [1:22:28<00:53, 17.75s/it] 99%|█████████▉| 283/285 [1:22:45<00:35, 17.69s/it]100%|█████████▉| 284/285 [1:23:04<00:17, 17.92s/it]100%|██████████| 285/285 [1:23:20<00:00, 17.58s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 285/285 [1:23:20<00:00, 17.58s/it]100%|██████████| 285/285 [1:23:20<00:00, 17.55s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'loss': 0.397, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 5000.7918, 'train_samples_per_second': 3.693, 'train_steps_per_second': 0.057, 'train_loss': 0.291146407211036, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:35, 11.69s/it]  5%|▌         | 2/40 [00:23<07:16, 11.48s/it]  8%|▊         | 3/40 [00:34<07:06, 11.53s/it] 10%|█         | 4/40 [00:46<06:59, 11.66s/it] 12%|█▎        | 5/40 [00:58<06:56, 11.89s/it] 15%|█▌        | 6/40 [01:10<06:39, 11.75s/it] 18%|█▊        | 7/40 [01:21<06:21, 11.55s/it] 20%|██        | 8/40 [01:33<06:14, 11.71s/it] 22%|██▎       | 9/40 [01:53<07:23, 14.31s/it] 25%|██▌       | 10/40 [02:04<06:41, 13.40s/it] 28%|██▊       | 11/40 [02:16<06:17, 13.01s/it] 30%|███       | 12/40 [02:28<05:54, 12.65s/it] 32%|███▎      | 13/40 [02:40<05:30, 12.23s/it] 35%|███▌      | 14/40 [02:52<05:19, 12.27s/it] 38%|███▊      | 15/40 [03:04<05:03, 12.16s/it] 40%|████      | 16/40 [03:16<04:50, 12.11s/it] 42%|████▎     | 17/40 [03:36<05:34, 14.52s/it] 45%|████▌     | 18/40 [03:47<04:59, 13.63s/it] 48%|████▊     | 19/40 [03:59<04:33, 13.04s/it] 50%|█████     | 20/40 [04:11<04:11, 12.56s/it] 52%|█████▎    | 21/40 [04:22<03:53, 12.29s/it] 55%|█████▌    | 22/40 [04:35<03:43, 12.42s/it] 57%|█████▊    | 23/40 [04:47<03:28, 12.28s/it] 60%|██████    | 24/40 [04:58<03:10, 11.92s/it] 62%|██████▎   | 25/40 [05:18<03:34, 14.30s/it] 65%|██████▌   | 26/40 [05:29<03:08, 13.49s/it] 68%|██████▊   | 27/40 [05:41<02:47, 12.90s/it] 70%|███████   | 28/40 [05:53<02:30, 12.57s/it] 72%|███████▎  | 29/40 [06:04<02:14, 12.26s/it] 75%|███████▌  | 30/40 [06:16<02:01, 12.13s/it] 78%|███████▊  | 31/40 [06:27<01:46, 11.86s/it] 80%|████████  | 32/40 [06:39<01:35, 11.88s/it] 82%|████████▎ | 33/40 [06:59<01:39, 14.26s/it] 85%|████████▌ | 34/40 [07:11<01:21, 13.65s/it] 88%|████████▊ | 35/40 [07:23<01:05, 13.17s/it] 90%|█████████ | 36/40 [07:35<00:50, 12.61s/it] 92%|█████████▎| 37/40 [07:46<00:36, 12.23s/it] 95%|█████████▌| 38/40 [07:57<00:23, 11.95s/it] 98%|█████████▊| 39/40 [08:09<00:11, 11.80s/it]100%|██████████| 40/40 [08:20<00:00, 11.67s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:20<00:00, 11.67s/it]100%|██████████| 40/40 [08:20<00:00, 12.52s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 500.6336, 'train_samples_per_second': 5.603, 'train_steps_per_second': 0.08, 'train_loss': 0.7463093757629394, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:29, 11.53s/it]  5%|▌         | 2/40 [00:22<07:13, 11.40s/it]  8%|▊         | 3/40 [00:34<07:04, 11.48s/it] 10%|█         | 4/40 [00:46<06:55, 11.55s/it] 12%|█▎        | 5/40 [00:58<06:55, 11.87s/it] 15%|█▌        | 6/40 [01:09<06:37, 11.70s/it] 18%|█▊        | 7/40 [01:20<06:17, 11.43s/it] 20%|██        | 8/40 [01:32<06:11, 11.60s/it] 22%|██▎       | 9/40 [01:52<07:22, 14.27s/it] 25%|██▌       | 10/40 [02:04<06:41, 13.39s/it] 28%|██▊       | 11/40 [02:16<06:17, 13.01s/it] 30%|███       | 12/40 [02:28<05:53, 12.62s/it] 32%|███▎      | 13/40 [02:39<05:31, 12.26s/it] 35%|███▌      | 14/40 [02:51<05:17, 12.23s/it] 38%|███▊      | 15/40 [03:03<05:01, 12.07s/it] 40%|████      | 16/40 [03:15<04:47, 11.96s/it] 42%|████▎     | 17/40 [03:35<05:32, 14.46s/it] 45%|████▌     | 18/40 [03:46<04:58, 13.58s/it] 48%|████▊     | 19/40 [03:58<04:33, 13.01s/it] 50%|█████     | 20/40 [04:09<04:09, 12.50s/it] 52%|█████▎    | 21/40 [04:21<03:52, 12.25s/it] 55%|█████▌    | 22/40 [04:34<03:43, 12.40s/it] 57%|█████▊    | 23/40 [04:46<03:28, 12.29s/it] 60%|██████    | 24/40 [04:57<03:11, 11.97s/it] 62%|██████▎   | 25/40 [05:17<03:35, 14.34s/it] 65%|██████▌   | 26/40 [05:28<03:08, 13.45s/it] 68%|██████▊   | 27/40 [05:40<02:47, 12.87s/it] 70%|███████   | 28/40 [05:52<02:30, 12.52s/it] 72%|███████▎  | 29/40 [06:03<02:14, 12.21s/it] 75%|███████▌  | 30/40 [06:15<02:00, 12.03s/it] 78%|███████▊  | 31/40 [06:26<01:46, 11.88s/it] 80%|████████  | 32/40 [06:38<01:35, 11.98s/it] 82%|████████▎ | 33/40 [06:58<01:39, 14.27s/it] 85%|████████▌ | 34/40 [07:10<01:21, 13.64s/it] 88%|████████▊ | 35/40 [07:22<01:05, 13.14s/it] 90%|█████████ | 36/40 [07:34<00:50, 12.60s/it] 92%|█████████▎| 37/40 [07:45<00:36, 12.28s/it] 95%|█████████▌| 38/40 [07:56<00:23, 11.95s/it] 98%|█████████▊| 39/40 [08:08<00:11, 11.84s/it]100%|██████████| 40/40 [08:20<00:00, 11.83s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:20<00:00, 11.83s/it]100%|██████████| 40/40 [08:20<00:00, 12.50s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 500.1253, 'train_samples_per_second': 5.609, 'train_steps_per_second': 0.08, 'train_loss': 0.7505879402160645, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:07, 17.12s/it]  5%|▌         | 2/40 [00:33<10:38, 16.81s/it]  8%|▊         | 3/40 [00:50<10:22, 16.83s/it] 10%|█         | 4/40 [01:07<10:10, 16.95s/it] 12%|█▎        | 5/40 [01:26<10:16, 17.63s/it] 15%|█▌        | 6/40 [01:43<09:49, 17.33s/it] 18%|█▊        | 7/40 [01:58<09:13, 16.77s/it] 20%|██        | 8/40 [02:16<09:05, 17.05s/it] 22%|██▎       | 9/40 [02:46<10:49, 20.94s/it] 25%|██▌       | 10/40 [03:02<09:50, 19.67s/it] 28%|██▊       | 11/40 [03:20<09:16, 19.19s/it] 30%|███       | 12/40 [03:38<08:40, 18.58s/it] 32%|███▎      | 13/40 [03:54<08:06, 18.01s/it] 35%|███▌      | 14/40 [04:12<07:49, 18.04s/it] 38%|███▊      | 15/40 [04:30<07:25, 17.83s/it] 40%|████      | 16/40 [04:47<07:03, 17.63s/it] 42%|████▎     | 17/40 [05:17<08:08, 21.25s/it] 45%|████▌     | 18/40 [05:34<07:18, 19.94s/it] 48%|████▊     | 19/40 [05:51<06:44, 19.28s/it] 50%|█████     | 20/40 [06:08<06:11, 18.60s/it] 52%|█████▎    | 21/40 [06:25<05:41, 17.97s/it] 55%|█████▌    | 22/40 [06:43<05:26, 18.11s/it] 57%|█████▊    | 23/40 [07:01<05:06, 18.00s/it] 60%|██████    | 24/40 [07:17<04:39, 17.48s/it] 62%|██████▎   | 25/40 [07:46<05:13, 20.91s/it] 65%|██████▌   | 26/40 [08:03<04:36, 19.77s/it] 68%|██████▊   | 27/40 [08:20<04:05, 18.92s/it] 70%|███████   | 28/40 [08:37<03:39, 18.27s/it] 72%|███████▎  | 29/40 [08:54<03:17, 17.95s/it] 75%|███████▌  | 30/40 [09:11<02:57, 17.71s/it] 78%|███████▊  | 31/40 [09:28<02:36, 17.35s/it] 80%|████████  | 32/40 [09:46<02:21, 17.67s/it] 82%|████████▎ | 33/40 [10:15<02:26, 20.99s/it] 85%|████████▌ | 34/40 [10:33<02:01, 20.22s/it] 88%|████████▊ | 35/40 [10:51<01:37, 19.51s/it] 90%|█████████ | 36/40 [11:08<01:14, 18.57s/it] 92%|█████████▎| 37/40 [11:24<00:54, 18.03s/it] 95%|█████████▌| 38/40 [11:41<00:34, 17.49s/it] 98%|█████████▊| 39/40 [11:58<00:17, 17.37s/it]100%|██████████| 40/40 [12:14<00:00, 17.11s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:14<00:00, 17.11s/it]100%|██████████| 40/40 [12:14<00:00, 18.37s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 734.7189, 'train_samples_per_second': 3.818, 'train_steps_per_second': 0.054, 'train_loss': 0.7536416530609131, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:40, 17.97s/it]  5%|▌         | 2/40 [00:35<11:13, 17.73s/it]  8%|▊         | 3/40 [00:53<10:58, 17.79s/it] 10%|█         | 4/40 [01:12<10:53, 18.15s/it] 12%|█▎        | 5/40 [01:31<10:54, 18.70s/it] 15%|█▌        | 6/40 [01:49<10:25, 18.38s/it] 18%|█▊        | 7/40 [02:06<09:49, 17.86s/it] 20%|██        | 8/40 [02:24<09:38, 18.07s/it] 22%|██▎       | 9/40 [02:55<11:24, 22.08s/it] 25%|██▌       | 10/40 [03:13<10:24, 20.81s/it] 28%|██▊       | 11/40 [03:32<09:48, 20.29s/it] 30%|███       | 12/40 [03:51<09:11, 19.71s/it] 32%|███▎      | 13/40 [04:08<08:36, 19.12s/it] 35%|███▌      | 14/40 [04:28<08:17, 19.12s/it] 38%|███▊      | 15/40 [04:46<07:51, 18.88s/it] 40%|████      | 16/40 [05:04<07:27, 18.63s/it] 42%|████▎     | 17/40 [05:36<08:38, 22.55s/it] 45%|████▌     | 18/40 [05:53<07:45, 21.14s/it] 48%|████▊     | 19/40 [06:12<07:07, 20.35s/it] 50%|█████     | 20/40 [06:30<06:31, 19.56s/it] 52%|█████▎    | 21/40 [06:47<06:00, 18.99s/it] 55%|█████▌    | 22/40 [07:07<05:45, 19.19s/it] 57%|█████▊    | 23/40 [07:26<05:24, 19.08s/it] 60%|██████    | 24/40 [07:43<04:57, 18.60s/it] 62%|██████▎   | 25/40 [08:14<05:32, 22.18s/it] 65%|██████▌   | 26/40 [08:32<04:52, 20.92s/it] 68%|██████▊   | 27/40 [08:50<04:20, 20.02s/it] 70%|███████   | 28/40 [09:08<03:53, 19.42s/it] 72%|███████▎  | 29/40 [09:26<03:29, 19.07s/it] 75%|███████▌  | 30/40 [09:44<03:08, 18.80s/it] 78%|███████▊  | 31/40 [10:02<02:46, 18.50s/it] 80%|████████  | 32/40 [10:21<02:29, 18.74s/it] 82%|████████▎ | 33/40 [10:52<02:35, 22.27s/it] 85%|████████▌ | 34/40 [11:11<02:08, 21.40s/it] 88%|████████▊ | 35/40 [11:30<01:43, 20.75s/it] 90%|█████████ | 36/40 [11:48<01:19, 19.86s/it] 92%|█████████▎| 37/40 [12:06<00:57, 19.18s/it] 95%|█████████▌| 38/40 [12:23<00:37, 18.68s/it] 98%|█████████▊| 39/40 [12:41<00:18, 18.53s/it]100%|██████████| 40/40 [12:59<00:00, 18.28s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:59<00:00, 18.28s/it]100%|██████████| 40/40 [12:59<00:00, 19.49s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 779.6817, 'train_samples_per_second': 3.598, 'train_steps_per_second': 0.051, 'train_loss': 0.7801527976989746, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:32, 11.61s/it]  5%|▌         | 2/40 [00:24<07:40, 12.13s/it]  8%|▊         | 3/40 [00:36<07:26, 12.07s/it] 10%|█         | 4/40 [00:48<07:15, 12.09s/it] 12%|█▎        | 5/40 [01:01<07:17, 12.49s/it] 15%|█▌        | 6/40 [01:13<06:59, 12.35s/it] 18%|█▊        | 7/40 [01:25<06:41, 12.17s/it] 20%|██        | 8/40 [01:38<06:35, 12.35s/it] 22%|██▎       | 9/40 [01:52<06:41, 12.94s/it] 25%|██▌       | 10/40 [02:04<06:18, 12.60s/it] 28%|██▊       | 11/40 [02:16<06:00, 12.45s/it] 30%|███       | 12/40 [02:27<05:42, 12.23s/it] 32%|███▎      | 13/40 [02:40<05:33, 12.33s/it] 35%|███▌      | 14/40 [02:52<05:20, 12.32s/it] 38%|███▊      | 15/40 [03:05<05:11, 12.46s/it] 40%|████      | 16/40 [03:18<05:03, 12.64s/it] 42%|████▎     | 17/40 [03:32<04:59, 13.03s/it] 45%|████▌     | 18/40 [03:45<04:44, 12.91s/it] 48%|████▊     | 19/40 [03:57<04:27, 12.72s/it] 50%|█████     | 20/40 [04:09<04:10, 12.54s/it] 52%|█████▎    | 21/40 [04:22<03:57, 12.50s/it] 55%|█████▌    | 22/40 [04:34<03:43, 12.40s/it] 57%|█████▊    | 23/40 [04:46<03:28, 12.29s/it] 60%|██████    | 24/40 [04:59<03:20, 12.55s/it] 62%|██████▎   | 25/40 [05:13<03:15, 13.04s/it] 65%|██████▌   | 26/40 [05:26<03:00, 12.91s/it] 68%|██████▊   | 27/40 [05:38<02:46, 12.82s/it] 70%|███████   | 28/40 [05:50<02:29, 12.44s/it] 72%|███████▎  | 29/40 [06:02<02:16, 12.44s/it] 75%|███████▌  | 30/40 [06:14<02:03, 12.34s/it] 78%|███████▊  | 31/40 [06:27<01:51, 12.42s/it] 80%|████████  | 32/40 [06:40<01:39, 12.50s/it] 82%|████████▎ | 33/40 [06:54<01:31, 13.07s/it] 85%|████████▌ | 34/40 [07:07<01:18, 13.01s/it] 88%|████████▊ | 35/40 [07:20<01:04, 12.96s/it] 90%|█████████ | 36/40 [07:32<00:51, 12.76s/it] 92%|█████████▎| 37/40 [07:44<00:37, 12.60s/it] 95%|█████████▌| 38/40 [07:57<00:25, 12.67s/it] 98%|█████████▊| 39/40 [08:09<00:12, 12.50s/it]100%|██████████| 40/40 [08:20<00:00, 11.84s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:20<00:00, 11.84s/it]100%|██████████| 40/40 [08:20<00:00, 12.50s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 500.0537, 'train_samples_per_second': 5.209, 'train_steps_per_second': 0.08, 'train_loss': 0.6989864349365235, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:58, 10.74s/it]  5%|▌         | 2/40 [00:23<07:30, 11.84s/it]  8%|▊         | 3/40 [00:35<07:24, 12.01s/it] 10%|█         | 4/40 [00:47<07:13, 12.05s/it] 12%|█▎        | 5/40 [01:01<07:20, 12.58s/it] 15%|█▌        | 6/40 [01:13<07:02, 12.42s/it] 18%|█▊        | 7/40 [01:25<06:44, 12.26s/it] 20%|██        | 8/40 [01:37<06:37, 12.41s/it] 22%|██▎       | 9/40 [01:52<06:42, 12.97s/it] 25%|██▌       | 10/40 [02:04<06:19, 12.66s/it] 28%|██▊       | 11/40 [02:16<06:02, 12.52s/it] 30%|███       | 12/40 [02:28<05:44, 12.32s/it] 32%|███▎      | 13/40 [02:40<05:33, 12.34s/it] 35%|███▌      | 14/40 [02:52<05:19, 12.30s/it] 38%|███▊      | 15/40 [03:05<05:10, 12.44s/it] 40%|████      | 16/40 [03:18<05:01, 12.58s/it] 42%|████▎     | 17/40 [03:32<04:59, 13.01s/it] 45%|████▌     | 18/40 [03:45<04:44, 12.94s/it] 48%|████▊     | 19/40 [03:57<04:26, 12.70s/it] 50%|█████     | 20/40 [04:09<04:11, 12.59s/it] 52%|█████▎    | 21/40 [04:22<03:58, 12.54s/it] 55%|█████▌    | 22/40 [04:34<03:44, 12.49s/it] 57%|█████▊    | 23/40 [04:46<03:30, 12.37s/it] 60%|██████    | 24/40 [04:59<03:22, 12.64s/it] 62%|██████▎   | 25/40 [05:14<03:16, 13.13s/it] 65%|██████▌   | 26/40 [05:26<03:00, 12.89s/it] 68%|██████▊   | 27/40 [05:38<02:44, 12.64s/it] 70%|███████   | 28/40 [05:50<02:28, 12.35s/it] 72%|███████▎  | 29/40 [06:02<02:16, 12.45s/it] 75%|███████▌  | 30/40 [06:15<02:03, 12.38s/it] 78%|███████▊  | 31/40 [06:27<01:51, 12.40s/it] 80%|████████  | 32/40 [06:40<01:40, 12.50s/it] 82%|████████▎ | 33/40 [06:54<01:31, 13.11s/it] 85%|████████▌ | 34/40 [07:07<01:18, 13.03s/it] 88%|████████▊ | 35/40 [07:20<01:04, 13.00s/it] 90%|█████████ | 36/40 [07:32<00:50, 12.73s/it] 92%|█████████▎| 37/40 [07:45<00:37, 12.60s/it] 95%|█████████▌| 38/40 [07:57<00:25, 12.70s/it] 98%|█████████▊| 39/40 [08:10<00:12, 12.56s/it]100%|██████████| 40/40 [08:21<00:00, 12.22s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:21<00:00, 12.22s/it]100%|██████████| 40/40 [08:21<00:00, 12.54s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 501.6286, 'train_samples_per_second': 5.193, 'train_steps_per_second': 0.08, 'train_loss': 0.7157849788665771, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:14, 17.30s/it]  5%|▌         | 2/40 [00:36<11:31, 18.20s/it]  8%|▊         | 3/40 [00:54<11:09, 18.08s/it] 10%|█         | 4/40 [01:11<10:45, 17.93s/it] 12%|█▎        | 5/40 [01:31<10:54, 18.69s/it] 15%|█▌        | 6/40 [01:49<10:25, 18.41s/it] 18%|█▊        | 7/40 [02:08<10:07, 18.40s/it] 20%|██        | 8/40 [02:27<09:58, 18.70s/it] 22%|██▎       | 9/40 [02:48<10:06, 19.58s/it] 25%|██▌       | 10/40 [03:06<09:32, 19.10s/it] 28%|██▊       | 11/40 [03:25<09:05, 18.82s/it] 30%|███       | 12/40 [03:43<08:39, 18.54s/it] 32%|███▎      | 13/40 [04:01<08:20, 18.53s/it] 35%|███▌      | 14/40 [04:19<07:56, 18.34s/it] 38%|███▊      | 15/40 [04:38<07:43, 18.52s/it] 40%|████      | 16/40 [04:57<07:29, 18.74s/it] 42%|████▎     | 17/40 [05:18<07:24, 19.31s/it] 45%|████▌     | 18/40 [05:36<06:59, 19.06s/it] 48%|████▊     | 19/40 [05:55<06:37, 18.92s/it] 50%|█████     | 20/40 [06:13<06:15, 18.76s/it] 52%|█████▎    | 21/40 [06:32<05:55, 18.73s/it] 55%|█████▌    | 22/40 [06:50<05:33, 18.51s/it] 57%|█████▊    | 23/40 [07:08<05:12, 18.36s/it] 60%|██████    | 24/40 [07:28<05:01, 18.86s/it] 62%|██████▎   | 25/40 [07:49<04:53, 19.57s/it] 65%|██████▌   | 26/40 [08:07<04:28, 19.15s/it] 68%|██████▊   | 27/40 [08:26<04:07, 19.01s/it] 70%|███████   | 28/40 [08:44<03:42, 18.57s/it] 72%|███████▎  | 29/40 [09:02<03:24, 18.62s/it] 75%|███████▌  | 30/40 [09:20<03:04, 18.43s/it] 78%|███████▊  | 31/40 [09:39<02:46, 18.52s/it] 80%|████████  | 32/40 [09:57<02:26, 18.36s/it] 82%|████████▎ | 33/40 [10:18<02:14, 19.16s/it] 85%|████████▌ | 34/40 [10:38<01:55, 19.30s/it] 88%|████████▊ | 35/40 [10:57<01:36, 19.22s/it] 90%|█████████ | 36/40 [11:15<01:15, 18.84s/it] 92%|█████████▎| 37/40 [11:32<00:55, 18.55s/it] 95%|█████████▌| 38/40 [11:52<00:37, 18.70s/it] 98%|█████████▊| 39/40 [12:10<00:18, 18.60s/it]100%|██████████| 40/40 [12:27<00:00, 18.01s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:27<00:00, 18.01s/it]100%|██████████| 40/40 [12:27<00:00, 18.68s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 747.0324, 'train_samples_per_second': 3.487, 'train_steps_per_second': 0.054, 'train_loss': 0.694802188873291, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:57, 18.40s/it]  5%|▌         | 2/40 [00:38<12:19, 19.45s/it]  8%|▊         | 3/40 [00:57<11:52, 19.26s/it] 10%|█         | 4/40 [01:16<11:27, 19.09s/it] 12%|█▎        | 5/40 [01:37<11:31, 19.76s/it] 15%|█▌        | 6/40 [01:56<11:06, 19.59s/it] 18%|█▊        | 7/40 [02:15<10:40, 19.41s/it] 20%|██        | 8/40 [02:35<10:29, 19.66s/it] 22%|██▎       | 9/40 [02:58<10:36, 20.53s/it] 25%|██▌       | 10/40 [03:17<09:59, 19.98s/it] 28%|██▊       | 11/40 [03:36<09:31, 19.71s/it] 30%|███       | 12/40 [03:54<09:01, 19.34s/it] 32%|███▎      | 13/40 [04:14<08:45, 19.46s/it] 35%|███▌      | 14/40 [04:33<08:23, 19.36s/it] 38%|███▊      | 15/40 [04:53<08:09, 19.57s/it] 40%|████      | 16/40 [05:14<07:57, 19.88s/it] 42%|████▎     | 17/40 [05:36<07:51, 20.50s/it] 45%|████▌     | 18/40 [05:55<07:23, 20.15s/it] 48%|████▊     | 19/40 [06:14<06:58, 19.92s/it] 50%|█████     | 20/40 [06:34<06:35, 19.78s/it] 52%|█████▎    | 21/40 [06:54<06:17, 19.85s/it] 55%|█████▌    | 22/40 [07:13<05:52, 19.60s/it] 57%|█████▊    | 23/40 [07:32<05:29, 19.35s/it] 60%|██████    | 24/40 [07:52<05:16, 19.77s/it] 62%|██████▎   | 25/40 [08:15<05:08, 20.55s/it] 65%|██████▌   | 26/40 [08:34<04:42, 20.15s/it] 68%|██████▊   | 27/40 [08:54<04:19, 19.98s/it] 70%|███████   | 28/40 [09:12<03:55, 19.60s/it] 72%|███████▎  | 29/40 [09:32<03:37, 19.74s/it] 75%|███████▌  | 30/40 [09:51<03:15, 19.58s/it] 78%|███████▊  | 31/40 [10:11<02:56, 19.63s/it] 80%|████████  | 32/40 [10:31<02:37, 19.67s/it] 82%|████████▎ | 33/40 [10:53<02:23, 20.47s/it] 85%|████████▌ | 34/40 [11:14<02:02, 20.48s/it] 88%|████████▊ | 35/40 [11:34<01:41, 20.29s/it] 90%|█████████ | 36/40 [11:53<01:19, 19.91s/it] 92%|█████████▎| 37/40 [12:12<00:58, 19.66s/it] 95%|█████████▌| 38/40 [12:32<00:39, 19.70s/it] 98%|█████████▊| 39/40 [12:51<00:19, 19.56s/it]100%|██████████| 40/40 [13:09<00:00, 19.08s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:09<00:00, 19.08s/it]100%|██████████| 40/40 [13:09<00:00, 19.73s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 789.2782, 'train_samples_per_second': 3.3, 'train_steps_per_second': 0.051, 'train_loss': 0.751321268081665, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<07:51, 12.08s/it]  5%|▌         | 2/40 [00:24<07:56, 12.54s/it]  8%|▊         | 3/40 [00:37<07:46, 12.60s/it] 10%|█         | 4/40 [00:50<07:34, 12.61s/it] 12%|█▎        | 5/40 [01:02<07:13, 12.39s/it] 15%|█▌        | 6/40 [01:14<07:02, 12.41s/it] 18%|█▊        | 7/40 [01:27<06:51, 12.46s/it] 20%|██        | 8/40 [01:40<06:44, 12.64s/it] 22%|██▎       | 9/40 [01:54<06:46, 13.10s/it] 25%|██▌       | 10/40 [02:06<06:27, 12.91s/it] 28%|██▊       | 11/40 [02:18<06:03, 12.52s/it] 30%|███       | 12/40 [02:31<05:51, 12.55s/it] 32%|███▎      | 13/40 [02:44<05:45, 12.79s/it] 35%|███▌      | 14/40 [02:56<05:29, 12.66s/it] 38%|███▊      | 15/40 [03:09<05:19, 12.80s/it] 40%|████      | 16/40 [03:23<05:09, 12.90s/it] 42%|████▎     | 17/40 [03:37<05:05, 13.27s/it] 45%|████▌     | 18/40 [03:49<04:48, 13.11s/it] 48%|████▊     | 19/40 [04:02<04:31, 12.91s/it] 50%|█████     | 20/40 [04:14<04:10, 12.55s/it] 52%|█████▎    | 21/40 [04:27<04:01, 12.72s/it] 55%|█████▌    | 22/40 [04:39<03:47, 12.65s/it] 57%|█████▊    | 23/40 [04:52<03:34, 12.59s/it] 60%|██████    | 24/40 [05:05<03:26, 12.91s/it] 62%|██████▎   | 25/40 [05:20<03:22, 13.48s/it] 65%|██████▌   | 26/40 [05:33<03:04, 13.16s/it] 68%|██████▊   | 27/40 [05:46<02:50, 13.14s/it] 70%|███████   | 28/40 [05:57<02:30, 12.54s/it] 72%|███████▎  | 29/40 [06:08<02:14, 12.22s/it] 75%|███████▌  | 30/40 [06:20<01:59, 11.94s/it] 78%|███████▊  | 31/40 [06:32<01:48, 12.08s/it] 80%|████████  | 32/40 [06:43<01:34, 11.81s/it] 82%|████████▎ | 33/40 [06:57<01:25, 12.28s/it] 85%|████████▌ | 34/40 [07:09<01:14, 12.45s/it] 88%|████████▊ | 35/40 [07:23<01:03, 12.66s/it] 90%|█████████ | 36/40 [07:34<00:49, 12.31s/it] 92%|█████████▎| 37/40 [07:47<00:37, 12.39s/it] 95%|█████████▌| 38/40 [07:58<00:24, 12.03s/it] 98%|█████████▊| 39/40 [08:10<00:12, 12.20s/it]100%|██████████| 40/40 [08:22<00:00, 12.16s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:22<00:00, 12.16s/it]100%|██████████| 40/40 [08:22<00:00, 12.57s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 502.9374, 'train_samples_per_second': 5.18, 'train_steps_per_second': 0.08, 'train_loss': 0.6989317417144776, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<07:54, 12.18s/it]  5%|▌         | 2/40 [00:24<07:53, 12.47s/it]  8%|▊         | 3/40 [00:37<07:46, 12.60s/it] 10%|█         | 4/40 [00:50<07:34, 12.62s/it] 12%|█▎        | 5/40 [01:02<07:15, 12.44s/it] 15%|█▌        | 6/40 [01:14<07:03, 12.45s/it] 18%|█▊        | 7/40 [01:27<06:53, 12.52s/it] 20%|██        | 8/40 [01:40<06:47, 12.73s/it] 22%|██▎       | 9/40 [01:54<06:46, 13.12s/it] 25%|██▌       | 10/40 [02:06<06:25, 12.86s/it] 28%|██▊       | 11/40 [02:18<06:00, 12.44s/it] 30%|███       | 12/40 [02:31<05:49, 12.49s/it] 32%|███▎      | 13/40 [02:44<05:44, 12.75s/it] 35%|███▌      | 14/40 [02:56<05:28, 12.62s/it] 38%|███▊      | 15/40 [03:09<05:17, 12.71s/it] 40%|████      | 16/40 [03:22<05:06, 12.78s/it] 42%|████▎     | 17/40 [03:36<05:00, 13.07s/it] 45%|████▌     | 18/40 [03:48<04:42, 12.86s/it] 48%|████▊     | 19/40 [04:01<04:29, 12.82s/it] 50%|█████     | 20/40 [04:13<04:09, 12.49s/it] 52%|█████▎    | 21/40 [04:26<04:01, 12.70s/it] 55%|█████▌    | 22/40 [04:38<03:46, 12.56s/it] 57%|█████▊    | 23/40 [04:51<03:33, 12.57s/it] 60%|██████    | 24/40 [05:04<03:26, 12.91s/it] 62%|██████▎   | 25/40 [05:19<03:19, 13.30s/it] 65%|██████▌   | 26/40 [05:31<03:02, 13.04s/it] 68%|██████▊   | 27/40 [05:44<02:49, 13.04s/it] 70%|███████   | 28/40 [05:56<02:31, 12.64s/it] 72%|███████▎  | 29/40 [06:08<02:18, 12.57s/it] 75%|███████▌  | 30/40 [06:20<02:04, 12.46s/it] 78%|███████▊  | 31/40 [06:33<01:53, 12.60s/it] 80%|████████  | 32/40 [06:45<01:38, 12.34s/it] 82%|████████▎ | 33/40 [06:58<01:27, 12.56s/it] 85%|████████▌ | 34/40 [07:11<01:15, 12.65s/it] 88%|████████▊ | 35/40 [07:24<01:04, 12.89s/it] 90%|█████████ | 36/40 [07:37<00:50, 12.72s/it] 92%|█████████▎| 37/40 [07:50<00:38, 12.89s/it] 95%|█████████▌| 38/40 [08:02<00:25, 12.51s/it] 98%|█████████▊| 39/40 [08:14<00:12, 12.49s/it]100%|██████████| 40/40 [08:25<00:00, 12.05s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:25<00:00, 12.05s/it]100%|██████████| 40/40 [08:25<00:00, 12.64s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 505.565, 'train_samples_per_second': 5.153, 'train_steps_per_second': 0.079, 'train_loss': 0.7157491207122803, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:48, 18.18s/it]  5%|▌         | 2/40 [00:36<11:44, 18.53s/it]  8%|▊         | 3/40 [00:55<11:32, 18.73s/it] 10%|█         | 4/40 [01:14<11:16, 18.79s/it] 12%|█▎        | 5/40 [01:32<10:43, 18.37s/it] 15%|█▌        | 6/40 [01:50<10:19, 18.23s/it] 18%|█▊        | 7/40 [02:09<10:07, 18.40s/it] 20%|██        | 8/40 [02:28<10:02, 18.82s/it] 22%|██▎       | 9/40 [02:49<10:04, 19.48s/it] 25%|██▌       | 10/40 [03:08<09:36, 19.20s/it] 28%|██▊       | 11/40 [03:25<08:56, 18.50s/it] 30%|███       | 12/40 [03:44<08:41, 18.62s/it] 32%|███▎      | 13/40 [04:04<08:33, 19.02s/it] 35%|███▌      | 14/40 [04:22<08:05, 18.68s/it] 38%|███▊      | 15/40 [04:40<07:48, 18.74s/it] 40%|████      | 16/40 [05:00<07:34, 18.94s/it] 42%|████▎     | 17/40 [05:20<07:24, 19.31s/it] 45%|████▌     | 18/40 [05:39<07:00, 19.09s/it] 48%|████▊     | 19/40 [05:58<06:42, 19.16s/it] 50%|█████     | 20/40 [06:15<06:13, 18.67s/it] 52%|█████▎    | 21/40 [06:35<06:00, 19.00s/it] 55%|█████▌    | 22/40 [06:53<05:35, 18.65s/it] 57%|█████▊    | 23/40 [07:12<05:16, 18.63s/it] 60%|██████    | 24/40 [07:32<05:07, 19.23s/it] 62%|██████▎   | 25/40 [07:54<04:58, 19.91s/it] 65%|██████▌   | 26/40 [08:12<04:30, 19.31s/it] 68%|██████▊   | 27/40 [08:31<04:12, 19.45s/it] 70%|███████   | 28/40 [08:49<03:45, 18.82s/it] 72%|███████▎  | 29/40 [09:07<03:26, 18.77s/it] 75%|███████▌  | 30/40 [09:26<03:06, 18.64s/it] 78%|███████▊  | 31/40 [09:46<02:50, 18.98s/it] 80%|████████  | 32/40 [10:04<02:31, 18.88s/it] 82%|████████▎ | 33/40 [10:25<02:15, 19.39s/it] 85%|████████▌ | 34/40 [10:44<01:56, 19.34s/it] 88%|████████▊ | 35/40 [11:04<01:37, 19.45s/it] 90%|█████████ | 36/40 [11:22<01:16, 19.12s/it] 92%|█████████▎| 37/40 [11:41<00:57, 19.22s/it] 95%|█████████▌| 38/40 [11:59<00:37, 18.61s/it] 98%|█████████▊| 39/40 [12:18<00:18, 18.95s/it]100%|██████████| 40/40 [12:35<00:00, 18.35s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:35<00:00, 18.35s/it]100%|██████████| 40/40 [12:35<00:00, 18.90s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 755.8563, 'train_samples_per_second': 3.446, 'train_steps_per_second': 0.053, 'train_loss': 0.6953922748565674, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:19<12:21, 19.01s/it]  5%|▌         | 2/40 [00:38<12:19, 19.46s/it]  8%|▊         | 3/40 [00:59<12:19, 19.98s/it] 10%|█         | 4/40 [01:19<11:55, 19.86s/it] 12%|█▎        | 5/40 [01:37<11:12, 19.21s/it] 15%|█▌        | 6/40 [01:56<10:49, 19.10s/it] 18%|█▊        | 7/40 [02:15<10:34, 19.24s/it] 20%|██        | 8/40 [02:36<10:29, 19.66s/it] 22%|██▎       | 9/40 [02:58<10:33, 20.43s/it] 25%|██▌       | 10/40 [03:17<10:04, 20.14s/it] 28%|██▊       | 11/40 [03:35<09:25, 19.50s/it] 30%|███       | 12/40 [03:55<09:07, 19.54s/it] 32%|███▎      | 13/40 [04:16<08:59, 19.99s/it] 35%|███▌      | 14/40 [04:35<08:32, 19.71s/it] 38%|███▊      | 15/40 [04:56<08:19, 19.99s/it] 40%|████      | 16/40 [05:16<08:02, 20.10s/it] 42%|████▎     | 17/40 [05:37<07:50, 20.48s/it] 45%|████▌     | 18/40 [05:57<07:23, 20.16s/it] 48%|████▊     | 19/40 [06:17<07:01, 20.09s/it] 50%|█████     | 20/40 [06:35<06:30, 19.55s/it] 52%|█████▎    | 21/40 [06:55<06:16, 19.80s/it] 55%|█████▌    | 22/40 [07:15<05:53, 19.64s/it] 57%|█████▊    | 23/40 [07:34<05:33, 19.64s/it] 60%|██████    | 24/40 [07:56<05:23, 20.23s/it] 62%|██████▎   | 25/40 [08:19<05:15, 21.01s/it] 65%|██████▌   | 26/40 [08:38<04:46, 20.43s/it] 68%|██████▊   | 27/40 [08:58<04:26, 20.48s/it] 70%|███████   | 28/40 [09:17<03:58, 19.89s/it] 72%|███████▎  | 29/40 [09:36<03:37, 19.80s/it] 75%|███████▌  | 30/40 [09:56<03:16, 19.67s/it] 78%|███████▊  | 31/40 [10:17<02:59, 19.99s/it] 80%|████████  | 32/40 [10:36<02:39, 19.92s/it] 82%|████████▎ | 33/40 [10:58<02:23, 20.44s/it] 85%|████████▌ | 34/40 [11:18<02:02, 20.36s/it] 88%|████████▊ | 35/40 [11:39<01:41, 20.40s/it] 90%|█████████ | 36/40 [11:58<01:20, 20.01s/it] 92%|█████████▎| 37/40 [12:18<01:00, 20.14s/it] 95%|█████████▌| 38/40 [12:36<00:39, 19.59s/it] 98%|█████████▊| 39/40 [12:57<00:19, 19.91s/it]100%|██████████| 40/40 [13:16<00:00, 19.49s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:16<00:00, 19.49s/it]100%|██████████| 40/40 [13:16<00:00, 19.90s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 796.1667, 'train_samples_per_second': 3.272, 'train_steps_per_second': 0.05, 'train_loss': 0.75123291015625, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:35, 11.68s/it]  5%|▌         | 2/40 [00:24<07:44, 12.21s/it]  8%|▊         | 3/40 [00:36<07:33, 12.26s/it] 10%|█         | 4/40 [00:48<07:20, 12.23s/it] 12%|█▎        | 5/40 [01:01<07:12, 12.36s/it] 15%|█▌        | 6/40 [01:13<06:55, 12.23s/it] 18%|█▊        | 7/40 [01:24<06:34, 11.95s/it] 20%|██        | 8/40 [01:37<06:32, 12.26s/it] 22%|██▎       | 9/40 [01:52<06:42, 12.99s/it] 25%|██▌       | 10/40 [02:03<06:16, 12.53s/it] 28%|██▊       | 11/40 [02:15<06:00, 12.45s/it] 30%|███       | 12/40 [02:27<05:41, 12.20s/it] 32%|███▎      | 13/40 [02:39<05:25, 12.04s/it] 35%|███▌      | 14/40 [02:50<05:08, 11.85s/it] 38%|███▊      | 15/40 [03:03<05:00, 12.01s/it] 40%|████      | 16/40 [03:15<04:49, 12.08s/it] 42%|████▎     | 17/40 [03:28<04:43, 12.31s/it] 45%|████▌     | 18/40 [03:40<04:31, 12.34s/it] 48%|████▊     | 19/40 [03:52<04:17, 12.25s/it] 50%|█████     | 20/40 [04:04<04:05, 12.25s/it] 52%|█████▎    | 21/40 [04:17<03:54, 12.33s/it] 55%|█████▌    | 22/40 [04:29<03:41, 12.30s/it] 57%|█████▊    | 23/40 [04:42<03:29, 12.35s/it] 60%|██████    | 24/40 [04:55<03:20, 12.52s/it] 62%|██████▎   | 25/40 [05:09<03:18, 13.22s/it] 65%|██████▌   | 26/40 [05:22<03:00, 12.90s/it] 68%|██████▊   | 27/40 [05:34<02:47, 12.88s/it] 70%|███████   | 28/40 [05:46<02:30, 12.57s/it] 72%|███████▎  | 29/40 [05:58<02:15, 12.34s/it] 75%|███████▌  | 30/40 [06:11<02:04, 12.40s/it] 78%|███████▊  | 31/40 [06:23<01:51, 12.38s/it] 80%|████████  | 32/40 [06:36<01:40, 12.52s/it] 82%|████████▎ | 33/40 [06:50<01:31, 13.07s/it] 85%|████████▌ | 34/40 [07:03<01:17, 12.98s/it] 88%|████████▊ | 35/40 [07:15<01:03, 12.74s/it] 90%|█████████ | 36/40 [07:27<00:49, 12.45s/it] 92%|█████████▎| 37/40 [07:39<00:36, 12.29s/it] 95%|█████████▌| 38/40 [07:50<00:24, 12.12s/it] 98%|█████████▊| 39/40 [08:02<00:12, 12.06s/it]100%|██████████| 40/40 [08:14<00:00, 11.98s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:14<00:00, 11.98s/it]100%|██████████| 40/40 [08:14<00:00, 12.37s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 494.6442, 'train_samples_per_second': 5.266, 'train_steps_per_second': 0.081, 'train_loss': 0.6988746643066406, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:30, 11.55s/it]  5%|▌         | 2/40 [00:24<07:47, 12.30s/it]  8%|▊         | 3/40 [00:36<07:37, 12.37s/it] 10%|█         | 4/40 [00:49<07:25, 12.38s/it] 12%|█▎        | 5/40 [01:01<07:13, 12.39s/it] 15%|█▌        | 6/40 [01:13<06:58, 12.31s/it] 18%|█▊        | 7/40 [01:25<06:38, 12.07s/it] 20%|██        | 8/40 [01:38<06:36, 12.39s/it] 22%|██▎       | 9/40 [01:52<06:44, 13.05s/it] 25%|██▌       | 10/40 [02:04<06:17, 12.57s/it] 28%|██▊       | 11/40 [02:16<06:02, 12.48s/it] 30%|███       | 12/40 [02:28<05:45, 12.35s/it] 32%|███▎      | 13/40 [02:41<05:37, 12.49s/it] 35%|███▌      | 14/40 [02:53<05:22, 12.39s/it] 38%|███▊      | 15/40 [03:06<05:11, 12.47s/it] 40%|████      | 16/40 [03:19<05:00, 12.52s/it] 42%|████▎     | 17/40 [03:31<04:49, 12.57s/it] 45%|████▌     | 18/40 [03:44<04:36, 12.55s/it] 48%|████▊     | 19/40 [03:56<04:20, 12.41s/it] 50%|█████     | 20/40 [04:08<04:06, 12.35s/it] 52%|█████▎    | 21/40 [04:21<03:56, 12.44s/it] 55%|█████▌    | 22/40 [04:33<03:43, 12.43s/it] 57%|█████▊    | 23/40 [04:45<03:30, 12.36s/it] 60%|██████    | 24/40 [04:58<03:19, 12.46s/it] 62%|██████▎   | 25/40 [05:12<03:14, 12.96s/it] 65%|██████▌   | 26/40 [05:24<02:55, 12.54s/it] 68%|██████▊   | 27/40 [05:36<02:42, 12.53s/it] 70%|███████   | 28/40 [05:47<02:25, 12.16s/it] 72%|███████▎  | 29/40 [05:59<02:12, 12.04s/it] 75%|███████▌  | 30/40 [06:11<02:00, 12.09s/it] 78%|███████▊  | 31/40 [06:24<01:49, 12.21s/it] 80%|████████  | 32/40 [06:37<01:39, 12.41s/it] 82%|████████▎ | 33/40 [06:51<01:29, 12.85s/it] 85%|████████▌ | 34/40 [07:03<01:16, 12.83s/it] 88%|████████▊ | 35/40 [07:16<01:03, 12.80s/it] 90%|█████████ | 36/40 [07:28<00:50, 12.65s/it] 92%|█████████▎| 37/40 [07:41<00:37, 12.56s/it] 95%|█████████▌| 38/40 [07:53<00:25, 12.51s/it] 98%|█████████▊| 39/40 [08:05<00:12, 12.35s/it]100%|██████████| 40/40 [08:17<00:00, 12.20s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:17<00:00, 12.20s/it]100%|██████████| 40/40 [08:17<00:00, 12.44s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 497.5329, 'train_samples_per_second': 5.236, 'train_steps_per_second': 0.08, 'train_loss': 0.7159554481506347, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:17, 17.38s/it]  5%|▌         | 2/40 [00:36<11:28, 18.12s/it]  8%|▊         | 3/40 [00:54<11:16, 18.28s/it] 10%|█         | 4/40 [01:12<10:56, 18.23s/it] 12%|█▎        | 5/40 [01:31<10:41, 18.34s/it] 15%|█▌        | 6/40 [01:49<10:19, 18.22s/it] 18%|█▊        | 7/40 [02:06<09:51, 17.92s/it] 20%|██        | 8/40 [02:25<09:47, 18.34s/it] 22%|██▎       | 9/40 [02:47<10:04, 19.51s/it] 25%|██▌       | 10/40 [03:03<09:07, 18.26s/it] 28%|██▊       | 11/40 [03:21<08:48, 18.21s/it] 30%|███       | 12/40 [03:39<08:27, 18.12s/it] 32%|███▎      | 13/40 [03:57<08:09, 18.13s/it] 35%|███▌      | 14/40 [04:15<07:49, 18.06s/it] 38%|███▊      | 15/40 [04:34<07:40, 18.42s/it] 40%|████      | 16/40 [04:53<07:29, 18.72s/it] 42%|████▎     | 17/40 [05:13<07:19, 19.10s/it] 45%|████▌     | 18/40 [05:32<06:55, 18.88s/it] 48%|████▊     | 19/40 [05:49<06:27, 18.46s/it] 50%|█████     | 20/40 [06:07<06:04, 18.24s/it] 52%|█████▎    | 21/40 [06:26<05:50, 18.46s/it] 55%|█████▌    | 22/40 [06:44<05:32, 18.45s/it] 57%|█████▊    | 23/40 [07:03<05:15, 18.54s/it] 60%|██████    | 24/40 [07:22<04:58, 18.67s/it] 62%|██████▎   | 25/40 [07:44<04:54, 19.64s/it] 65%|██████▌   | 26/40 [08:02<04:26, 19.03s/it] 68%|██████▊   | 27/40 [08:21<04:06, 18.97s/it] 70%|███████   | 28/40 [08:38<03:41, 18.43s/it] 72%|███████▎  | 29/40 [08:55<03:19, 18.17s/it] 75%|███████▌  | 30/40 [09:13<03:01, 18.17s/it] 78%|███████▊  | 31/40 [09:32<02:44, 18.25s/it] 80%|████████  | 32/40 [09:51<02:28, 18.57s/it] 82%|████████▎ | 33/40 [10:12<02:15, 19.30s/it] 85%|████████▌ | 34/40 [10:31<01:55, 19.21s/it] 88%|████████▊ | 35/40 [10:49<01:34, 18.91s/it] 90%|█████████ | 36/40 [11:08<01:15, 18.81s/it] 92%|█████████▎| 37/40 [11:26<00:55, 18.59s/it] 95%|█████████▌| 38/40 [11:44<00:36, 18.42s/it] 98%|█████████▊| 39/40 [12:02<00:18, 18.13s/it]100%|██████████| 40/40 [12:19<00:00, 17.93s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:19<00:00, 17.93s/it]100%|██████████| 40/40 [12:19<00:00, 18.49s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 739.4998, 'train_samples_per_second': 3.523, 'train_steps_per_second': 0.054, 'train_loss': 0.6949410438537598, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<12:02, 18.53s/it]  5%|▌         | 2/40 [00:37<12:00, 18.96s/it]  8%|▊         | 3/40 [00:57<11:47, 19.11s/it] 10%|█         | 4/40 [01:16<11:26, 19.07s/it] 12%|█▎        | 5/40 [01:35<11:07, 19.07s/it] 15%|█▌        | 6/40 [01:54<10:48, 19.07s/it] 18%|█▊        | 7/40 [02:12<10:20, 18.80s/it] 20%|██        | 8/40 [02:33<10:21, 19.41s/it] 22%|██▎       | 9/40 [02:56<10:37, 20.58s/it] 25%|██▌       | 10/40 [03:13<09:48, 19.63s/it] 28%|██▊       | 11/40 [03:32<09:19, 19.29s/it] 30%|███       | 12/40 [03:50<08:50, 18.94s/it] 32%|███▎      | 13/40 [04:09<08:35, 19.10s/it] 35%|███▌      | 14/40 [04:28<08:15, 19.06s/it] 38%|███▊      | 15/40 [04:49<08:04, 19.39s/it] 40%|████      | 16/40 [05:09<07:53, 19.74s/it] 42%|████▎     | 17/40 [05:30<07:45, 20.23s/it] 45%|████▌     | 18/40 [05:50<07:21, 20.05s/it] 48%|████▊     | 19/40 [06:09<06:53, 19.70s/it] 50%|█████     | 20/40 [06:28<06:28, 19.44s/it] 52%|█████▎    | 21/40 [06:48<06:13, 19.68s/it] 55%|█████▌    | 22/40 [07:07<05:52, 19.56s/it] 57%|█████▊    | 23/40 [07:27<05:33, 19.60s/it] 60%|██████    | 24/40 [07:47<05:17, 19.84s/it] 62%|██████▎   | 25/40 [08:10<05:11, 20.80s/it] 65%|██████▌   | 26/40 [08:29<04:42, 20.20s/it] 68%|██████▊   | 27/40 [08:49<04:19, 19.94s/it] 70%|███████   | 28/40 [09:06<03:49, 19.16s/it] 72%|███████▎  | 29/40 [09:24<03:27, 18.89s/it] 75%|███████▌  | 30/40 [09:44<03:10, 19.06s/it] 78%|███████▊  | 31/40 [10:03<02:52, 19.11s/it] 80%|████████  | 32/40 [10:23<02:35, 19.43s/it] 82%|████████▎ | 33/40 [10:46<02:22, 20.34s/it] 85%|████████▌ | 34/40 [11:05<02:01, 20.21s/it] 88%|████████▊ | 35/40 [11:25<01:40, 20.00s/it] 90%|█████████ | 36/40 [11:44<01:18, 19.72s/it] 92%|█████████▎| 37/40 [12:03<00:58, 19.41s/it] 95%|█████████▌| 38/40 [12:22<00:38, 19.42s/it] 98%|█████████▊| 39/40 [12:42<00:19, 19.42s/it]100%|██████████| 40/40 [13:00<00:00, 19.19s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:00<00:00, 19.19s/it]100%|██████████| 40/40 [13:00<00:00, 19.52s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 780.725, 'train_samples_per_second': 3.337, 'train_steps_per_second': 0.051, 'train_loss': 0.7510952949523926, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:30, 11.54s/it]  5%|▌         | 2/40 [00:23<07:22, 11.66s/it]  8%|▊         | 3/40 [00:34<07:11, 11.66s/it] 10%|█         | 4/40 [00:46<06:56, 11.57s/it] 12%|█▎        | 5/40 [00:58<06:49, 11.69s/it] 15%|█▌        | 6/40 [01:09<06:29, 11.45s/it] 18%|█▊        | 7/40 [01:21<06:23, 11.63s/it] 20%|██        | 8/40 [01:32<06:10, 11.58s/it] 22%|██▎       | 9/40 [01:53<07:25, 14.37s/it] 25%|██▌       | 10/40 [02:05<06:47, 13.57s/it] 28%|██▊       | 11/40 [02:17<06:20, 13.12s/it] 30%|███       | 12/40 [02:28<05:53, 12.64s/it] 32%|███▎      | 13/40 [02:39<05:29, 12.19s/it] 35%|███▌      | 14/40 [02:51<05:10, 11.94s/it] 38%|███▊      | 15/40 [03:02<04:54, 11.76s/it] 40%|████      | 16/40 [03:14<04:45, 11.89s/it] 42%|████▎     | 17/40 [03:35<05:33, 14.52s/it] 45%|████▌     | 18/40 [03:46<04:59, 13.62s/it] 48%|████▊     | 19/40 [03:58<04:31, 12.92s/it] 50%|█████     | 20/40 [04:10<04:15, 12.76s/it] 52%|█████▎    | 21/40 [04:21<03:53, 12.29s/it] 55%|█████▌    | 22/40 [04:34<03:42, 12.34s/it] 57%|█████▊    | 23/40 [04:45<03:25, 12.10s/it] 60%|██████    | 24/40 [04:57<03:10, 11.93s/it] 62%|██████▎   | 25/40 [05:16<03:31, 14.11s/it] 65%|██████▌   | 26/40 [05:26<03:02, 13.01s/it] 68%|██████▊   | 27/40 [05:38<02:42, 12.47s/it] 70%|███████   | 28/40 [05:49<02:25, 12.09s/it] 72%|███████▎  | 29/40 [06:01<02:14, 12.23s/it] 75%|███████▌  | 30/40 [06:14<02:01, 12.20s/it] 78%|███████▊  | 31/40 [06:25<01:48, 12.09s/it] 80%|████████  | 32/40 [06:37<01:36, 12.09s/it] 82%|████████▎ | 33/40 [06:57<01:40, 14.34s/it] 85%|████████▌ | 34/40 [07:09<01:21, 13.64s/it] 88%|████████▊ | 35/40 [07:20<01:04, 12.90s/it] 90%|█████████ | 36/40 [07:32<00:50, 12.63s/it] 92%|█████████▎| 37/40 [07:44<00:37, 12.35s/it] 95%|█████████▌| 38/40 [07:55<00:24, 12.07s/it] 98%|█████████▊| 39/40 [08:07<00:12, 12.05s/it]100%|██████████| 40/40 [08:18<00:00, 11.58s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:18<00:00, 11.58s/it]100%|██████████| 40/40 [08:18<00:00, 12.46s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 498.3202, 'train_samples_per_second': 5.629, 'train_steps_per_second': 0.08, 'train_loss': 0.746495246887207, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:30, 11.55s/it]  5%|▌         | 2/40 [00:23<07:29, 11.84s/it]  8%|▊         | 3/40 [00:35<07:14, 11.75s/it] 10%|█         | 4/40 [00:45<06:48, 11.36s/it] 12%|█▎        | 5/40 [00:57<06:35, 11.30s/it] 15%|█▌        | 6/40 [01:06<06:03, 10.69s/it] 18%|█▊        | 7/40 [01:18<06:00, 10.91s/it] 20%|██        | 8/40 [01:29<05:49, 10.93s/it] 22%|██▎       | 9/40 [01:49<07:08, 13.81s/it] 25%|██▌       | 10/40 [02:00<06:34, 13.15s/it] 28%|██▊       | 11/40 [02:12<06:09, 12.73s/it] 30%|███       | 12/40 [02:23<05:44, 12.31s/it] 32%|███▎      | 13/40 [02:35<05:25, 12.05s/it] 35%|███▌      | 14/40 [02:47<05:13, 12.06s/it] 38%|███▊      | 15/40 [02:59<04:57, 11.90s/it] 40%|████      | 16/40 [03:11<04:48, 12.04s/it] 42%|████▎     | 17/40 [03:32<05:36, 14.65s/it] 45%|████▌     | 18/40 [03:43<05:02, 13.74s/it] 48%|████▊     | 19/40 [03:54<04:31, 12.92s/it] 50%|█████     | 20/40 [04:07<04:15, 12.77s/it] 52%|█████▎    | 21/40 [04:18<03:53, 12.32s/it] 55%|█████▌    | 22/40 [04:30<03:42, 12.36s/it] 57%|█████▊    | 23/40 [04:42<03:25, 12.11s/it] 60%|██████    | 24/40 [04:53<03:10, 11.93s/it] 62%|██████▎   | 25/40 [05:12<03:29, 13.97s/it] 65%|██████▌   | 26/40 [05:23<03:04, 13.16s/it] 68%|██████▊   | 27/40 [05:35<02:45, 12.70s/it] 70%|███████   | 28/40 [05:46<02:27, 12.29s/it] 72%|███████▎  | 29/40 [05:59<02:16, 12.40s/it] 75%|███████▌  | 30/40 [06:11<02:03, 12.34s/it] 78%|███████▊  | 31/40 [06:23<01:49, 12.15s/it] 80%|████████  | 32/40 [06:35<01:36, 12.05s/it] 82%|████████▎ | 33/40 [06:54<01:39, 14.27s/it] 85%|████████▌ | 34/40 [07:05<01:20, 13.36s/it] 88%|████████▊ | 35/40 [07:16<01:02, 12.44s/it] 90%|█████████ | 36/40 [07:28<00:49, 12.28s/it] 92%|█████████▎| 37/40 [07:39<00:36, 12.12s/it] 95%|█████████▌| 38/40 [07:51<00:23, 11.93s/it] 98%|█████████▊| 39/40 [08:03<00:11, 11.96s/it]100%|██████████| 40/40 [08:13<00:00, 11.46s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:13<00:00, 11.46s/it]100%|██████████| 40/40 [08:13<00:00, 12.34s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 493.6909, 'train_samples_per_second': 5.682, 'train_steps_per_second': 0.081, 'train_loss': 0.7506836414337158, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:08, 17.13s/it]  5%|▌         | 2/40 [00:34<10:57, 17.30s/it]  8%|▊         | 3/40 [00:51<10:34, 17.15s/it] 10%|█         | 4/40 [01:08<10:11, 17.00s/it] 12%|█▎        | 5/40 [01:26<10:06, 17.33s/it] 15%|█▌        | 6/40 [01:41<09:30, 16.79s/it] 18%|█▊        | 7/40 [01:59<09:23, 17.06s/it] 20%|██        | 8/40 [02:16<09:02, 16.94s/it] 22%|██▎       | 9/40 [02:45<10:40, 20.66s/it] 25%|██▌       | 10/40 [03:01<09:40, 19.34s/it] 28%|██▊       | 11/40 [03:18<09:00, 18.64s/it] 30%|███       | 12/40 [03:34<08:22, 17.94s/it] 32%|███▎      | 13/40 [03:51<07:54, 17.58s/it] 35%|███▌      | 14/40 [04:09<07:36, 17.57s/it] 38%|███▊      | 15/40 [04:25<07:11, 17.25s/it] 40%|████      | 16/40 [04:43<07:00, 17.53s/it] 42%|████▎     | 17/40 [05:13<08:08, 21.24s/it] 45%|████▌     | 18/40 [05:30<07:19, 19.97s/it] 48%|████▊     | 19/40 [05:47<06:36, 18.88s/it] 50%|█████     | 20/40 [06:05<06:12, 18.63s/it] 52%|█████▎    | 21/40 [06:21<05:38, 17.83s/it] 55%|█████▌    | 22/40 [06:39<05:23, 17.95s/it] 57%|█████▊    | 23/40 [06:56<04:59, 17.60s/it] 60%|██████    | 24/40 [07:13<04:39, 17.50s/it] 62%|██████▎   | 25/40 [07:42<05:12, 20.86s/it] 65%|██████▌   | 26/40 [07:58<04:31, 19.39s/it] 68%|██████▊   | 27/40 [08:14<03:59, 18.46s/it] 70%|███████   | 28/40 [08:30<03:33, 17.76s/it] 72%|███████▎  | 29/40 [08:49<03:19, 18.13s/it] 75%|███████▌  | 30/40 [09:07<03:00, 18.04s/it] 78%|███████▊  | 31/40 [09:23<02:37, 17.52s/it] 80%|████████  | 32/40 [09:41<02:20, 17.61s/it] 82%|████████▎ | 33/40 [10:09<02:25, 20.83s/it] 85%|████████▌ | 34/40 [10:27<01:58, 19.81s/it] 88%|████████▊ | 35/40 [10:43<01:34, 18.84s/it] 90%|█████████ | 36/40 [11:01<01:13, 18.49s/it] 92%|█████████▎| 37/40 [11:18<00:54, 18.10s/it] 95%|█████████▌| 38/40 [11:35<00:35, 17.72s/it] 98%|█████████▊| 39/40 [11:53<00:17, 17.75s/it]100%|██████████| 40/40 [12:07<00:00, 16.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:07<00:00, 16.82s/it]100%|██████████| 40/40 [12:07<00:00, 18.20s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 727.9145, 'train_samples_per_second': 3.853, 'train_steps_per_second': 0.055, 'train_loss': 0.7543527126312256, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:26, 17.59s/it]  5%|▌         | 2/40 [00:35<11:21, 17.94s/it]  8%|▊         | 3/40 [00:53<11:03, 17.94s/it] 10%|█         | 4/40 [01:11<10:47, 17.99s/it] 12%|█▎        | 5/40 [01:31<10:45, 18.44s/it] 15%|█▌        | 6/40 [01:47<10:07, 17.86s/it] 18%|█▊        | 7/40 [02:06<09:54, 18.03s/it] 20%|██        | 8/40 [02:23<09:33, 17.91s/it] 22%|██▎       | 9/40 [02:55<11:26, 22.15s/it] 25%|██▌       | 10/40 [03:13<10:24, 20.81s/it] 28%|██▊       | 11/40 [03:30<09:35, 19.86s/it] 30%|███       | 12/40 [03:48<08:55, 19.13s/it] 32%|███▎      | 13/40 [04:06<08:27, 18.80s/it] 35%|███▌      | 14/40 [04:25<08:09, 18.82s/it] 38%|███▊      | 15/40 [04:42<07:42, 18.48s/it] 40%|████      | 16/40 [05:01<07:27, 18.67s/it] 42%|████▎     | 17/40 [05:33<08:39, 22.59s/it] 45%|████▌     | 18/40 [05:51<07:46, 21.21s/it] 48%|████▊     | 19/40 [06:09<07:03, 20.18s/it] 50%|█████     | 20/40 [06:28<06:36, 19.84s/it] 52%|█████▎    | 21/40 [06:45<06:01, 19.03s/it] 55%|█████▌    | 22/40 [07:04<05:44, 19.12s/it] 57%|█████▊    | 23/40 [07:22<05:18, 18.76s/it] 60%|██████    | 24/40 [07:41<04:57, 18.60s/it] 62%|██████▎   | 25/40 [08:12<05:35, 22.39s/it] 65%|██████▌   | 26/40 [08:29<04:52, 20.90s/it] 68%|██████▊   | 27/40 [08:46<04:15, 19.68s/it] 70%|███████   | 28/40 [09:02<03:43, 18.65s/it] 72%|███████▎  | 29/40 [09:22<03:29, 19.01s/it] 75%|███████▌  | 30/40 [09:41<03:10, 19.03s/it] 78%|███████▊  | 31/40 [09:59<02:48, 18.76s/it] 80%|████████  | 32/40 [10:19<02:31, 18.89s/it] 82%|████████▎ | 33/40 [10:49<02:36, 22.38s/it] 85%|████████▌ | 34/40 [11:08<02:07, 21.27s/it] 88%|████████▊ | 35/40 [11:25<01:40, 20.14s/it] 90%|█████████ | 36/40 [11:44<01:18, 19.75s/it] 92%|█████████▎| 37/40 [12:02<00:57, 19.19s/it] 95%|█████████▌| 38/40 [12:19<00:36, 18.48s/it] 98%|█████████▊| 39/40 [12:37<00:18, 18.42s/it]100%|██████████| 40/40 [12:53<00:00, 17.70s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:53<00:00, 17.70s/it]100%|██████████| 40/40 [12:53<00:00, 19.34s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 773.6211, 'train_samples_per_second': 3.626, 'train_steps_per_second': 0.052, 'train_loss': 0.7791570663452149, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:33, 11.63s/it]  5%|▌         | 2/40 [00:23<07:22, 11.65s/it]  8%|▊         | 3/40 [00:35<07:15, 11.76s/it] 10%|█         | 4/40 [00:46<07:02, 11.74s/it] 12%|█▎        | 5/40 [00:58<06:55, 11.86s/it] 15%|█▌        | 6/40 [01:10<06:40, 11.78s/it] 18%|█▊        | 7/40 [01:22<06:30, 11.83s/it] 20%|██        | 8/40 [01:34<06:15, 11.73s/it] 22%|██▎       | 9/40 [01:54<07:26, 14.40s/it] 25%|██▌       | 10/40 [02:05<06:44, 13.48s/it] 28%|██▊       | 11/40 [02:17<06:17, 13.01s/it] 30%|███       | 12/40 [02:29<05:51, 12.55s/it] 32%|███▎      | 13/40 [02:40<05:27, 12.14s/it] 35%|███▌      | 14/40 [02:52<05:14, 12.10s/it] 38%|███▊      | 15/40 [03:04<05:00, 12.02s/it] 40%|████      | 16/40 [03:16<04:47, 11.98s/it] 42%|████▎     | 17/40 [03:36<05:30, 14.37s/it] 45%|████▌     | 18/40 [03:47<04:59, 13.63s/it] 48%|████▊     | 19/40 [03:59<04:31, 12.95s/it] 50%|█████     | 20/40 [04:11<04:12, 12.64s/it] 52%|█████▎    | 21/40 [04:22<03:52, 12.23s/it] 55%|█████▌    | 22/40 [04:34<03:41, 12.31s/it] 57%|█████▊    | 23/40 [04:45<03:21, 11.83s/it] 60%|██████    | 24/40 [04:57<03:08, 11.76s/it] 62%|██████▎   | 25/40 [05:16<03:30, 14.04s/it] 65%|██████▌   | 26/40 [05:27<03:05, 13.22s/it] 68%|██████▊   | 27/40 [05:39<02:46, 12.80s/it] 70%|███████   | 28/40 [05:51<02:31, 12.59s/it] 72%|███████▎  | 29/40 [06:04<02:17, 12.47s/it] 75%|███████▌  | 30/40 [06:15<02:00, 12.03s/it] 78%|███████▊  | 31/40 [06:25<01:44, 11.62s/it] 80%|████████  | 32/40 [06:37<01:33, 11.67s/it] 82%|████████▎ | 33/40 [06:57<01:38, 14.05s/it] 85%|████████▌ | 34/40 [07:09<01:20, 13.45s/it] 88%|████████▊ | 35/40 [07:20<01:04, 12.92s/it] 90%|█████████ | 36/40 [07:32<00:49, 12.49s/it] 92%|█████████▎| 37/40 [07:42<00:35, 11.90s/it] 95%|█████████▌| 38/40 [07:54<00:23, 11.71s/it] 98%|█████████▊| 39/40 [08:05<00:11, 11.63s/it]100%|██████████| 40/40 [08:16<00:00, 11.40s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:16<00:00, 11.40s/it]100%|██████████| 40/40 [08:16<00:00, 12.41s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 496.4513, 'train_samples_per_second': 5.65, 'train_steps_per_second': 0.081, 'train_loss': 0.7462799549102783, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:34, 11.64s/it]  5%|▌         | 2/40 [00:23<07:16, 11.48s/it]  8%|▊         | 3/40 [00:34<07:10, 11.64s/it] 10%|█         | 4/40 [00:46<06:57, 11.61s/it] 12%|█▎        | 5/40 [00:57<06:42, 11.51s/it] 15%|█▌        | 6/40 [01:08<06:22, 11.24s/it] 18%|█▊        | 7/40 [01:19<06:09, 11.20s/it] 20%|██        | 8/40 [01:30<05:56, 11.15s/it] 22%|██▎       | 9/40 [01:50<07:11, 13.93s/it] 25%|██▌       | 10/40 [02:02<06:34, 13.14s/it] 28%|██▊       | 11/40 [02:13<06:10, 12.77s/it] 30%|███       | 12/40 [02:25<05:46, 12.37s/it] 32%|███▎      | 13/40 [02:36<05:25, 12.05s/it] 35%|███▌      | 14/40 [02:48<05:13, 12.06s/it] 38%|███▊      | 15/40 [03:00<04:58, 11.92s/it] 40%|████      | 16/40 [03:12<04:46, 11.93s/it] 42%|████▎     | 17/40 [03:32<05:32, 14.46s/it] 45%|████▌     | 18/40 [03:44<05:00, 13.68s/it] 48%|████▊     | 19/40 [03:55<04:31, 12.93s/it] 50%|█████     | 20/40 [04:07<04:11, 12.58s/it] 52%|█████▎    | 21/40 [04:18<03:51, 12.21s/it] 55%|█████▌    | 22/40 [04:31<03:40, 12.25s/it] 57%|█████▊    | 23/40 [04:42<03:21, 11.84s/it] 60%|██████    | 24/40 [04:53<03:07, 11.70s/it] 62%|██████▎   | 25/40 [05:12<03:29, 14.00s/it] 65%|██████▌   | 26/40 [05:23<03:03, 13.10s/it] 68%|██████▊   | 27/40 [05:35<02:44, 12.65s/it] 70%|███████   | 28/40 [05:47<02:29, 12.47s/it] 72%|███████▎  | 29/40 [06:00<02:17, 12.51s/it] 75%|███████▌  | 30/40 [06:11<02:00, 12.10s/it] 78%|███████▊  | 31/40 [06:22<01:46, 11.87s/it] 80%|████████  | 32/40 [06:34<01:35, 11.89s/it] 82%|████████▎ | 33/40 [06:54<01:40, 14.34s/it] 85%|████████▌ | 34/40 [07:06<01:21, 13.55s/it] 88%|████████▊ | 35/40 [07:17<01:04, 12.90s/it] 90%|█████████ | 36/40 [07:27<00:48, 12.14s/it] 92%|█████████▎| 37/40 [07:38<00:35, 11.72s/it] 95%|█████████▌| 38/40 [07:50<00:23, 11.66s/it] 98%|█████████▊| 39/40 [08:02<00:11, 11.70s/it]100%|██████████| 40/40 [08:13<00:00, 11.72s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:13<00:00, 11.72s/it]100%|██████████| 40/40 [08:13<00:00, 12.35s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 493.8073, 'train_samples_per_second': 5.68, 'train_steps_per_second': 0.081, 'train_loss': 0.7504343032836914, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:03, 17.02s/it]  5%|▌         | 2/40 [00:33<10:38, 16.80s/it]  8%|▊         | 3/40 [00:51<10:31, 17.06s/it] 10%|█         | 4/40 [01:08<10:16, 17.12s/it] 12%|█▎        | 5/40 [01:25<10:02, 17.22s/it] 15%|█▌        | 6/40 [01:42<09:39, 17.05s/it] 18%|█▊        | 7/40 [01:59<09:23, 17.07s/it] 20%|██        | 8/40 [02:15<08:57, 16.79s/it] 22%|██▎       | 9/40 [02:44<10:37, 20.57s/it] 25%|██▌       | 10/40 [03:01<09:43, 19.44s/it] 28%|██▊       | 11/40 [03:18<09:06, 18.83s/it] 30%|███       | 12/40 [03:35<08:29, 18.21s/it] 32%|███▎      | 13/40 [03:52<07:56, 17.65s/it] 35%|███▌      | 14/40 [04:09<07:39, 17.66s/it] 38%|███▊      | 15/40 [04:26<07:15, 17.43s/it] 40%|████      | 16/40 [04:44<07:03, 17.64s/it] 42%|████▎     | 17/40 [05:14<08:10, 21.33s/it] 45%|████▌     | 18/40 [05:32<07:24, 20.19s/it] 48%|████▊     | 19/40 [05:48<06:40, 19.09s/it] 50%|█████     | 20/40 [06:06<06:13, 18.70s/it] 52%|█████▎    | 21/40 [06:23<05:43, 18.07s/it] 55%|█████▌    | 22/40 [06:41<05:29, 18.29s/it] 57%|█████▊    | 23/40 [06:58<05:00, 17.70s/it] 60%|██████    | 24/40 [07:14<04:38, 17.41s/it] 62%|██████▎   | 25/40 [07:43<05:12, 20.81s/it] 65%|██████▌   | 26/40 [08:00<04:33, 19.53s/it] 68%|██████▊   | 27/40 [08:17<04:03, 18.77s/it] 70%|███████   | 28/40 [08:34<03:41, 18.45s/it] 72%|███████▎  | 29/40 [08:53<03:23, 18.48s/it] 75%|███████▌  | 30/40 [09:09<02:58, 17.81s/it] 78%|███████▊  | 31/40 [09:25<02:35, 17.32s/it] 80%|████████  | 32/40 [09:43<02:18, 17.31s/it] 82%|████████▎ | 33/40 [10:11<02:25, 20.72s/it] 85%|████████▌ | 34/40 [10:29<01:57, 19.66s/it] 88%|████████▊ | 35/40 [10:46<01:34, 18.89s/it] 90%|█████████ | 36/40 [11:02<01:12, 18.23s/it] 92%|█████████▎| 37/40 [11:18<00:52, 17.42s/it] 95%|█████████▌| 38/40 [11:35<00:34, 17.24s/it] 98%|█████████▊| 39/40 [11:52<00:17, 17.34s/it]100%|██████████| 40/40 [12:09<00:00, 17.23s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:09<00:00, 17.23s/it]100%|██████████| 40/40 [12:09<00:00, 18.24s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 729.7777, 'train_samples_per_second': 3.844, 'train_steps_per_second': 0.055, 'train_loss': 0.7542026042938232, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:51, 18.25s/it]  5%|▌         | 2/40 [00:36<11:28, 18.12s/it]  8%|▊         | 3/40 [00:55<11:20, 18.40s/it] 10%|█         | 4/40 [01:13<11:02, 18.40s/it] 12%|█▎        | 5/40 [01:32<10:49, 18.56s/it] 15%|█▌        | 6/40 [01:50<10:24, 18.36s/it] 18%|█▊        | 7/40 [02:08<10:01, 18.22s/it] 20%|██        | 8/40 [02:25<09:29, 17.79s/it] 22%|██▎       | 9/40 [02:56<11:20, 21.96s/it] 25%|██▌       | 10/40 [03:13<10:15, 20.51s/it] 28%|██▊       | 11/40 [03:30<09:28, 19.60s/it] 30%|███       | 12/40 [03:48<08:51, 18.98s/it] 32%|███▎      | 13/40 [04:06<08:21, 18.56s/it] 35%|███▌      | 14/40 [04:24<08:04, 18.63s/it] 38%|███▊      | 15/40 [04:42<07:41, 18.46s/it] 40%|████      | 16/40 [05:01<07:26, 18.61s/it] 42%|████▎     | 17/40 [05:33<08:39, 22.59s/it] 45%|████▌     | 18/40 [05:51<07:47, 21.27s/it] 48%|████▊     | 19/40 [06:09<07:04, 20.21s/it] 50%|█████     | 20/40 [06:28<06:35, 19.76s/it] 52%|█████▎    | 21/40 [06:46<06:06, 19.29s/it] 55%|█████▌    | 22/40 [07:06<05:49, 19.44s/it] 57%|█████▊    | 23/40 [07:23<05:18, 18.73s/it] 60%|██████    | 24/40 [07:41<04:54, 18.41s/it] 62%|██████▎   | 25/40 [08:11<05:30, 22.06s/it] 65%|██████▌   | 26/40 [08:29<04:50, 20.78s/it] 68%|██████▊   | 27/40 [08:47<04:19, 19.94s/it] 70%|███████   | 28/40 [09:06<03:56, 19.67s/it] 72%|███████▎  | 29/40 [09:26<03:35, 19.62s/it] 75%|███████▌  | 30/40 [09:43<03:09, 18.98s/it] 78%|███████▊  | 31/40 [10:00<02:46, 18.45s/it] 80%|████████  | 32/40 [10:19<02:27, 18.46s/it] 82%|████████▎ | 33/40 [10:49<02:33, 22.00s/it] 85%|████████▌ | 34/40 [11:07<02:04, 20.77s/it] 88%|████████▊ | 35/40 [11:25<01:40, 20.04s/it] 90%|█████████ | 36/40 [11:43<01:17, 19.43s/it] 92%|█████████▎| 37/40 [12:00<00:55, 18.57s/it] 95%|█████████▌| 38/40 [12:17<00:36, 18.27s/it] 98%|█████████▊| 39/40 [12:36<00:18, 18.41s/it]100%|██████████| 40/40 [12:54<00:00, 18.36s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:54<00:00, 18.36s/it]100%|██████████| 40/40 [12:54<00:00, 19.37s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 774.8282, 'train_samples_per_second': 3.62, 'train_steps_per_second': 0.052, 'train_loss': 0.7793723583221436, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:01, 12.34s/it]  5%|▌         | 2/40 [00:23<07:23, 11.68s/it]  8%|▊         | 3/40 [00:35<07:13, 11.71s/it] 10%|█         | 4/40 [00:47<07:01, 11.71s/it] 12%|█▎        | 5/40 [00:58<06:51, 11.75s/it] 15%|█▌        | 6/40 [01:09<06:31, 11.50s/it] 18%|█▊        | 7/40 [01:19<06:03, 11.01s/it] 20%|██        | 8/40 [01:30<05:49, 10.94s/it] 22%|██▎       | 9/40 [01:51<07:10, 13.90s/it] 25%|██▌       | 10/40 [02:03<06:40, 13.35s/it] 28%|██▊       | 11/40 [02:14<06:13, 12.88s/it] 30%|███       | 12/40 [02:26<05:52, 12.59s/it] 32%|███▎      | 13/40 [02:38<05:33, 12.35s/it] 35%|███▌      | 14/40 [02:50<05:16, 12.16s/it] 38%|███▊      | 15/40 [03:01<04:55, 11.82s/it] 40%|████      | 16/40 [03:13<04:43, 11.80s/it] 42%|████▎     | 17/40 [03:34<05:38, 14.71s/it] 45%|████▌     | 18/40 [03:46<05:01, 13.72s/it] 48%|████▊     | 19/40 [03:57<04:33, 13.04s/it] 50%|█████     | 20/40 [04:09<04:14, 12.75s/it] 52%|█████▎    | 21/40 [04:21<03:56, 12.44s/it] 55%|█████▌    | 22/40 [04:33<03:41, 12.29s/it] 57%|█████▊    | 23/40 [04:44<03:24, 12.05s/it] 60%|██████    | 24/40 [04:56<03:09, 11.84s/it] 62%|██████▎   | 25/40 [05:16<03:34, 14.28s/it] 65%|██████▌   | 26/40 [05:27<03:06, 13.35s/it] 68%|██████▊   | 27/40 [05:39<02:50, 13.10s/it] 70%|███████   | 28/40 [05:51<02:32, 12.73s/it] 72%|███████▎  | 29/40 [06:03<02:18, 12.56s/it] 75%|███████▌  | 30/40 [06:15<02:04, 12.41s/it] 78%|███████▊  | 31/40 [06:27<01:49, 12.14s/it] 80%|████████  | 32/40 [06:39<01:38, 12.28s/it] 82%|████████▎ | 33/40 [07:00<01:42, 14.61s/it] 85%|████████▌ | 34/40 [07:11<01:22, 13.82s/it] 88%|████████▊ | 35/40 [07:24<01:06, 13.28s/it] 90%|█████████ | 36/40 [07:36<00:51, 12.90s/it] 92%|█████████▎| 37/40 [07:47<00:37, 12.33s/it] 95%|█████████▌| 38/40 [07:58<00:24, 12.03s/it] 98%|█████████▊| 39/40 [08:10<00:12, 12.08s/it]100%|██████████| 40/40 [08:22<00:00, 12.18s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:22<00:00, 12.18s/it]100%|██████████| 40/40 [08:22<00:00, 12.57s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 502.9951, 'train_samples_per_second': 5.577, 'train_steps_per_second': 0.08, 'train_loss': 0.7466923713684082, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:05, 12.44s/it]  5%|▌         | 2/40 [00:23<07:32, 11.92s/it]  8%|▊         | 3/40 [00:35<07:21, 11.93s/it] 10%|█         | 4/40 [00:48<07:12, 12.03s/it] 12%|█▎        | 5/40 [01:00<07:00, 12.02s/it] 15%|█▌        | 6/40 [01:12<06:47, 11.99s/it] 18%|█▊        | 7/40 [01:23<06:26, 11.72s/it] 20%|██        | 8/40 [01:34<06:13, 11.69s/it] 22%|██▎       | 9/40 [01:55<07:30, 14.55s/it] 25%|██▌       | 10/40 [02:07<06:55, 13.86s/it] 28%|██▊       | 11/40 [02:19<06:23, 13.22s/it] 30%|███       | 12/40 [02:31<05:54, 12.66s/it] 32%|███▎      | 13/40 [02:42<05:33, 12.34s/it] 35%|███▌      | 14/40 [02:54<05:18, 12.23s/it] 38%|███▊      | 15/40 [03:05<04:58, 11.92s/it] 40%|████      | 16/40 [03:17<04:44, 11.87s/it] 42%|████▎     | 17/40 [03:39<05:39, 14.76s/it] 45%|████▌     | 18/40 [03:50<05:03, 13.80s/it] 48%|████▊     | 19/40 [04:02<04:35, 13.10s/it] 50%|█████     | 20/40 [04:14<04:16, 12.81s/it] 52%|█████▎    | 21/40 [04:26<03:57, 12.51s/it] 55%|█████▌    | 22/40 [04:38<03:44, 12.47s/it] 57%|█████▊    | 23/40 [04:50<03:27, 12.18s/it] 60%|██████    | 24/40 [05:01<03:09, 11.85s/it] 62%|██████▎   | 25/40 [05:21<03:35, 14.34s/it] 65%|██████▌   | 26/40 [05:32<03:08, 13.44s/it] 68%|██████▊   | 27/40 [05:45<02:50, 13.14s/it] 70%|███████   | 28/40 [05:56<02:33, 12.77s/it] 72%|███████▎  | 29/40 [06:08<02:16, 12.45s/it] 75%|███████▌  | 30/40 [06:20<02:02, 12.23s/it] 78%|███████▊  | 31/40 [06:31<01:47, 11.90s/it] 80%|████████  | 32/40 [06:43<01:36, 12.04s/it] 82%|████████▎ | 33/40 [07:03<01:40, 14.29s/it] 85%|████████▌ | 34/40 [07:15<01:21, 13.52s/it] 88%|████████▊ | 35/40 [07:26<01:05, 13.01s/it] 90%|█████████ | 36/40 [07:38<00:50, 12.70s/it] 92%|█████████▎| 37/40 [07:50<00:36, 12.23s/it] 95%|█████████▌| 38/40 [08:01<00:23, 11.96s/it] 98%|█████████▊| 39/40 [08:12<00:11, 11.75s/it]100%|██████████| 40/40 [08:24<00:00, 11.88s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:24<00:00, 11.88s/it]100%|██████████| 40/40 [08:24<00:00, 12.62s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 504.8334, 'train_samples_per_second': 5.556, 'train_steps_per_second': 0.079, 'train_loss': 0.7505687236785888, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:59, 18.44s/it]  5%|▌         | 2/40 [00:34<10:57, 17.31s/it]  8%|▊         | 3/40 [00:52<10:42, 17.36s/it] 10%|█         | 4/40 [01:10<10:32, 17.57s/it] 12%|█▎        | 5/40 [01:27<10:16, 17.62s/it] 15%|█▌        | 6/40 [01:45<09:53, 17.47s/it] 18%|█▊        | 7/40 [02:00<09:17, 16.88s/it] 20%|██        | 8/40 [02:17<09:00, 16.90s/it] 22%|██▎       | 9/40 [02:48<10:53, 21.08s/it] 25%|██▌       | 10/40 [03:06<10:06, 20.22s/it] 28%|██▊       | 11/40 [03:23<09:19, 19.29s/it] 30%|███       | 12/40 [03:40<08:39, 18.55s/it] 32%|███▎      | 13/40 [03:57<08:10, 18.17s/it] 35%|███▌      | 14/40 [04:15<07:47, 17.99s/it] 38%|███▊      | 15/40 [04:31<07:17, 17.52s/it] 40%|████      | 16/40 [04:49<07:00, 17.50s/it] 42%|████▎     | 17/40 [05:20<08:20, 21.76s/it] 45%|████▌     | 18/40 [05:37<07:24, 20.22s/it] 48%|████▊     | 19/40 [05:54<06:45, 19.30s/it] 50%|█████     | 20/40 [06:12<06:17, 18.87s/it] 52%|█████▎    | 21/40 [06:29<05:49, 18.38s/it] 55%|█████▌    | 22/40 [06:47<05:27, 18.17s/it] 57%|█████▊    | 23/40 [07:03<04:57, 17.53s/it] 60%|██████    | 24/40 [07:19<04:34, 17.13s/it] 62%|██████▎   | 25/40 [07:49<05:13, 20.91s/it] 65%|██████▌   | 26/40 [08:05<04:33, 19.52s/it] 68%|██████▊   | 27/40 [08:23<04:08, 19.15s/it] 70%|███████   | 28/40 [08:40<03:41, 18.50s/it] 72%|███████▎  | 29/40 [08:58<03:19, 18.12s/it] 75%|███████▌  | 30/40 [09:15<03:00, 18.01s/it] 78%|███████▊  | 31/40 [09:32<02:38, 17.61s/it] 80%|████████  | 32/40 [09:50<02:22, 17.81s/it] 82%|████████▎ | 33/40 [10:19<02:28, 21.15s/it] 85%|████████▌ | 34/40 [10:37<02:00, 20.07s/it] 88%|████████▊ | 35/40 [10:54<01:36, 19.34s/it] 90%|█████████ | 36/40 [11:12<01:15, 18.76s/it] 92%|█████████▎| 37/40 [11:28<00:54, 18.01s/it] 95%|█████████▌| 38/40 [11:45<00:35, 17.64s/it] 98%|█████████▊| 39/40 [12:03<00:17, 17.80s/it]100%|██████████| 40/40 [12:22<00:00, 17.99s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:22<00:00, 17.99s/it]100%|██████████| 40/40 [12:22<00:00, 18.55s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 742.0183, 'train_samples_per_second': 3.78, 'train_steps_per_second': 0.054, 'train_loss': 0.7542641639709473, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:19<12:21, 19.02s/it]  5%|▌         | 2/40 [00:35<11:13, 17.72s/it]  8%|▊         | 3/40 [00:54<11:11, 18.15s/it] 10%|█         | 4/40 [01:13<11:01, 18.38s/it] 12%|█▎        | 5/40 [01:31<10:47, 18.50s/it] 15%|█▌        | 6/40 [01:49<10:16, 18.15s/it] 18%|█▊        | 7/40 [02:06<09:47, 17.81s/it] 20%|██        | 8/40 [02:23<09:25, 17.67s/it] 22%|██▎       | 9/40 [02:55<11:20, 21.96s/it] 25%|██▌       | 10/40 [03:14<10:33, 21.13s/it] 28%|██▊       | 11/40 [03:32<09:47, 20.26s/it] 30%|███       | 12/40 [03:50<09:07, 19.56s/it] 32%|███▎      | 13/40 [04:09<08:39, 19.22s/it] 35%|███▌      | 14/40 [04:28<08:16, 19.11s/it] 38%|███▊      | 15/40 [04:45<07:44, 18.58s/it] 40%|████      | 16/40 [05:03<07:25, 18.57s/it] 42%|████▎     | 17/40 [05:37<08:51, 23.12s/it] 45%|████▌     | 18/40 [05:55<07:51, 21.41s/it] 48%|████▊     | 19/40 [06:13<07:10, 20.49s/it] 50%|█████     | 20/40 [06:32<06:40, 20.02s/it] 52%|█████▎    | 21/40 [06:50<06:11, 19.53s/it] 55%|█████▌    | 22/40 [07:09<05:49, 19.43s/it] 57%|█████▊    | 23/40 [07:27<05:22, 18.96s/it] 60%|██████    | 24/40 [07:45<04:56, 18.51s/it] 62%|██████▎   | 25/40 [08:16<05:36, 22.46s/it] 65%|██████▌   | 26/40 [08:34<04:55, 21.08s/it] 68%|██████▊   | 27/40 [08:54<04:28, 20.62s/it] 70%|███████   | 28/40 [09:12<03:59, 19.99s/it] 72%|███████▎  | 29/40 [09:31<03:35, 19.55s/it] 75%|███████▌  | 30/40 [09:50<03:12, 19.29s/it] 78%|███████▊  | 31/40 [10:07<02:49, 18.80s/it] 80%|████████  | 32/40 [10:27<02:31, 18.96s/it] 82%|████████▎ | 33/40 [10:57<02:36, 22.38s/it] 85%|████████▌ | 34/40 [11:15<02:07, 21.19s/it] 88%|████████▊ | 35/40 [11:34<01:41, 20.32s/it] 90%|█████████ | 36/40 [11:52<01:18, 19.74s/it] 92%|█████████▎| 37/40 [12:10<00:57, 19.06s/it] 95%|█████████▌| 38/40 [12:27<00:37, 18.65s/it] 98%|█████████▊| 39/40 [12:46<00:18, 18.74s/it]100%|██████████| 40/40 [13:05<00:00, 18.88s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:05<00:00, 18.88s/it]100%|██████████| 40/40 [13:05<00:00, 19.65s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 785.8836, 'train_samples_per_second': 3.569, 'train_steps_per_second': 0.051, 'train_loss': 0.7794239044189453, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<07:57, 12.23s/it]  5%|▌         | 2/40 [00:25<08:04, 12.74s/it]  8%|▊         | 3/40 [00:36<07:29, 12.15s/it] 10%|█         | 4/40 [00:49<07:23, 12.31s/it] 12%|█▎        | 5/40 [01:01<07:10, 12.30s/it] 15%|█▌        | 6/40 [01:13<06:55, 12.21s/it] 18%|█▊        | 7/40 [01:25<06:39, 12.11s/it] 20%|██        | 8/40 [01:38<06:37, 12.43s/it] 22%|██▎       | 9/40 [01:52<06:40, 12.92s/it] 25%|██▌       | 10/40 [02:05<06:23, 12.79s/it] 28%|██▊       | 11/40 [02:17<06:07, 12.69s/it] 30%|███       | 12/40 [02:29<05:47, 12.40s/it] 32%|███▎      | 13/40 [02:41<05:35, 12.43s/it] 35%|███▌      | 14/40 [02:54<05:25, 12.51s/it] 38%|███▊      | 15/40 [03:07<05:16, 12.66s/it] 40%|████      | 16/40 [03:20<05:06, 12.76s/it] 42%|████▎     | 17/40 [03:34<05:02, 13.16s/it] 45%|████▌     | 18/40 [03:46<04:42, 12.82s/it] 48%|████▊     | 19/40 [03:59<04:27, 12.75s/it] 50%|█████     | 20/40 [04:11<04:14, 12.75s/it] 52%|█████▎    | 21/40 [04:24<04:02, 12.77s/it] 55%|█████▌    | 22/40 [04:36<03:44, 12.46s/it] 57%|█████▊    | 23/40 [04:49<03:32, 12.49s/it] 60%|██████    | 24/40 [05:02<03:24, 12.80s/it] 62%|██████▎   | 25/40 [05:17<03:21, 13.40s/it] 65%|██████▌   | 26/40 [05:29<03:03, 13.09s/it] 68%|██████▊   | 27/40 [05:42<02:48, 12.99s/it] 70%|███████   | 28/40 [05:54<02:33, 12.81s/it] 72%|███████▎  | 29/40 [06:07<02:19, 12.72s/it] 75%|███████▌  | 30/40 [06:20<02:06, 12.69s/it] 78%|███████▊  | 31/40 [06:33<01:54, 12.77s/it] 80%|████████  | 32/40 [06:45<01:40, 12.55s/it] 82%|████████▎ | 33/40 [06:58<01:30, 12.95s/it] 85%|████████▌ | 34/40 [07:12<01:17, 12.98s/it] 88%|████████▊ | 35/40 [07:25<01:05, 13.00s/it] 90%|█████████ | 36/40 [07:37<00:51, 12.78s/it] 92%|█████████▎| 37/40 [07:49<00:37, 12.57s/it] 95%|█████████▌| 38/40 [08:02<00:25, 12.61s/it] 98%|█████████▊| 39/40 [08:14<00:12, 12.68s/it]100%|██████████| 40/40 [08:27<00:00, 12.51s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:27<00:00, 12.51s/it]100%|██████████| 40/40 [08:27<00:00, 12.68s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 507.0728, 'train_samples_per_second': 5.137, 'train_steps_per_second': 0.079, 'train_loss': 0.698764705657959, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<07:56, 12.21s/it]  5%|▌         | 2/40 [00:25<08:06, 12.80s/it]  8%|▊         | 3/40 [00:37<07:40, 12.44s/it] 10%|█         | 4/40 [00:50<07:30, 12.50s/it] 12%|█▎        | 5/40 [01:02<07:21, 12.62s/it] 15%|█▌        | 6/40 [01:15<07:04, 12.47s/it] 18%|█▊        | 7/40 [01:26<06:45, 12.30s/it] 20%|██        | 8/40 [01:40<06:43, 12.61s/it] 22%|██▎       | 9/40 [01:54<06:45, 13.07s/it] 25%|██▌       | 10/40 [02:07<06:28, 12.95s/it] 28%|██▊       | 11/40 [02:19<06:11, 12.81s/it] 30%|███       | 12/40 [02:31<05:50, 12.53s/it] 32%|███▎      | 13/40 [02:44<05:39, 12.57s/it] 35%|███▌      | 14/40 [02:57<05:29, 12.68s/it] 38%|███▊      | 15/40 [03:10<05:20, 12.82s/it] 40%|████      | 16/40 [03:23<05:10, 12.93s/it] 42%|████▎     | 17/40 [03:37<05:03, 13.21s/it] 45%|████▌     | 18/40 [03:49<04:47, 13.07s/it] 48%|████▊     | 19/40 [04:02<04:31, 12.95s/it] 50%|█████     | 20/40 [04:15<04:17, 12.87s/it] 52%|█████▎    | 21/40 [04:28<04:04, 12.88s/it] 55%|█████▌    | 22/40 [04:39<03:41, 12.30s/it] 57%|█████▊    | 23/40 [04:51<03:29, 12.35s/it] 60%|██████    | 24/40 [05:04<03:22, 12.66s/it] 62%|██████▎   | 25/40 [05:19<03:18, 13.24s/it] 65%|██████▌   | 26/40 [05:31<03:00, 12.90s/it] 68%|██████▊   | 27/40 [05:44<02:47, 12.85s/it] 70%|███████   | 28/40 [05:56<02:32, 12.69s/it] 72%|███████▎  | 29/40 [06:09<02:19, 12.64s/it] 75%|███████▌  | 30/40 [06:21<02:06, 12.63s/it] 78%|███████▊  | 31/40 [06:34<01:52, 12.49s/it] 80%|████████  | 32/40 [06:45<01:37, 12.22s/it] 82%|████████▎ | 33/40 [06:59<01:28, 12.70s/it] 85%|████████▌ | 34/40 [07:12<01:16, 12.77s/it] 88%|████████▊ | 35/40 [07:25<01:03, 12.79s/it] 90%|█████████ | 36/40 [07:37<00:50, 12.64s/it] 92%|█████████▎| 37/40 [07:49<00:37, 12.52s/it] 95%|█████████▌| 38/40 [08:02<00:25, 12.55s/it] 98%|█████████▊| 39/40 [08:15<00:12, 12.62s/it]100%|██████████| 40/40 [08:27<00:00, 12.50s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:27<00:00, 12.50s/it]100%|██████████| 40/40 [08:27<00:00, 12.69s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 507.406, 'train_samples_per_second': 5.134, 'train_steps_per_second': 0.079, 'train_loss': 0.7158039093017579, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:43, 18.03s/it]  5%|▌         | 2/40 [00:37<11:56, 18.84s/it]  8%|▊         | 3/40 [00:54<11:14, 18.23s/it] 10%|█         | 4/40 [01:14<11:08, 18.57s/it] 12%|█▎        | 5/40 [01:33<10:56, 18.76s/it] 15%|█▌        | 6/40 [01:50<10:20, 18.24s/it] 18%|█▊        | 7/40 [02:08<09:56, 18.07s/it] 20%|██        | 8/40 [02:27<09:55, 18.60s/it] 22%|██▎       | 9/40 [02:48<09:57, 19.29s/it] 25%|██▌       | 10/40 [03:07<09:33, 19.12s/it] 28%|██▊       | 11/40 [03:25<09:08, 18.92s/it] 30%|███       | 12/40 [03:42<08:33, 18.35s/it] 32%|███▎      | 13/40 [04:01<08:16, 18.38s/it] 35%|███▌      | 14/40 [04:19<08:00, 18.47s/it] 38%|███▊      | 15/40 [04:39<07:48, 18.76s/it] 40%|████      | 16/40 [04:58<07:35, 18.97s/it] 42%|████▎     | 17/40 [05:19<07:25, 19.38s/it] 45%|████▌     | 18/40 [05:38<07:04, 19.29s/it] 48%|████▊     | 19/40 [05:57<06:41, 19.13s/it] 50%|█████     | 20/40 [06:15<06:19, 18.99s/it] 52%|█████▎    | 21/40 [06:35<06:03, 19.14s/it] 55%|█████▌    | 22/40 [06:52<05:33, 18.55s/it] 57%|█████▊    | 23/40 [07:10<05:14, 18.51s/it] 60%|██████    | 24/40 [07:30<05:03, 18.94s/it] 62%|██████▎   | 25/40 [07:52<04:56, 19.77s/it] 65%|██████▌   | 26/40 [08:09<04:24, 18.91s/it] 68%|██████▊   | 27/40 [08:28<04:05, 18.85s/it] 70%|███████   | 28/40 [08:46<03:43, 18.66s/it] 72%|███████▎  | 29/40 [09:04<03:23, 18.54s/it] 75%|███████▌  | 30/40 [09:23<03:05, 18.52s/it] 78%|███████▊  | 31/40 [09:41<02:47, 18.64s/it] 80%|████████  | 32/40 [10:00<02:27, 18.48s/it] 82%|████████▎ | 33/40 [10:20<02:13, 19.11s/it] 85%|████████▌ | 34/40 [10:40<01:55, 19.33s/it] 88%|████████▊ | 35/40 [10:59<01:36, 19.36s/it] 90%|█████████ | 36/40 [11:18<01:16, 19.04s/it] 92%|█████████▎| 37/40 [11:36<00:56, 18.69s/it] 95%|█████████▌| 38/40 [11:54<00:37, 18.63s/it] 98%|█████████▊| 39/40 [12:13<00:18, 18.80s/it]100%|██████████| 40/40 [12:31<00:00, 18.45s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:31<00:00, 18.45s/it]100%|██████████| 40/40 [12:31<00:00, 18.78s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 751.3753, 'train_samples_per_second': 3.467, 'train_steps_per_second': 0.053, 'train_loss': 0.695246410369873, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:19<12:28, 19.20s/it]  5%|▌         | 2/40 [00:39<12:45, 20.14s/it]  8%|▊         | 3/40 [00:58<12:01, 19.50s/it] 10%|█         | 4/40 [01:18<11:52, 19.78s/it] 12%|█▎        | 5/40 [01:39<11:37, 19.93s/it] 15%|█▌        | 6/40 [01:57<11:00, 19.42s/it] 18%|█▊        | 7/40 [02:16<10:32, 19.17s/it] 20%|██        | 8/40 [02:36<10:27, 19.61s/it] 22%|██▎       | 9/40 [02:58<10:28, 20.26s/it] 25%|██▌       | 10/40 [03:18<10:01, 20.05s/it] 28%|██▊       | 11/40 [03:37<09:34, 19.82s/it] 30%|███       | 12/40 [03:55<09:00, 19.29s/it] 32%|███▎      | 13/40 [04:14<08:39, 19.25s/it] 35%|███▌      | 14/40 [04:34<08:22, 19.32s/it] 38%|███▊      | 15/40 [04:54<08:10, 19.63s/it] 40%|████      | 16/40 [05:14<07:57, 19.91s/it] 42%|████▎     | 17/40 [05:36<07:51, 20.50s/it] 45%|████▌     | 18/40 [05:56<07:28, 20.38s/it] 48%|████▊     | 19/40 [06:16<07:03, 20.17s/it] 50%|█████     | 20/40 [06:36<06:39, 19.96s/it] 52%|█████▎    | 21/40 [06:56<06:19, 19.97s/it] 55%|█████▌    | 22/40 [07:14<05:50, 19.46s/it] 57%|█████▊    | 23/40 [07:33<05:30, 19.45s/it] 60%|██████    | 24/40 [07:54<05:18, 19.89s/it] 62%|██████▎   | 25/40 [08:16<05:08, 20.58s/it] 65%|██████▌   | 26/40 [08:35<04:39, 19.95s/it] 68%|██████▊   | 27/40 [08:54<04:17, 19.85s/it] 70%|███████   | 28/40 [09:14<03:56, 19.69s/it] 72%|███████▎  | 29/40 [09:33<03:34, 19.50s/it] 75%|███████▌  | 30/40 [09:53<03:15, 19.57s/it] 78%|███████▊  | 31/40 [10:13<02:58, 19.79s/it] 80%|████████  | 32/40 [10:32<02:37, 19.66s/it] 82%|████████▎ | 33/40 [10:54<02:21, 20.24s/it] 85%|████████▌ | 34/40 [11:14<02:01, 20.33s/it] 88%|████████▊ | 35/40 [11:35<01:41, 20.33s/it] 90%|█████████ | 36/40 [11:54<01:20, 20.04s/it] 92%|█████████▎| 37/40 [12:13<00:59, 19.68s/it] 95%|█████████▌| 38/40 [12:32<00:39, 19.64s/it] 98%|█████████▊| 39/40 [12:53<00:19, 19.81s/it]100%|██████████| 40/40 [13:11<00:00, 19.51s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:11<00:00, 19.51s/it]100%|██████████| 40/40 [13:11<00:00, 19.80s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 791.9884, 'train_samples_per_second': 3.289, 'train_steps_per_second': 0.051, 'train_loss': 0.7507439136505127, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:11, 12.59s/it]  5%|▌         | 2/40 [00:25<08:12, 12.95s/it]  8%|▊         | 3/40 [00:37<07:41, 12.46s/it] 10%|█         | 4/40 [00:49<07:24, 12.34s/it] 12%|█▎        | 5/40 [01:02<07:15, 12.43s/it] 15%|█▌        | 6/40 [01:14<07:03, 12.46s/it] 18%|█▊        | 7/40 [01:26<06:42, 12.19s/it] 20%|██        | 8/40 [01:39<06:34, 12.34s/it] 22%|██▎       | 9/40 [01:53<06:37, 12.83s/it] 25%|██▌       | 10/40 [02:05<06:17, 12.60s/it] 28%|██▊       | 11/40 [02:17<06:02, 12.50s/it] 30%|███       | 12/40 [02:29<05:42, 12.24s/it] 32%|███▎      | 13/40 [02:42<05:36, 12.46s/it] 35%|███▌      | 14/40 [02:55<05:28, 12.63s/it] 38%|███▊      | 15/40 [03:08<05:21, 12.88s/it] 40%|████      | 16/40 [03:21<05:10, 12.92s/it] 42%|████▎     | 17/40 [03:35<05:03, 13.18s/it] 45%|████▌     | 18/40 [03:47<04:44, 12.94s/it] 48%|████▊     | 19/40 [04:00<04:30, 12.90s/it] 50%|█████     | 20/40 [04:13<04:15, 12.78s/it] 52%|█████▎    | 21/40 [04:25<04:01, 12.72s/it] 55%|█████▌    | 22/40 [04:38<03:47, 12.63s/it] 57%|█████▊    | 23/40 [04:50<03:31, 12.42s/it] 60%|██████    | 24/40 [05:03<03:23, 12.72s/it] 62%|██████▎   | 25/40 [05:18<03:20, 13.34s/it] 65%|██████▌   | 26/40 [05:30<03:03, 13.08s/it] 68%|██████▊   | 27/40 [05:43<02:50, 13.11s/it] 70%|███████   | 28/40 [05:56<02:33, 12.83s/it] 72%|███████▎  | 29/40 [06:08<02:19, 12.66s/it] 75%|███████▌  | 30/40 [06:20<02:05, 12.58s/it] 78%|███████▊  | 31/40 [06:33<01:52, 12.53s/it] 80%|████████  | 32/40 [06:45<01:39, 12.39s/it] 82%|████████▎ | 33/40 [06:59<01:30, 12.88s/it] 85%|████████▌ | 34/40 [07:12<01:17, 12.92s/it] 88%|████████▊ | 35/40 [07:25<01:05, 13.09s/it] 90%|█████████ | 36/40 [07:37<00:51, 12.84s/it] 92%|█████████▎| 37/40 [07:50<00:38, 12.68s/it] 95%|█████████▌| 38/40 [08:02<00:25, 12.68s/it] 98%|█████████▊| 39/40 [08:15<00:12, 12.74s/it]100%|██████████| 40/40 [08:27<00:00, 12.43s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:27<00:00, 12.43s/it]100%|██████████| 40/40 [08:27<00:00, 12.69s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 507.5107, 'train_samples_per_second': 5.133, 'train_steps_per_second': 0.079, 'train_loss': 0.6986006736755371, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<07:50, 12.06s/it]  5%|▌         | 2/40 [00:25<07:59, 12.61s/it]  8%|▊         | 3/40 [00:36<07:28, 12.12s/it] 10%|█         | 4/40 [00:48<07:12, 12.01s/it] 12%|█▎        | 5/40 [01:01<07:07, 12.22s/it] 15%|█▌        | 6/40 [01:13<07:01, 12.39s/it] 18%|█▊        | 7/40 [01:25<06:47, 12.34s/it] 20%|██        | 8/40 [01:38<06:41, 12.54s/it] 22%|██▎       | 9/40 [01:52<06:42, 13.00s/it] 25%|██▌       | 10/40 [02:05<06:21, 12.71s/it] 28%|██▊       | 11/40 [02:17<06:04, 12.58s/it] 30%|███       | 12/40 [02:28<05:43, 12.26s/it] 32%|███▎      | 13/40 [02:41<05:37, 12.50s/it] 35%|███▌      | 14/40 [02:54<05:29, 12.65s/it] 38%|███▊      | 15/40 [03:08<05:23, 12.95s/it] 40%|████      | 16/40 [03:21<05:11, 12.98s/it] 42%|████▎     | 17/40 [03:35<05:04, 13.26s/it] 45%|████▌     | 18/40 [03:48<04:47, 13.06s/it] 48%|████▊     | 19/40 [04:00<04:31, 12.92s/it] 50%|█████     | 20/40 [04:13<04:15, 12.77s/it] 52%|█████▎    | 21/40 [04:25<03:59, 12.59s/it] 55%|█████▌    | 22/40 [04:36<03:39, 12.21s/it] 57%|█████▊    | 23/40 [04:47<03:22, 11.88s/it] 60%|██████    | 24/40 [05:01<03:17, 12.32s/it] 62%|██████▎   | 25/40 [05:15<03:15, 13.04s/it] 65%|██████▌   | 26/40 [05:28<02:59, 12.83s/it] 68%|██████▊   | 27/40 [05:41<02:47, 12.87s/it] 70%|███████   | 28/40 [05:53<02:31, 12.64s/it] 72%|███████▎  | 29/40 [06:05<02:16, 12.40s/it] 75%|███████▌  | 30/40 [06:16<02:01, 12.12s/it] 78%|███████▊  | 31/40 [06:28<01:47, 11.95s/it] 80%|████████  | 32/40 [06:39<01:34, 11.86s/it] 82%|████████▎ | 33/40 [06:53<01:27, 12.46s/it] 85%|████████▌ | 34/40 [07:06<01:15, 12.64s/it] 88%|████████▊ | 35/40 [07:20<01:04, 12.90s/it] 90%|█████████ | 36/40 [07:32<00:50, 12.62s/it] 92%|█████████▎| 37/40 [07:44<00:37, 12.48s/it] 95%|█████████▌| 38/40 [07:56<00:25, 12.52s/it] 98%|█████████▊| 39/40 [08:09<00:12, 12.62s/it]100%|██████████| 40/40 [08:21<00:00, 12.24s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:21<00:00, 12.24s/it]100%|██████████| 40/40 [08:21<00:00, 12.53s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 501.0706, 'train_samples_per_second': 5.199, 'train_steps_per_second': 0.08, 'train_loss': 0.7158558368682861, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:25, 17.57s/it]  5%|▌         | 2/40 [00:37<11:49, 18.67s/it]  8%|▊         | 3/40 [00:54<11:08, 18.08s/it] 10%|█         | 4/40 [01:12<10:49, 18.03s/it] 12%|█▎        | 5/40 [01:31<10:40, 18.29s/it] 15%|█▌        | 6/40 [01:49<10:24, 18.38s/it] 18%|█▊        | 7/40 [02:07<09:58, 18.14s/it] 20%|██        | 8/40 [02:25<09:43, 18.22s/it] 22%|██▎       | 9/40 [02:46<09:49, 19.02s/it] 25%|██▌       | 10/40 [03:03<09:14, 18.50s/it] 28%|██▊       | 11/40 [03:22<08:55, 18.46s/it] 30%|███       | 12/40 [03:38<08:21, 17.91s/it] 32%|███▎      | 13/40 [03:58<08:15, 18.35s/it] 35%|███▌      | 14/40 [04:17<08:01, 18.52s/it] 38%|███▊      | 15/40 [04:37<07:54, 18.97s/it] 40%|████      | 16/40 [04:56<07:40, 19.19s/it] 42%|████▎     | 17/40 [05:17<07:30, 19.61s/it] 45%|████▌     | 18/40 [05:35<07:04, 19.29s/it] 48%|████▊     | 19/40 [05:54<06:42, 19.17s/it] 50%|█████     | 20/40 [06:13<06:17, 18.89s/it] 52%|█████▎    | 21/40 [06:31<05:56, 18.75s/it] 55%|█████▌    | 22/40 [06:49<05:33, 18.54s/it] 57%|█████▊    | 23/40 [07:07<05:11, 18.30s/it] 60%|██████    | 24/40 [07:27<05:00, 18.78s/it] 62%|██████▎   | 25/40 [07:49<04:55, 19.70s/it] 65%|██████▌   | 26/40 [08:06<04:28, 19.16s/it] 68%|██████▊   | 27/40 [08:26<04:10, 19.24s/it] 70%|███████   | 28/40 [08:44<03:45, 18.76s/it] 72%|███████▎  | 29/40 [09:03<03:27, 18.87s/it] 75%|███████▌  | 30/40 [09:21<03:06, 18.67s/it] 78%|███████▊  | 31/40 [09:39<02:47, 18.60s/it] 80%|████████  | 32/40 [09:57<02:27, 18.43s/it] 82%|████████▎ | 33/40 [10:18<02:14, 19.16s/it] 85%|████████▌ | 34/40 [10:38<01:55, 19.32s/it] 88%|████████▊ | 35/40 [10:58<01:38, 19.62s/it] 90%|█████████ | 36/40 [11:16<01:16, 19.03s/it] 92%|█████████▎| 37/40 [11:33<00:55, 18.57s/it] 95%|█████████▌| 38/40 [11:52<00:37, 18.63s/it] 98%|█████████▊| 39/40 [12:11<00:18, 18.80s/it]100%|██████████| 40/40 [12:28<00:00, 18.22s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:28<00:00, 18.22s/it]100%|██████████| 40/40 [12:28<00:00, 18.72s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 748.6709, 'train_samples_per_second': 3.479, 'train_steps_per_second': 0.053, 'train_loss': 0.694769811630249, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<12:19, 18.96s/it]  5%|▌         | 2/40 [00:39<12:37, 19.94s/it]  8%|▊         | 3/40 [00:58<11:54, 19.30s/it] 10%|█         | 4/40 [01:17<11:31, 19.20s/it] 12%|█▎        | 5/40 [01:36<11:17, 19.36s/it] 15%|█▌        | 6/40 [01:56<11:01, 19.44s/it] 18%|█▊        | 7/40 [02:15<10:38, 19.34s/it] 20%|██        | 8/40 [02:35<10:25, 19.56s/it] 22%|██▎       | 9/40 [02:57<10:30, 20.34s/it] 25%|██▌       | 10/40 [03:16<09:57, 19.92s/it] 28%|██▊       | 11/40 [03:35<09:32, 19.74s/it] 30%|███       | 12/40 [03:53<08:55, 19.13s/it] 32%|███▎      | 13/40 [04:14<08:48, 19.57s/it] 35%|███▌      | 14/40 [04:34<08:33, 19.76s/it] 38%|███▊      | 15/40 [04:55<08:24, 20.18s/it] 40%|████      | 16/40 [05:16<08:08, 20.34s/it] 42%|████▎     | 17/40 [05:38<07:57, 20.76s/it] 45%|████▌     | 18/40 [05:57<07:28, 20.36s/it] 48%|████▊     | 19/40 [06:17<07:06, 20.31s/it] 50%|█████     | 20/40 [06:37<06:40, 20.04s/it] 52%|█████▎    | 21/40 [06:56<06:18, 19.91s/it] 55%|█████▌    | 22/40 [07:16<05:55, 19.73s/it] 57%|█████▊    | 23/40 [07:34<05:31, 19.48s/it] 60%|██████    | 24/40 [07:55<05:18, 19.90s/it] 62%|██████▎   | 25/40 [08:18<05:12, 20.83s/it] 65%|██████▌   | 26/40 [08:38<04:45, 20.37s/it] 68%|██████▊   | 27/40 [08:58<04:24, 20.38s/it] 70%|███████   | 28/40 [09:17<03:59, 19.94s/it] 72%|███████▎  | 29/40 [09:37<03:39, 19.96s/it] 75%|███████▌  | 30/40 [09:56<03:17, 19.75s/it] 78%|███████▊  | 31/40 [10:16<02:56, 19.62s/it] 80%|████████  | 32/40 [10:35<02:35, 19.45s/it] 82%|████████▎ | 33/40 [10:57<02:21, 20.21s/it] 85%|████████▌ | 34/40 [11:17<02:02, 20.33s/it] 88%|████████▊ | 35/40 [11:38<01:43, 20.61s/it] 90%|█████████ | 36/40 [11:57<01:20, 20.10s/it] 92%|█████████▎| 37/40 [12:16<00:58, 19.64s/it] 95%|█████████▌| 38/40 [12:36<00:39, 19.68s/it] 98%|█████████▊| 39/40 [12:56<00:19, 19.91s/it]100%|██████████| 40/40 [13:14<00:00, 19.35s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:14<00:00, 19.35s/it]100%|██████████| 40/40 [13:14<00:00, 19.87s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 794.6629, 'train_samples_per_second': 3.278, 'train_steps_per_second': 0.05, 'train_loss': 0.750820541381836, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:17, 11.23s/it]  5%|▌         | 2/40 [00:22<07:15, 11.46s/it]  8%|▊         | 3/40 [00:35<07:18, 11.84s/it] 10%|█         | 4/40 [00:46<07:03, 11.75s/it] 12%|█▎        | 5/40 [00:58<06:50, 11.72s/it] 15%|█▌        | 6/40 [01:09<06:32, 11.54s/it] 18%|█▊        | 7/40 [01:20<06:16, 11.42s/it] 20%|██        | 8/40 [01:31<05:59, 11.23s/it] 22%|██▎       | 9/40 [01:52<07:20, 14.22s/it] 25%|██▌       | 10/40 [02:04<06:46, 13.55s/it] 28%|██▊       | 11/40 [02:15<06:11, 12.81s/it] 30%|███       | 12/40 [02:27<05:47, 12.41s/it] 32%|███▎      | 13/40 [02:38<05:26, 12.09s/it] 35%|███▌      | 14/40 [02:50<05:14, 12.11s/it] 38%|███▊      | 15/40 [03:01<04:55, 11.80s/it] 40%|████      | 16/40 [03:13<04:40, 11.70s/it] 42%|████▎     | 17/40 [03:34<05:36, 14.62s/it] 45%|████▌     | 18/40 [03:46<05:02, 13.75s/it] 48%|████▊     | 19/40 [03:57<04:34, 13.06s/it] 50%|█████     | 20/40 [04:09<04:13, 12.70s/it] 52%|█████▎    | 21/40 [04:20<03:53, 12.29s/it] 55%|█████▌    | 22/40 [04:33<03:41, 12.28s/it] 57%|█████▊    | 23/40 [04:44<03:23, 11.99s/it] 60%|██████    | 24/40 [04:54<03:03, 11.45s/it] 62%|██████▎   | 25/40 [05:12<03:20, 13.38s/it] 65%|██████▌   | 26/40 [05:24<03:00, 12.88s/it] 68%|██████▊   | 27/40 [05:36<02:45, 12.71s/it] 70%|███████   | 28/40 [05:48<02:29, 12.45s/it] 72%|███████▎  | 29/40 [06:00<02:14, 12.23s/it] 75%|███████▌  | 30/40 [06:11<01:59, 11.92s/it] 78%|███████▊  | 31/40 [06:23<01:46, 11.83s/it] 80%|████████  | 32/40 [06:34<01:34, 11.87s/it] 82%|████████▎ | 33/40 [06:54<01:39, 14.27s/it] 85%|████████▌ | 34/40 [07:06<01:21, 13.59s/it] 88%|████████▊ | 35/40 [07:17<01:04, 12.84s/it] 90%|█████████ | 36/40 [07:29<00:50, 12.58s/it] 92%|█████████▎| 37/40 [07:41<00:36, 12.20s/it] 95%|█████████▌| 38/40 [07:52<00:23, 12.00s/it] 98%|█████████▊| 39/40 [08:04<00:11, 11.84s/it]100%|██████████| 40/40 [08:15<00:00, 11.60s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:15<00:00, 11.60s/it]100%|██████████| 40/40 [08:15<00:00, 12.38s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 495.2435, 'train_samples_per_second': 5.664, 'train_steps_per_second': 0.081, 'train_loss': 0.7461763381958008, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:16, 11.19s/it]  5%|▌         | 2/40 [00:22<07:14, 11.43s/it]  8%|▊         | 3/40 [00:35<07:17, 11.82s/it] 10%|█         | 4/40 [00:46<07:01, 11.70s/it] 12%|█▎        | 5/40 [00:58<06:48, 11.68s/it] 15%|█▌        | 6/40 [01:09<06:32, 11.53s/it] 18%|█▊        | 7/40 [01:20<06:16, 11.40s/it] 20%|██        | 8/40 [01:31<06:01, 11.30s/it] 22%|██▎       | 9/40 [01:52<07:21, 14.25s/it] 25%|██▌       | 10/40 [02:04<06:45, 13.50s/it] 28%|██▊       | 11/40 [02:15<06:10, 12.79s/it] 30%|███       | 12/40 [02:26<05:46, 12.37s/it] 32%|███▎      | 13/40 [02:38<05:24, 12.04s/it] 35%|███▌      | 14/40 [02:50<05:14, 12.08s/it] 38%|███▊      | 15/40 [03:01<04:54, 11.78s/it] 40%|████      | 16/40 [03:13<04:42, 11.75s/it] 42%|████▎     | 17/40 [03:34<05:35, 14.60s/it] 45%|████▌     | 18/40 [03:45<04:59, 13.61s/it] 48%|████▊     | 19/40 [03:55<04:22, 12.51s/it] 50%|█████     | 20/40 [04:06<03:59, 11.97s/it] 52%|█████▎    | 21/40 [04:16<03:37, 11.43s/it] 55%|█████▌    | 22/40 [04:28<03:28, 11.59s/it] 57%|█████▊    | 23/40 [04:39<03:15, 11.52s/it] 60%|██████    | 24/40 [04:50<03:02, 11.44s/it] 62%|██████▎   | 25/40 [05:09<03:24, 13.65s/it] 65%|██████▌   | 26/40 [05:21<03:02, 13.04s/it] 68%|██████▊   | 27/40 [05:33<02:46, 12.83s/it] 70%|███████   | 28/40 [05:45<02:29, 12.44s/it] 72%|███████▎  | 29/40 [05:56<02:13, 12.18s/it] 75%|███████▌  | 30/40 [06:08<01:59, 11.90s/it] 78%|███████▊  | 31/40 [06:19<01:46, 11.80s/it] 80%|████████  | 32/40 [06:31<01:34, 11.78s/it] 82%|████████▎ | 33/40 [06:51<01:39, 14.26s/it] 85%|████████▌ | 34/40 [07:03<01:21, 13.58s/it] 88%|████████▊ | 35/40 [07:14<01:04, 12.82s/it] 90%|█████████ | 36/40 [07:26<00:50, 12.54s/it] 92%|█████████▎| 37/40 [07:37<00:36, 12.16s/it] 95%|█████████▌| 38/40 [07:49<00:23, 11.97s/it] 98%|█████████▊| 39/40 [08:00<00:11, 11.83s/it]100%|██████████| 40/40 [08:11<00:00, 11.61s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:11<00:00, 11.61s/it]100%|██████████| 40/40 [08:11<00:00, 12.29s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 491.7662, 'train_samples_per_second': 5.704, 'train_steps_per_second': 0.081, 'train_loss': 0.7507218360900879, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:16<10:40, 16.44s/it]  5%|▌         | 2/40 [00:33<10:44, 16.96s/it]  8%|▊         | 3/40 [00:51<10:46, 17.48s/it] 10%|█         | 4/40 [01:08<10:20, 17.25s/it] 12%|█▎        | 5/40 [01:26<10:04, 17.26s/it] 15%|█▌        | 6/40 [01:42<09:35, 16.94s/it] 18%|█▊        | 7/40 [01:58<09:12, 16.74s/it] 20%|██        | 8/40 [02:14<08:46, 16.44s/it] 22%|██▎       | 9/40 [02:44<10:41, 20.70s/it] 25%|██▌       | 10/40 [03:02<09:52, 19.73s/it] 28%|██▊       | 11/40 [03:18<09:00, 18.62s/it] 30%|███       | 12/40 [03:34<08:25, 18.04s/it] 32%|███▎      | 13/40 [03:51<07:53, 17.55s/it] 35%|███▌      | 14/40 [04:09<07:39, 17.68s/it] 38%|███▊      | 15/40 [04:25<07:11, 17.24s/it] 40%|████      | 16/40 [04:42<06:51, 17.16s/it] 42%|████▎     | 17/40 [05:13<08:09, 21.28s/it] 45%|████▌     | 18/40 [05:30<07:21, 20.05s/it] 48%|████▊     | 19/40 [05:47<06:38, 18.99s/it] 50%|█████     | 20/40 [06:04<06:09, 18.46s/it] 52%|█████▎    | 21/40 [06:20<05:39, 17.86s/it] 55%|█████▌    | 22/40 [06:38<05:21, 17.85s/it] 57%|█████▊    | 23/40 [06:54<04:53, 17.25s/it] 60%|██████    | 24/40 [07:10<04:29, 16.82s/it] 62%|██████▎   | 25/40 [07:37<04:58, 19.90s/it] 65%|██████▌   | 26/40 [07:54<04:26, 19.02s/it] 68%|██████▊   | 27/40 [08:12<04:02, 18.62s/it] 70%|███████   | 28/40 [08:28<03:36, 18.05s/it] 72%|███████▎  | 29/40 [08:46<03:16, 17.85s/it] 75%|███████▌  | 30/40 [09:02<02:54, 17.48s/it] 78%|███████▊  | 31/40 [09:19<02:35, 17.25s/it] 80%|████████  | 32/40 [09:36<02:17, 17.15s/it] 82%|████████▎ | 33/40 [10:05<02:24, 20.67s/it] 85%|████████▌ | 34/40 [10:22<01:58, 19.77s/it] 88%|████████▊ | 35/40 [10:38<01:32, 18.55s/it] 90%|█████████ | 36/40 [10:56<01:12, 18.23s/it] 92%|█████████▎| 37/40 [11:12<00:53, 17.74s/it] 95%|█████████▌| 38/40 [11:29<00:34, 17.47s/it] 98%|█████████▊| 39/40 [11:46<00:17, 17.26s/it]100%|██████████| 40/40 [12:02<00:00, 16.95s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:02<00:00, 16.95s/it]100%|██████████| 40/40 [12:02<00:00, 18.06s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 722.5432, 'train_samples_per_second': 3.882, 'train_steps_per_second': 0.055, 'train_loss': 0.7537886142730713, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:25, 17.57s/it]  5%|▌         | 2/40 [00:35<11:22, 17.96s/it]  8%|▊         | 3/40 [00:54<11:23, 18.48s/it] 10%|█         | 4/40 [01:12<10:58, 18.30s/it] 12%|█▎        | 5/40 [01:31<10:39, 18.29s/it] 15%|█▌        | 6/40 [01:48<10:13, 18.05s/it] 18%|█▊        | 7/40 [02:06<09:49, 17.86s/it] 20%|██        | 8/40 [02:23<09:22, 17.57s/it] 22%|██▎       | 9/40 [02:55<11:26, 22.13s/it] 25%|██▌       | 10/40 [03:13<10:30, 21.01s/it] 28%|██▊       | 11/40 [03:31<09:36, 19.88s/it] 30%|███       | 12/40 [03:48<08:58, 19.22s/it] 32%|███▎      | 13/40 [04:06<08:25, 18.73s/it] 35%|███▌      | 14/40 [04:25<08:09, 18.81s/it] 38%|███▊      | 15/40 [04:42<07:38, 18.35s/it] 40%|████      | 16/40 [05:01<07:19, 18.31s/it] 42%|████▎     | 17/40 [05:33<08:39, 22.60s/it] 45%|████▌     | 18/40 [05:51<07:48, 21.29s/it] 48%|████▊     | 19/40 [06:09<07:02, 20.14s/it] 50%|█████     | 20/40 [06:27<06:32, 19.64s/it] 52%|█████▎    | 21/40 [06:45<06:01, 19.03s/it] 55%|█████▌    | 22/40 [07:04<05:41, 18.99s/it] 57%|█████▊    | 23/40 [07:21<05:12, 18.41s/it] 60%|██████    | 24/40 [07:38<04:49, 18.11s/it] 62%|██████▎   | 25/40 [08:07<05:21, 21.45s/it] 65%|██████▌   | 26/40 [08:26<04:46, 20.46s/it] 68%|██████▊   | 27/40 [08:44<04:19, 19.93s/it] 70%|███████   | 28/40 [09:02<03:51, 19.28s/it] 72%|███████▎  | 29/40 [09:21<03:29, 19.06s/it] 75%|███████▌  | 30/40 [09:38<03:06, 18.66s/it] 78%|███████▊  | 31/40 [09:56<02:45, 18.39s/it] 80%|████████  | 32/40 [10:14<02:26, 18.32s/it] 82%|████████▎ | 33/40 [10:45<02:34, 22.10s/it] 85%|████████▌ | 34/40 [11:04<02:06, 21.13s/it] 88%|████████▊ | 35/40 [11:21<01:39, 19.98s/it] 90%|█████████ | 36/40 [11:40<01:18, 19.65s/it] 92%|█████████▎| 37/40 [11:58<00:57, 19.07s/it] 95%|█████████▌| 38/40 [12:16<00:37, 18.72s/it] 98%|█████████▊| 39/40 [12:34<00:18, 18.50s/it]100%|██████████| 40/40 [12:51<00:00, 18.15s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:51<00:00, 18.15s/it]100%|██████████| 40/40 [12:51<00:00, 19.29s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 771.6828, 'train_samples_per_second': 3.635, 'train_steps_per_second': 0.052, 'train_loss': 0.7791856288909912, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:14, 12.68s/it]  5%|▌         | 2/40 [00:25<08:08, 12.86s/it]  8%|▊         | 3/40 [00:38<07:49, 12.68s/it] 10%|█         | 4/40 [00:50<07:26, 12.41s/it] 12%|█▎        | 5/40 [01:02<07:17, 12.50s/it] 15%|█▌        | 6/40 [01:15<07:05, 12.52s/it] 18%|█▊        | 7/40 [01:27<06:52, 12.50s/it] 20%|██        | 8/40 [01:41<06:54, 12.96s/it] 22%|██▎       | 9/40 [01:56<06:54, 13.38s/it] 25%|██▌       | 10/40 [02:08<06:36, 13.22s/it] 28%|██▊       | 11/40 [02:19<06:01, 12.48s/it] 30%|███       | 12/40 [02:30<05:37, 12.05s/it] 32%|███▎      | 13/40 [02:42<05:26, 12.08s/it] 35%|███▌      | 14/40 [02:55<05:18, 12.26s/it] 38%|███▊      | 15/40 [03:09<05:15, 12.64s/it] 40%|████      | 16/40 [03:21<05:03, 12.64s/it] 42%|████▎     | 17/40 [03:36<05:01, 13.12s/it] 45%|████▌     | 18/40 [03:49<04:48, 13.09s/it] 48%|████▊     | 19/40 [04:00<04:27, 12.75s/it] 50%|█████     | 20/40 [04:13<04:11, 12.57s/it] 52%|█████▎    | 21/40 [04:26<04:02, 12.77s/it] 55%|█████▌    | 22/40 [04:38<03:47, 12.62s/it] 57%|█████▊    | 23/40 [04:51<03:33, 12.58s/it] 60%|██████    | 24/40 [05:03<03:20, 12.54s/it] 62%|██████▎   | 25/40 [05:17<03:15, 13.01s/it] 65%|██████▌   | 26/40 [05:30<03:01, 12.93s/it] 68%|██████▊   | 27/40 [05:43<02:48, 12.99s/it] 70%|███████   | 28/40 [05:56<02:34, 12.87s/it] 72%|███████▎  | 29/40 [06:08<02:21, 12.84s/it] 75%|███████▌  | 30/40 [06:20<02:06, 12.61s/it] 78%|███████▊  | 31/40 [06:34<01:56, 12.93s/it] 80%|████████  | 32/40 [06:46<01:39, 12.48s/it] 82%|████████▎ | 33/40 [07:00<01:31, 13.05s/it] 85%|████████▌ | 34/40 [07:13<01:17, 12.92s/it] 88%|████████▊ | 35/40 [07:26<01:04, 12.93s/it] 90%|█████████ | 36/40 [07:38<00:50, 12.68s/it] 92%|█████████▎| 37/40 [07:50<00:37, 12.64s/it] 95%|█████████▌| 38/40 [08:03<00:25, 12.69s/it] 98%|█████████▊| 39/40 [08:15<00:12, 12.61s/it]100%|██████████| 40/40 [08:28<00:00, 12.45s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:28<00:00, 12.45s/it]100%|██████████| 40/40 [08:28<00:00, 12.70s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 508.0045, 'train_samples_per_second': 5.128, 'train_steps_per_second': 0.079, 'train_loss': 0.6991657733917236, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<07:57, 12.26s/it]  5%|▌         | 2/40 [00:25<08:00, 12.64s/it]  8%|▊         | 3/40 [00:37<07:44, 12.55s/it] 10%|█         | 4/40 [00:49<07:25, 12.36s/it] 12%|█▎        | 5/40 [01:02<07:15, 12.45s/it] 15%|█▌        | 6/40 [01:14<07:02, 12.41s/it] 18%|█▊        | 7/40 [01:27<06:49, 12.40s/it] 20%|██        | 8/40 [01:40<06:50, 12.84s/it] 22%|██▎       | 9/40 [01:55<06:52, 13.30s/it] 25%|██▌       | 10/40 [02:08<06:37, 13.25s/it] 28%|██▊       | 11/40 [02:20<06:13, 12.88s/it] 30%|███       | 12/40 [02:32<05:53, 12.63s/it] 32%|███▎      | 13/40 [02:45<05:43, 12.71s/it] 35%|███▌      | 14/40 [02:58<05:31, 12.73s/it] 38%|███▊      | 15/40 [03:11<05:23, 12.92s/it] 40%|████      | 16/40 [03:24<05:08, 12.85s/it] 42%|████▎     | 17/40 [03:37<04:59, 13.02s/it] 45%|████▌     | 18/40 [03:50<04:43, 12.89s/it] 48%|████▊     | 19/40 [04:01<04:20, 12.41s/it] 50%|█████     | 20/40 [04:12<04:03, 12.18s/it] 52%|█████▎    | 21/40 [04:25<03:55, 12.37s/it] 55%|█████▌    | 22/40 [04:38<03:41, 12.32s/it] 57%|█████▊    | 23/40 [04:51<03:33, 12.53s/it] 60%|██████    | 24/40 [05:03<03:22, 12.66s/it] 62%|██████▎   | 25/40 [05:18<03:18, 13.21s/it] 65%|██████▌   | 26/40 [05:31<03:03, 13.10s/it] 68%|██████▊   | 27/40 [05:44<02:49, 13.04s/it] 70%|███████   | 28/40 [05:56<02:33, 12.80s/it] 72%|███████▎  | 29/40 [06:09<02:20, 12.74s/it] 75%|███████▌  | 30/40 [06:21<02:06, 12.61s/it] 78%|███████▊  | 31/40 [06:35<01:56, 12.96s/it] 80%|████████  | 32/40 [06:47<01:41, 12.71s/it] 82%|████████▎ | 33/40 [07:01<01:32, 13.23s/it] 85%|████████▌ | 34/40 [07:14<01:18, 13.08s/it] 88%|████████▊ | 35/40 [07:27<01:05, 13.09s/it] 90%|█████████ | 36/40 [07:39<00:51, 12.88s/it] 92%|█████████▎| 37/40 [07:52<00:38, 12.73s/it] 95%|█████████▌| 38/40 [08:04<00:25, 12.68s/it] 98%|█████████▊| 39/40 [08:17<00:12, 12.54s/it]100%|██████████| 40/40 [08:29<00:00, 12.40s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:29<00:00, 12.40s/it]100%|██████████| 40/40 [08:29<00:00, 12.73s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 509.1846, 'train_samples_per_second': 5.116, 'train_steps_per_second': 0.079, 'train_loss': 0.715841817855835, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:40, 17.96s/it]  5%|▌         | 2/40 [00:37<11:50, 18.70s/it]  8%|▊         | 3/40 [00:55<11:30, 18.66s/it] 10%|█         | 4/40 [01:13<10:59, 18.32s/it] 12%|█▎        | 5/40 [01:31<10:39, 18.28s/it] 15%|█▌        | 6/40 [01:50<10:20, 18.26s/it] 18%|█▊        | 7/40 [02:08<10:05, 18.35s/it] 20%|██        | 8/40 [02:29<10:11, 19.11s/it] 22%|██▎       | 9/40 [02:50<10:12, 19.77s/it] 25%|██▌       | 10/40 [03:10<09:53, 19.78s/it] 28%|██▊       | 11/40 [03:27<09:12, 19.04s/it] 30%|███       | 12/40 [03:45<08:42, 18.65s/it] 32%|███▎      | 13/40 [04:04<08:28, 18.83s/it] 35%|███▌      | 14/40 [04:23<08:08, 18.80s/it] 38%|███▊      | 15/40 [04:43<07:59, 19.20s/it] 40%|████      | 16/40 [05:02<07:38, 19.10s/it] 42%|████▎     | 17/40 [05:23<07:34, 19.77s/it] 45%|████▌     | 18/40 [05:43<07:14, 19.73s/it] 48%|████▊     | 19/40 [06:01<06:42, 19.14s/it] 50%|█████     | 20/40 [06:18<06:13, 18.66s/it] 52%|█████▎    | 21/40 [06:38<06:01, 19.02s/it] 55%|█████▌    | 22/40 [06:56<05:37, 18.76s/it] 57%|█████▊    | 23/40 [07:16<05:22, 18.98s/it] 60%|██████    | 24/40 [07:35<05:05, 19.08s/it] 62%|██████▎   | 25/40 [07:56<04:56, 19.80s/it] 65%|██████▌   | 26/40 [08:15<04:33, 19.50s/it] 68%|██████▊   | 27/40 [08:34<04:11, 19.35s/it] 70%|███████   | 28/40 [08:53<03:48, 19.05s/it] 72%|███████▎  | 29/40 [09:11<03:28, 18.98s/it] 75%|███████▌  | 30/40 [09:29<03:05, 18.57s/it] 78%|███████▊  | 31/40 [09:50<02:52, 19.21s/it] 80%|████████  | 32/40 [10:07<02:29, 18.73s/it] 82%|████████▎ | 33/40 [10:29<02:17, 19.62s/it] 85%|████████▌ | 34/40 [10:48<01:57, 19.55s/it] 88%|████████▊ | 35/40 [11:08<01:37, 19.52s/it] 90%|█████████ | 36/40 [11:26<01:16, 19.14s/it] 92%|█████████▎| 37/40 [11:45<00:56, 18.98s/it] 95%|█████████▌| 38/40 [12:04<00:37, 19.00s/it] 98%|█████████▊| 39/40 [12:22<00:18, 18.83s/it]100%|██████████| 40/40 [12:40<00:00, 18.46s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:40<00:00, 18.46s/it]100%|██████████| 40/40 [12:40<00:00, 19.01s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 760.368, 'train_samples_per_second': 3.426, 'train_steps_per_second': 0.053, 'train_loss': 0.694652795791626, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<12:20, 18.98s/it]  5%|▌         | 2/40 [00:39<12:30, 19.74s/it]  8%|▊         | 3/40 [00:58<12:08, 19.70s/it] 10%|█         | 4/40 [01:17<11:39, 19.42s/it] 12%|█▎        | 5/40 [01:37<11:22, 19.50s/it] 15%|█▌        | 6/40 [01:56<11:01, 19.46s/it] 18%|█▊        | 7/40 [02:16<10:42, 19.47s/it] 20%|██        | 8/40 [02:38<10:46, 20.20s/it] 22%|██▎       | 9/40 [03:00<10:48, 20.90s/it] 25%|██▌       | 10/40 [03:21<10:25, 20.86s/it] 28%|██▊       | 11/40 [03:40<09:47, 20.25s/it] 30%|███       | 12/40 [03:58<09:13, 19.78s/it] 32%|███▎      | 13/40 [04:19<08:58, 19.93s/it] 35%|███▌      | 14/40 [04:38<08:36, 19.86s/it] 38%|███▊      | 15/40 [04:59<08:23, 20.16s/it] 40%|████      | 16/40 [05:19<08:02, 20.09s/it] 42%|████▎     | 17/40 [05:42<07:58, 20.79s/it] 45%|████▌     | 18/40 [06:02<07:36, 20.77s/it] 48%|████▊     | 19/40 [06:21<07:05, 20.25s/it] 50%|█████     | 20/40 [06:40<06:35, 19.79s/it] 52%|█████▎    | 21/40 [07:01<06:21, 20.09s/it] 55%|█████▌    | 22/40 [07:20<05:56, 19.80s/it] 57%|█████▊    | 23/40 [07:40<05:39, 19.99s/it] 60%|██████    | 24/40 [08:00<05:19, 19.97s/it] 62%|██████▎   | 25/40 [08:23<05:10, 20.71s/it] 65%|██████▌   | 26/40 [08:43<04:46, 20.48s/it] 68%|██████▊   | 27/40 [09:03<04:23, 20.29s/it] 70%|███████   | 28/40 [09:22<04:00, 20.04s/it] 72%|███████▎  | 29/40 [09:42<03:39, 19.98s/it] 75%|███████▌  | 30/40 [10:01<03:16, 19.64s/it] 78%|███████▊  | 31/40 [10:22<03:02, 20.25s/it] 80%|████████  | 32/40 [10:41<02:37, 19.73s/it] 82%|████████▎ | 33/40 [11:04<02:24, 20.62s/it] 85%|████████▌ | 34/40 [11:24<02:03, 20.50s/it] 88%|████████▊ | 35/40 [11:44<01:42, 20.53s/it] 90%|█████████ | 36/40 [12:04<01:20, 20.17s/it] 92%|█████████▎| 37/40 [12:24<01:00, 20.06s/it] 95%|█████████▌| 38/40 [12:44<00:40, 20.12s/it] 98%|█████████▊| 39/40 [13:04<00:20, 20.09s/it]100%|██████████| 40/40 [13:23<00:00, 19.71s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:23<00:00, 19.71s/it]100%|██████████| 40/40 [13:23<00:00, 20.08s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 803.2041, 'train_samples_per_second': 3.243, 'train_steps_per_second': 0.05, 'train_loss': 0.7519645690917969, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:41, 11.85s/it]  5%|▌         | 2/40 [00:25<08:04, 12.75s/it]  8%|▊         | 3/40 [00:37<07:45, 12.58s/it] 10%|█         | 4/40 [00:49<07:25, 12.39s/it] 12%|█▎        | 5/40 [01:01<07:10, 12.31s/it] 15%|█▌        | 6/40 [01:14<07:00, 12.36s/it] 18%|█▊        | 7/40 [01:26<06:46, 12.33s/it] 20%|██        | 8/40 [01:40<06:48, 12.75s/it] 22%|██▎       | 9/40 [01:53<06:40, 12.93s/it] 25%|██▌       | 10/40 [02:05<06:21, 12.71s/it] 28%|██▊       | 11/40 [02:18<06:11, 12.81s/it] 30%|███       | 12/40 [02:31<05:56, 12.74s/it] 32%|███▎      | 13/40 [02:44<05:44, 12.76s/it] 35%|███▌      | 14/40 [02:56<05:31, 12.76s/it] 38%|███▊      | 15/40 [03:09<05:18, 12.74s/it] 40%|████      | 16/40 [03:22<05:06, 12.79s/it] 42%|████▎     | 17/40 [03:36<05:01, 13.11s/it] 45%|████▌     | 18/40 [03:50<04:51, 13.25s/it] 48%|████▊     | 19/40 [04:02<04:33, 13.03s/it] 50%|█████     | 20/40 [04:14<04:16, 12.85s/it] 52%|█████▎    | 21/40 [04:27<03:59, 12.63s/it] 55%|█████▌    | 22/40 [04:39<03:45, 12.50s/it] 57%|█████▊    | 23/40 [04:51<03:30, 12.41s/it] 60%|██████    | 24/40 [05:04<03:23, 12.69s/it] 62%|██████▎   | 25/40 [05:19<03:18, 13.21s/it] 65%|██████▌   | 26/40 [05:31<03:02, 13.07s/it] 68%|██████▊   | 27/40 [05:44<02:49, 13.04s/it] 70%|███████   | 28/40 [05:56<02:30, 12.53s/it] 72%|███████▎  | 29/40 [06:09<02:18, 12.61s/it] 75%|███████▌  | 30/40 [06:21<02:06, 12.60s/it] 78%|███████▊  | 31/40 [06:34<01:54, 12.77s/it] 80%|████████  | 32/40 [06:47<01:40, 12.62s/it] 82%|████████▎ | 33/40 [07:00<01:30, 12.89s/it] 85%|████████▌ | 34/40 [07:13<01:17, 12.93s/it] 88%|████████▊ | 35/40 [07:26<01:05, 13.01s/it] 90%|█████████ | 36/40 [07:39<00:51, 12.91s/it] 92%|█████████▎| 37/40 [07:52<00:38, 12.83s/it] 95%|█████████▌| 38/40 [08:05<00:25, 12.86s/it] 98%|█████████▊| 39/40 [08:17<00:12, 12.69s/it]100%|██████████| 40/40 [08:28<00:00, 12.33s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:28<00:00, 12.33s/it]100%|██████████| 40/40 [08:28<00:00, 12.72s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 508.8764, 'train_samples_per_second': 5.119, 'train_steps_per_second': 0.079, 'train_loss': 0.6992891788482666, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:42, 11.86s/it]  5%|▌         | 2/40 [00:25<08:06, 12.80s/it]  8%|▊         | 3/40 [00:37<07:46, 12.60s/it] 10%|█         | 4/40 [00:49<07:25, 12.37s/it] 12%|█▎        | 5/40 [01:01<07:10, 12.30s/it] 15%|█▌        | 6/40 [01:14<06:59, 12.35s/it] 18%|█▊        | 7/40 [01:26<06:46, 12.33s/it] 20%|██        | 8/40 [01:40<06:50, 12.83s/it] 22%|██▎       | 9/40 [01:53<06:43, 13.01s/it] 25%|██▌       | 10/40 [02:06<06:26, 12.89s/it] 28%|██▊       | 11/40 [02:19<06:14, 12.92s/it] 30%|███       | 12/40 [02:31<05:57, 12.77s/it] 32%|███▎      | 13/40 [02:44<05:42, 12.70s/it] 35%|███▌      | 14/40 [02:57<05:32, 12.78s/it] 38%|███▊      | 15/40 [03:10<05:20, 12.82s/it] 40%|████      | 16/40 [03:23<05:08, 12.85s/it] 42%|████▎     | 17/40 [03:36<05:00, 13.06s/it] 45%|████▌     | 18/40 [03:50<04:49, 13.18s/it] 48%|████▊     | 19/40 [04:02<04:32, 12.96s/it] 50%|█████     | 20/40 [04:15<04:16, 12.83s/it] 52%|█████▎    | 21/40 [04:27<04:02, 12.78s/it] 55%|█████▌    | 22/40 [04:40<03:47, 12.66s/it] 57%|█████▊    | 23/40 [04:52<03:30, 12.39s/it] 60%|██████    | 24/40 [05:05<03:23, 12.72s/it] 62%|██████▎   | 25/40 [05:19<03:18, 13.22s/it] 65%|██████▌   | 26/40 [05:32<03:04, 13.16s/it] 68%|██████▊   | 27/40 [05:45<02:49, 13.05s/it] 70%|███████   | 28/40 [05:57<02:31, 12.59s/it] 72%|███████▎  | 29/40 [06:10<02:19, 12.66s/it] 75%|███████▌  | 30/40 [06:22<02:06, 12.61s/it] 78%|███████▊  | 31/40 [06:35<01:54, 12.76s/it] 80%|████████  | 32/40 [06:48<01:41, 12.64s/it] 82%|████████▎ | 33/40 [07:01<01:30, 12.93s/it] 85%|████████▌ | 34/40 [07:14<01:17, 12.94s/it] 88%|████████▊ | 35/40 [07:27<01:05, 13.05s/it] 90%|█████████ | 36/40 [07:40<00:51, 12.91s/it] 92%|█████████▎| 37/40 [07:53<00:39, 13.03s/it] 95%|█████████▌| 38/40 [08:07<00:26, 13.10s/it] 98%|█████████▊| 39/40 [08:19<00:12, 12.98s/it]100%|██████████| 40/40 [08:31<00:00, 12.55s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:31<00:00, 12.55s/it]100%|██████████| 40/40 [08:31<00:00, 12.78s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 511.3472, 'train_samples_per_second': 5.094, 'train_steps_per_second': 0.078, 'train_loss': 0.7158164024353028, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:25, 17.59s/it]  5%|▌         | 2/40 [00:37<12:03, 19.03s/it]  8%|▊         | 3/40 [00:56<11:33, 18.75s/it] 10%|█         | 4/40 [01:13<11:02, 18.41s/it] 12%|█▎        | 5/40 [01:32<10:41, 18.32s/it] 15%|█▌        | 6/40 [01:50<10:25, 18.38s/it] 18%|█▊        | 7/40 [02:08<10:05, 18.36s/it] 20%|██        | 8/40 [02:29<10:11, 19.10s/it] 22%|██▎       | 9/40 [02:49<09:58, 19.31s/it] 25%|██▌       | 10/40 [03:08<09:36, 19.23s/it] 28%|██▊       | 11/40 [03:27<09:17, 19.23s/it] 30%|███       | 12/40 [03:45<08:49, 18.91s/it] 32%|███▎      | 13/40 [04:04<08:30, 18.93s/it] 35%|███▌      | 14/40 [04:23<08:10, 18.87s/it] 38%|███▊      | 15/40 [04:41<07:48, 18.74s/it] 40%|████      | 16/40 [05:01<07:33, 18.90s/it] 42%|████▎     | 17/40 [05:20<07:19, 19.11s/it] 45%|████▌     | 18/40 [05:41<07:09, 19.51s/it] 48%|████▊     | 19/40 [05:59<06:43, 19.20s/it] 50%|█████     | 20/40 [06:18<06:20, 19.03s/it] 52%|█████▎    | 21/40 [06:37<06:01, 19.01s/it] 55%|█████▌    | 22/40 [06:55<05:35, 18.64s/it] 57%|█████▊    | 23/40 [07:12<05:11, 18.34s/it] 60%|██████    | 24/40 [07:33<05:04, 19.00s/it] 62%|██████▎   | 25/40 [07:54<04:56, 19.76s/it] 65%|██████▌   | 26/40 [08:13<04:33, 19.55s/it] 68%|██████▊   | 27/40 [08:33<04:13, 19.47s/it] 70%|███████   | 28/40 [08:50<03:45, 18.75s/it] 72%|███████▎  | 29/40 [09:09<03:27, 18.85s/it] 75%|███████▌  | 30/40 [09:27<03:07, 18.72s/it] 78%|███████▊  | 31/40 [09:46<02:48, 18.77s/it] 80%|████████  | 32/40 [10:05<02:29, 18.68s/it] 82%|████████▎ | 33/40 [10:25<02:13, 19.09s/it] 85%|████████▌ | 34/40 [10:45<01:56, 19.38s/it] 88%|████████▊ | 35/40 [11:04<01:36, 19.36s/it] 90%|█████████ | 36/40 [11:23<01:17, 19.26s/it] 92%|█████████▎| 37/40 [11:42<00:57, 19.21s/it] 95%|█████████▌| 38/40 [12:02<00:38, 19.27s/it] 98%|█████████▊| 39/40 [12:20<00:19, 19.12s/it]100%|██████████| 40/40 [12:38<00:00, 18.56s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:38<00:00, 18.56s/it]100%|██████████| 40/40 [12:38<00:00, 18.95s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 758.1091, 'train_samples_per_second': 3.436, 'train_steps_per_second': 0.053, 'train_loss': 0.6959497451782226, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<12:15, 18.86s/it]  5%|▌         | 2/40 [00:39<12:47, 20.19s/it]  8%|▊         | 3/40 [00:59<12:19, 19.97s/it] 10%|█         | 4/40 [01:18<11:40, 19.46s/it] 12%|█▎        | 5/40 [01:37<11:20, 19.43s/it] 15%|█▌        | 6/40 [01:57<11:03, 19.53s/it] 18%|█▊        | 7/40 [02:16<10:41, 19.43s/it] 20%|██        | 8/40 [02:38<10:43, 20.12s/it] 22%|██▎       | 9/40 [02:59<10:35, 20.50s/it] 25%|██▌       | 10/40 [03:19<10:08, 20.28s/it] 28%|██▊       | 11/40 [03:39<09:49, 20.32s/it] 30%|███       | 12/40 [03:59<09:22, 20.08s/it] 32%|███▎      | 13/40 [04:19<09:01, 20.05s/it] 35%|███▌      | 14/40 [04:38<08:38, 19.93s/it] 38%|███▊      | 15/40 [04:58<08:16, 19.86s/it] 40%|████      | 16/40 [05:19<08:02, 20.10s/it] 42%|████▎     | 17/40 [05:40<07:49, 20.43s/it] 45%|████▌     | 18/40 [06:01<07:36, 20.74s/it] 48%|████▊     | 19/40 [06:21<07:07, 20.37s/it] 50%|█████     | 20/40 [06:41<06:43, 20.18s/it] 52%|█████▎    | 21/40 [07:01<06:23, 20.18s/it] 55%|█████▌    | 22/40 [07:20<05:57, 19.84s/it] 57%|█████▊    | 23/40 [07:39<05:35, 19.71s/it] 60%|██████    | 24/40 [08:01<05:24, 20.27s/it] 62%|██████▎   | 25/40 [08:24<05:14, 20.98s/it] 65%|██████▌   | 26/40 [08:44<04:50, 20.75s/it] 68%|██████▊   | 27/40 [09:04<04:28, 20.66s/it] 70%|███████   | 28/40 [09:22<03:58, 19.90s/it] 72%|███████▎  | 29/40 [09:42<03:39, 19.94s/it] 75%|███████▌  | 30/40 [10:02<03:17, 19.75s/it] 78%|███████▊  | 31/40 [10:22<02:58, 19.85s/it] 80%|████████  | 32/40 [10:42<02:38, 19.85s/it] 82%|████████▎ | 33/40 [11:03<02:22, 20.30s/it] 85%|████████▌ | 34/40 [11:24<02:02, 20.46s/it] 88%|████████▊ | 35/40 [11:44<01:42, 20.43s/it] 90%|█████████ | 36/40 [12:04<01:21, 20.28s/it] 92%|█████████▎| 37/40 [12:24<01:00, 20.22s/it] 95%|█████████▌| 38/40 [12:44<00:40, 20.20s/it] 98%|█████████▊| 39/40 [13:04<00:20, 20.03s/it]100%|██████████| 40/40 [13:22<00:00, 19.49s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:22<00:00, 19.49s/it]100%|██████████| 40/40 [13:22<00:00, 20.07s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 802.7386, 'train_samples_per_second': 3.245, 'train_steps_per_second': 0.05, 'train_loss': 0.751447057723999, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:32, 11.61s/it]  5%|▌         | 2/40 [00:22<07:14, 11.42s/it]  8%|▊         | 3/40 [00:34<06:57, 11.29s/it] 10%|█         | 4/40 [00:45<06:50, 11.40s/it] 12%|█▎        | 5/40 [00:57<06:42, 11.51s/it] 15%|█▌        | 6/40 [01:08<06:27, 11.41s/it] 18%|█▊        | 7/40 [01:19<06:07, 11.13s/it] 20%|██        | 8/40 [01:30<06:02, 11.32s/it] 22%|██▎       | 9/40 [01:51<07:19, 14.18s/it] 25%|██▌       | 10/40 [02:02<06:36, 13.21s/it] 28%|██▊       | 11/40 [02:13<06:06, 12.63s/it] 30%|███       | 12/40 [02:25<05:45, 12.32s/it] 32%|███▎      | 13/40 [02:36<05:27, 12.15s/it] 35%|███▌      | 14/40 [02:48<05:10, 11.92s/it] 38%|███▊      | 15/40 [02:59<04:53, 11.75s/it] 40%|████      | 16/40 [03:11<04:41, 11.73s/it] 42%|████▎     | 17/40 [03:32<05:31, 14.40s/it] 45%|████▌     | 18/40 [03:43<04:56, 13.49s/it] 48%|████▊     | 19/40 [03:54<04:28, 12.79s/it] 50%|█████     | 20/40 [04:06<04:09, 12.46s/it] 52%|█████▎    | 21/40 [04:18<03:52, 12.25s/it] 55%|█████▌    | 22/40 [04:29<03:38, 12.16s/it] 57%|█████▊    | 23/40 [04:41<03:21, 11.85s/it] 60%|██████    | 24/40 [04:52<03:06, 11.66s/it] 62%|██████▎   | 25/40 [05:12<03:32, 14.17s/it] 65%|██████▌   | 26/40 [05:23<03:04, 13.20s/it] 68%|██████▊   | 27/40 [05:34<02:43, 12.60s/it] 70%|███████   | 28/40 [05:46<02:28, 12.35s/it] 72%|███████▎  | 29/40 [05:57<02:13, 12.14s/it] 75%|███████▌  | 30/40 [06:09<01:59, 11.95s/it] 78%|███████▊  | 31/40 [06:20<01:45, 11.72s/it] 80%|████████  | 32/40 [06:32<01:35, 11.92s/it] 82%|████████▎ | 33/40 [06:52<01:40, 14.30s/it] 85%|████████▌ | 34/40 [07:05<01:22, 13.71s/it] 88%|████████▊ | 35/40 [07:16<01:04, 12.92s/it] 90%|█████████ | 36/40 [07:27<00:49, 12.48s/it] 92%|█████████▎| 37/40 [07:39<00:36, 12.16s/it] 95%|█████████▌| 38/40 [07:50<00:23, 11.99s/it] 98%|█████████▊| 39/40 [08:02<00:11, 11.91s/it]100%|██████████| 40/40 [08:13<00:00, 11.57s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:13<00:00, 11.57s/it]100%|██████████| 40/40 [08:13<00:00, 12.33s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 493.1887, 'train_samples_per_second': 5.687, 'train_steps_per_second': 0.081, 'train_loss': 0.7463566303253174, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:39, 11.78s/it]  5%|▌         | 2/40 [00:23<07:17, 11.52s/it]  8%|▊         | 3/40 [00:34<07:03, 11.44s/it] 10%|█         | 4/40 [00:46<06:56, 11.57s/it] 12%|█▎        | 5/40 [00:58<06:47, 11.65s/it] 15%|█▌        | 6/40 [01:08<06:27, 11.39s/it] 18%|█▊        | 7/40 [01:19<06:05, 11.09s/it] 20%|██        | 8/40 [01:31<06:01, 11.31s/it] 22%|██▎       | 9/40 [01:51<07:20, 14.22s/it] 25%|██▌       | 10/40 [02:03<06:39, 13.31s/it] 28%|██▊       | 11/40 [02:14<06:08, 12.72s/it] 30%|███       | 12/40 [02:26<05:48, 12.46s/it] 32%|███▎      | 13/40 [02:38<05:30, 12.24s/it] 35%|███▌      | 14/40 [02:49<05:12, 12.03s/it] 38%|███▊      | 15/40 [03:00<04:56, 11.84s/it] 40%|████      | 16/40 [03:12<04:44, 11.86s/it] 42%|████▎     | 17/40 [03:33<05:30, 14.37s/it] 45%|████▌     | 18/40 [03:44<04:55, 13.45s/it] 48%|████▊     | 19/40 [03:55<04:28, 12.81s/it] 50%|█████     | 20/40 [04:07<04:09, 12.49s/it] 52%|█████▎    | 21/40 [04:18<03:48, 12.05s/it] 55%|█████▌    | 22/40 [04:30<03:35, 11.97s/it] 57%|█████▊    | 23/40 [04:41<03:20, 11.77s/it] 60%|██████    | 24/40 [04:52<03:05, 11.59s/it] 62%|██████▎   | 25/40 [05:12<03:32, 14.17s/it] 65%|██████▌   | 26/40 [05:23<03:03, 13.14s/it] 68%|██████▊   | 27/40 [05:34<02:42, 12.52s/it] 70%|███████   | 28/40 [05:46<02:27, 12.33s/it] 72%|███████▎  | 29/40 [05:58<02:13, 12.17s/it] 75%|███████▌  | 30/40 [06:09<01:59, 11.99s/it] 78%|███████▊  | 31/40 [06:21<01:45, 11.74s/it] 80%|████████  | 32/40 [06:33<01:35, 11.90s/it] 82%|████████▎ | 33/40 [06:53<01:39, 14.27s/it] 85%|████████▌ | 34/40 [07:05<01:21, 13.66s/it] 88%|████████▊ | 35/40 [07:16<01:04, 12.89s/it] 90%|█████████ | 36/40 [07:27<00:49, 12.43s/it] 92%|█████████▎| 37/40 [07:38<00:35, 12.00s/it] 95%|█████████▌| 38/40 [07:50<00:23, 11.86s/it] 98%|█████████▊| 39/40 [08:01<00:11, 11.77s/it]100%|██████████| 40/40 [08:12<00:00, 11.52s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:12<00:00, 11.52s/it]100%|██████████| 40/40 [08:12<00:00, 12.32s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 492.921, 'train_samples_per_second': 5.691, 'train_steps_per_second': 0.081, 'train_loss': 0.7508948802947998, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:17, 17.38s/it]  5%|▌         | 2/40 [00:33<10:35, 16.73s/it]  8%|▊         | 3/40 [00:50<10:19, 16.74s/it] 10%|█         | 4/40 [01:07<10:12, 17.02s/it] 12%|█▎        | 5/40 [01:25<10:00, 17.16s/it] 15%|█▌        | 6/40 [01:41<09:32, 16.84s/it] 18%|█▊        | 7/40 [01:56<08:58, 16.31s/it] 20%|██        | 8/40 [02:13<08:49, 16.55s/it] 22%|██▎       | 9/40 [02:43<10:37, 20.57s/it] 25%|██▌       | 10/40 [02:59<09:36, 19.20s/it] 28%|██▊       | 11/40 [03:15<08:51, 18.34s/it] 30%|███       | 12/40 [03:32<08:23, 18.00s/it] 32%|███▎      | 13/40 [03:50<07:59, 17.76s/it] 35%|███▌      | 14/40 [04:07<07:36, 17.57s/it] 38%|███▊      | 15/40 [04:23<07:08, 17.14s/it] 40%|████      | 16/40 [04:40<06:49, 17.05s/it] 42%|████▎     | 17/40 [05:09<07:59, 20.85s/it] 45%|████▌     | 18/40 [05:26<07:09, 19.52s/it] 48%|████▊     | 19/40 [05:42<06:29, 18.56s/it] 50%|█████     | 20/40 [05:59<06:03, 18.17s/it] 52%|█████▎    | 21/40 [06:16<05:36, 17.70s/it] 55%|█████▌    | 22/40 [06:33<05:16, 17.60s/it] 57%|█████▊    | 23/40 [06:50<04:53, 17.27s/it] 60%|██████    | 24/40 [07:06<04:31, 17.00s/it] 62%|██████▎   | 25/40 [07:36<05:11, 20.73s/it] 65%|██████▌   | 26/40 [07:51<04:29, 19.24s/it] 68%|██████▊   | 27/40 [08:07<03:57, 18.23s/it] 70%|███████   | 28/40 [08:24<03:32, 17.67s/it] 72%|███████▎  | 29/40 [08:41<03:13, 17.56s/it] 75%|███████▌  | 30/40 [08:58<02:53, 17.31s/it] 78%|███████▊  | 31/40 [09:14<02:32, 16.92s/it] 80%|████████  | 32/40 [09:32<02:18, 17.27s/it] 82%|████████▎ | 33/40 [10:00<02:24, 20.63s/it] 85%|████████▌ | 34/40 [10:18<01:58, 19.74s/it] 88%|████████▊ | 35/40 [10:34<01:32, 18.58s/it] 90%|█████████ | 36/40 [10:51<01:12, 18.04s/it] 92%|█████████▎| 37/40 [11:07<00:52, 17.51s/it] 95%|█████████▌| 38/40 [11:24<00:34, 17.33s/it] 98%|█████████▊| 39/40 [11:41<00:17, 17.27s/it]100%|██████████| 40/40 [11:57<00:00, 16.90s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [11:57<00:00, 16.90s/it]100%|██████████| 40/40 [11:57<00:00, 17.94s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 717.4883, 'train_samples_per_second': 3.909, 'train_steps_per_second': 0.056, 'train_loss': 0.7541850090026856, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:59, 18.46s/it]  5%|▌         | 2/40 [00:36<11:29, 18.13s/it]  8%|▊         | 3/40 [00:54<11:03, 17.93s/it] 10%|█         | 4/40 [01:12<10:50, 18.07s/it] 12%|█▎        | 5/40 [01:30<10:38, 18.24s/it] 15%|█▌        | 6/40 [01:47<10:06, 17.84s/it] 18%|█▊        | 7/40 [02:04<09:32, 17.35s/it] 20%|██        | 8/40 [02:22<09:26, 17.71s/it] 22%|██▎       | 9/40 [02:54<11:23, 22.06s/it] 25%|██▌       | 10/40 [03:11<10:15, 20.51s/it] 28%|██▊       | 11/40 [03:28<09:27, 19.58s/it] 30%|███       | 12/40 [03:47<08:57, 19.18s/it] 32%|███▎      | 13/40 [04:05<08:31, 18.95s/it] 35%|███▌      | 14/40 [04:23<08:07, 18.77s/it] 38%|███▊      | 15/40 [04:41<07:40, 18.40s/it] 40%|████      | 16/40 [04:59<07:20, 18.37s/it] 42%|████▎     | 17/40 [05:31<08:35, 22.43s/it] 45%|████▌     | 18/40 [05:49<07:44, 21.13s/it] 48%|████▊     | 19/40 [06:07<07:03, 20.15s/it] 50%|█████     | 20/40 [06:26<06:33, 19.67s/it] 52%|█████▎    | 21/40 [06:44<06:03, 19.14s/it] 55%|█████▌    | 22/40 [07:02<05:40, 18.93s/it] 57%|█████▊    | 23/40 [07:19<05:14, 18.50s/it] 60%|██████    | 24/40 [07:37<04:50, 18.17s/it] 62%|██████▎   | 25/40 [08:08<05:32, 22.15s/it] 65%|██████▌   | 26/40 [08:25<04:46, 20.43s/it] 68%|██████▊   | 27/40 [08:42<04:14, 19.59s/it] 70%|███████   | 28/40 [09:00<03:48, 19.07s/it] 72%|███████▎  | 29/40 [09:19<03:27, 18.89s/it] 75%|███████▌  | 30/40 [09:36<03:05, 18.53s/it] 78%|███████▊  | 31/40 [09:54<02:44, 18.26s/it] 80%|████████  | 32/40 [10:13<02:28, 18.51s/it] 82%|████████▎ | 33/40 [10:44<02:35, 22.23s/it] 85%|████████▌ | 34/40 [11:03<02:07, 21.27s/it] 88%|████████▊ | 35/40 [11:20<01:39, 19.96s/it] 90%|█████████ | 36/40 [11:38<01:17, 19.33s/it] 92%|█████████▎| 37/40 [11:56<00:56, 18.87s/it] 95%|█████████▌| 38/40 [12:14<00:37, 18.67s/it] 98%|█████████▊| 39/40 [12:32<00:18, 18.45s/it]100%|██████████| 40/40 [12:49<00:00, 18.10s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:49<00:00, 18.10s/it]100%|██████████| 40/40 [12:49<00:00, 19.24s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 769.5486, 'train_samples_per_second': 3.645, 'train_steps_per_second': 0.052, 'train_loss': 0.7793676853179932, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:19, 11.28s/it]  5%|▌         | 2/40 [00:22<07:14, 11.45s/it]  8%|▊         | 3/40 [00:34<07:10, 11.64s/it] 10%|█         | 4/40 [00:46<06:54, 11.50s/it] 12%|█▎        | 5/40 [00:57<06:48, 11.68s/it] 15%|█▌        | 6/40 [01:08<06:26, 11.37s/it] 18%|█▊        | 7/40 [01:20<06:17, 11.43s/it] 20%|██        | 8/40 [01:32<06:08, 11.52s/it] 22%|██▎       | 9/40 [01:51<07:13, 13.97s/it] 25%|██▌       | 10/40 [02:02<06:35, 13.18s/it] 28%|██▊       | 11/40 [02:14<06:06, 12.64s/it] 30%|███       | 12/40 [02:26<05:48, 12.45s/it] 32%|███▎      | 13/40 [02:37<05:28, 12.18s/it] 35%|███▌      | 14/40 [02:49<05:13, 12.06s/it] 38%|███▊      | 15/40 [03:00<04:51, 11.68s/it] 40%|████      | 16/40 [03:12<04:45, 11.88s/it] 42%|████▎     | 17/40 [03:32<05:30, 14.37s/it] 45%|████▌     | 18/40 [03:44<04:55, 13.41s/it] 48%|████▊     | 19/40 [03:55<04:29, 12.84s/it] 50%|█████     | 20/40 [04:07<04:10, 12.53s/it] 52%|█████▎    | 21/40 [04:18<03:50, 12.12s/it] 55%|█████▌    | 22/40 [04:30<03:37, 12.10s/it] 57%|█████▊    | 23/40 [04:41<03:20, 11.80s/it] 60%|██████    | 24/40 [04:53<03:10, 11.89s/it] 62%|██████▎   | 25/40 [05:14<03:37, 14.50s/it] 65%|██████▌   | 26/40 [05:25<03:09, 13.56s/it] 68%|██████▊   | 27/40 [05:37<02:48, 12.98s/it] 70%|███████   | 28/40 [05:48<02:30, 12.50s/it] 72%|███████▎  | 29/40 [06:00<02:14, 12.22s/it] 75%|███████▌  | 30/40 [06:11<01:59, 11.99s/it] 78%|███████▊  | 31/40 [06:22<01:45, 11.74s/it] 80%|████████  | 32/40 [06:34<01:34, 11.83s/it] 82%|████████▎ | 33/40 [06:54<01:39, 14.15s/it] 85%|████████▌ | 34/40 [07:06<01:21, 13.51s/it] 88%|████████▊ | 35/40 [07:18<01:04, 12.91s/it] 90%|█████████ | 36/40 [07:28<00:49, 12.31s/it] 92%|█████████▎| 37/40 [07:40<00:35, 11.97s/it] 95%|█████████▌| 38/40 [07:51<00:23, 11.84s/it] 98%|█████████▊| 39/40 [08:03<00:11, 11.90s/it]100%|██████████| 40/40 [08:14<00:00, 11.69s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:14<00:00, 11.69s/it]100%|██████████| 40/40 [08:14<00:00, 12.37s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 494.9333, 'train_samples_per_second': 5.667, 'train_steps_per_second': 0.081, 'train_loss': 0.7461617469787598, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:13, 11.11s/it]  5%|▌         | 2/40 [00:22<06:58, 11.02s/it]  8%|▊         | 3/40 [00:33<06:59, 11.33s/it] 10%|█         | 4/40 [00:45<06:48, 11.35s/it] 12%|█▎        | 5/40 [00:57<06:44, 11.57s/it] 15%|█▌        | 6/40 [01:07<06:23, 11.28s/it] 18%|█▊        | 7/40 [01:19<06:14, 11.36s/it] 20%|██        | 8/40 [01:31<06:07, 11.50s/it] 22%|██▎       | 9/40 [01:50<07:10, 13.88s/it] 25%|██▌       | 10/40 [02:01<06:32, 13.10s/it] 28%|██▊       | 11/40 [02:11<05:55, 12.26s/it] 30%|███       | 12/40 [02:23<05:39, 12.13s/it] 32%|███▎      | 13/40 [02:35<05:20, 11.88s/it] 35%|███▌      | 14/40 [02:47<05:09, 11.92s/it] 38%|███▊      | 15/40 [02:57<04:49, 11.57s/it] 40%|████      | 16/40 [03:10<04:44, 11.84s/it] 42%|████▎     | 17/40 [03:30<05:30, 14.35s/it] 45%|████▌     | 18/40 [03:41<04:54, 13.38s/it] 48%|████▊     | 19/40 [03:53<04:28, 12.77s/it] 50%|█████     | 20/40 [04:04<04:07, 12.37s/it] 52%|█████▎    | 21/40 [04:15<03:47, 11.98s/it] 55%|█████▌    | 22/40 [04:27<03:36, 12.03s/it] 57%|█████▊    | 23/40 [04:38<03:20, 11.82s/it] 60%|██████    | 24/40 [04:51<03:10, 11.88s/it] 62%|██████▎   | 25/40 [05:11<03:37, 14.52s/it] 65%|██████▌   | 26/40 [05:22<03:08, 13.47s/it] 68%|██████▊   | 27/40 [05:34<02:47, 12.89s/it] 70%|███████   | 28/40 [05:45<02:28, 12.41s/it] 72%|███████▎  | 29/40 [05:56<02:12, 12.07s/it] 75%|███████▌  | 30/40 [06:08<01:59, 11.91s/it] 78%|███████▊  | 31/40 [06:19<01:44, 11.61s/it] 80%|████████  | 32/40 [06:31<01:33, 11.70s/it] 82%|████████▎ | 33/40 [06:51<01:38, 14.14s/it] 85%|████████▌ | 34/40 [07:02<01:20, 13.48s/it] 88%|████████▊ | 35/40 [07:14<01:04, 12.89s/it] 90%|█████████ | 36/40 [07:25<00:49, 12.35s/it] 92%|█████████▎| 37/40 [07:36<00:36, 12.05s/it] 95%|█████████▌| 38/40 [07:48<00:24, 12.02s/it] 98%|█████████▊| 39/40 [08:00<00:12, 12.05s/it]100%|██████████| 40/40 [08:11<00:00, 11.71s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:11<00:00, 11.71s/it]100%|██████████| 40/40 [08:11<00:00, 12.30s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 491.903, 'train_samples_per_second': 5.702, 'train_steps_per_second': 0.081, 'train_loss': 0.7505679607391358, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:16<11:01, 16.97s/it]  5%|▌         | 2/40 [00:33<10:33, 16.67s/it]  8%|▊         | 3/40 [00:50<10:23, 16.85s/it] 10%|█         | 4/40 [01:07<10:03, 16.76s/it] 12%|█▎        | 5/40 [01:24<10:00, 17.15s/it] 15%|█▌        | 6/40 [01:40<09:27, 16.70s/it] 18%|█▊        | 7/40 [01:57<09:16, 16.86s/it] 20%|██        | 8/40 [02:15<09:03, 16.98s/it] 22%|██▎       | 9/40 [02:43<10:31, 20.38s/it] 25%|██▌       | 10/40 [03:00<09:39, 19.32s/it] 28%|██▊       | 11/40 [03:16<08:55, 18.46s/it] 30%|███       | 12/40 [03:34<08:33, 18.32s/it] 32%|███▎      | 13/40 [03:51<08:02, 17.86s/it] 35%|███▌      | 14/40 [04:08<07:37, 17.59s/it] 38%|███▊      | 15/40 [04:23<07:03, 16.93s/it] 40%|████      | 16/40 [04:42<06:57, 17.39s/it] 42%|████▎     | 17/40 [05:11<08:02, 20.96s/it] 45%|████▌     | 18/40 [05:27<07:11, 19.63s/it] 48%|████▊     | 19/40 [05:44<06:34, 18.78s/it] 50%|█████     | 20/40 [06:01<06:05, 18.26s/it] 52%|█████▎    | 21/40 [06:17<05:34, 17.62s/it] 55%|█████▌    | 22/40 [06:35<05:18, 17.69s/it] 57%|█████▊    | 23/40 [06:52<04:55, 17.35s/it] 60%|██████    | 24/40 [07:10<04:39, 17.48s/it] 62%|██████▎   | 25/40 [07:39<05:16, 21.13s/it] 65%|██████▌   | 26/40 [07:55<04:33, 19.53s/it] 68%|██████▊   | 27/40 [08:12<04:04, 18.82s/it] 70%|███████   | 28/40 [08:29<03:37, 18.11s/it] 72%|███████▎  | 29/40 [08:46<03:16, 17.88s/it] 75%|███████▌  | 30/40 [09:03<02:55, 17.54s/it] 78%|███████▊  | 31/40 [09:19<02:33, 17.09s/it] 80%|████████  | 32/40 [09:37<02:18, 17.29s/it] 82%|████████▎ | 33/40 [10:05<02:24, 20.68s/it] 85%|████████▌ | 34/40 [10:23<01:58, 19.83s/it] 88%|████████▊ | 35/40 [10:40<01:34, 18.89s/it] 90%|█████████ | 36/40 [10:56<01:12, 18.00s/it] 92%|█████████▎| 37/40 [11:12<00:52, 17.52s/it] 95%|█████████▌| 38/40 [11:30<00:35, 17.52s/it] 98%|█████████▊| 39/40 [11:48<00:17, 17.71s/it]100%|██████████| 40/40 [12:04<00:00, 17.22s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:04<00:00, 17.22s/it]100%|██████████| 40/40 [12:04<00:00, 18.11s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 724.2932, 'train_samples_per_second': 3.873, 'train_steps_per_second': 0.055, 'train_loss': 0.7539309978485107, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:50, 18.21s/it]  5%|▌         | 2/40 [00:36<11:24, 18.01s/it]  8%|▊         | 3/40 [00:54<11:09, 18.09s/it] 10%|█         | 4/40 [01:12<10:47, 17.97s/it] 12%|█▎        | 5/40 [01:30<10:39, 18.27s/it] 15%|█▌        | 6/40 [01:47<10:03, 17.74s/it] 18%|█▊        | 7/40 [02:05<09:49, 17.86s/it] 20%|██        | 8/40 [02:23<09:34, 17.95s/it] 22%|██▎       | 9/40 [02:53<11:06, 21.51s/it] 25%|██▌       | 10/40 [03:11<10:14, 20.47s/it] 28%|██▊       | 11/40 [03:29<09:29, 19.64s/it] 30%|███       | 12/40 [03:48<09:04, 19.46s/it] 32%|███▎      | 13/40 [04:05<08:32, 18.99s/it] 35%|███▌      | 14/40 [04:24<08:08, 18.78s/it] 38%|███▊      | 15/40 [04:41<07:35, 18.22s/it] 40%|████      | 16/40 [05:00<07:28, 18.67s/it] 42%|████▎     | 17/40 [05:32<08:37, 22.51s/it] 45%|████▌     | 18/40 [05:50<07:44, 21.11s/it] 48%|████▊     | 19/40 [06:07<07:02, 20.10s/it] 50%|█████     | 20/40 [06:25<06:29, 19.46s/it] 52%|█████▎    | 21/40 [06:43<05:57, 18.82s/it] 55%|█████▌    | 22/40 [07:02<05:38, 18.81s/it] 57%|█████▊    | 23/40 [07:19<05:13, 18.45s/it] 60%|██████    | 24/40 [07:38<04:57, 18.58s/it] 62%|██████▎   | 25/40 [08:09<05:35, 22.35s/it] 65%|██████▌   | 26/40 [08:27<04:51, 20.85s/it] 68%|██████▊   | 27/40 [08:45<04:20, 20.05s/it] 70%|███████   | 28/40 [09:02<03:51, 19.30s/it] 72%|███████▎  | 29/40 [09:21<03:29, 19.06s/it] 75%|███████▌  | 30/40 [09:39<03:08, 18.84s/it] 78%|███████▊  | 31/40 [09:56<02:45, 18.39s/it] 80%|████████  | 32/40 [10:15<02:28, 18.51s/it] 82%|████████▎ | 33/40 [10:46<02:35, 22.17s/it] 85%|████████▌ | 34/40 [11:05<02:07, 21.17s/it] 88%|████████▊ | 35/40 [11:23<01:40, 20.17s/it] 90%|█████████ | 36/40 [11:40<01:17, 19.33s/it] 92%|█████████▎| 37/40 [11:57<00:56, 18.78s/it] 95%|█████████▌| 38/40 [12:16<00:37, 18.83s/it] 98%|█████████▊| 39/40 [12:36<00:18, 18.93s/it]100%|██████████| 40/40 [12:53<00:00, 18.46s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:53<00:00, 18.46s/it]100%|██████████| 40/40 [12:53<00:00, 19.34s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 773.4797, 'train_samples_per_second': 3.626, 'train_steps_per_second': 0.052, 'train_loss': 0.7795104503631591, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:39, 11.79s/it]  5%|▌         | 2/40 [00:24<07:59, 12.61s/it]  8%|▊         | 3/40 [00:37<07:43, 12.54s/it] 10%|█         | 4/40 [00:49<07:24, 12.35s/it] 12%|█▎        | 5/40 [01:02<07:21, 12.61s/it] 15%|█▌        | 6/40 [01:15<07:09, 12.63s/it] 18%|█▊        | 7/40 [01:27<06:56, 12.62s/it] 20%|██        | 8/40 [01:41<06:51, 12.86s/it] 22%|██▎       | 9/40 [01:56<06:58, 13.51s/it] 25%|██▌       | 10/40 [02:08<06:36, 13.21s/it] 28%|██▊       | 11/40 [02:21<06:17, 13.01s/it] 30%|███       | 12/40 [02:33<06:02, 12.93s/it] 32%|███▎      | 13/40 [02:46<05:47, 12.88s/it] 35%|███▌      | 14/40 [02:59<05:31, 12.76s/it] 38%|███▊      | 15/40 [03:12<05:19, 12.79s/it] 40%|████      | 16/40 [03:25<05:08, 12.86s/it] 42%|████▎     | 17/40 [03:39<05:04, 13.24s/it] 45%|████▌     | 18/40 [03:51<04:46, 13.04s/it] 48%|████▊     | 19/40 [04:04<04:34, 13.08s/it] 50%|█████     | 20/40 [04:17<04:18, 12.90s/it] 52%|█████▎    | 21/40 [04:30<04:03, 12.80s/it] 55%|█████▌    | 22/40 [04:42<03:48, 12.67s/it] 57%|█████▊    | 23/40 [04:54<03:34, 12.60s/it] 60%|██████    | 24/40 [05:07<03:24, 12.77s/it] 62%|██████▎   | 25/40 [05:21<03:17, 13.13s/it] 65%|██████▌   | 26/40 [05:34<03:01, 12.94s/it] 68%|██████▊   | 27/40 [05:47<02:48, 12.93s/it] 70%|███████   | 28/40 [05:59<02:31, 12.59s/it] 72%|███████▎  | 29/40 [06:12<02:20, 12.74s/it] 75%|███████▌  | 30/40 [06:24<02:07, 12.72s/it] 78%|███████▊  | 31/40 [06:38<01:55, 12.85s/it] 80%|████████  | 32/40 [06:50<01:42, 12.83s/it] 82%|████████▎ | 33/40 [07:05<01:34, 13.51s/it] 85%|████████▌ | 34/40 [07:18<01:19, 13.19s/it] 88%|████████▊ | 35/40 [07:32<01:06, 13.37s/it] 90%|█████████ | 36/40 [07:44<00:52, 13.07s/it] 92%|█████████▎| 37/40 [07:56<00:38, 12.84s/it] 95%|█████████▌| 38/40 [08:09<00:25, 12.87s/it] 98%|█████████▊| 39/40 [08:22<00:12, 12.79s/it]100%|██████████| 40/40 [08:34<00:00, 12.49s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:34<00:00, 12.49s/it]100%|██████████| 40/40 [08:34<00:00, 12.86s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 514.2285, 'train_samples_per_second': 5.066, 'train_steps_per_second': 0.078, 'train_loss': 0.6989601612091064, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:39, 11.78s/it]  5%|▌         | 2/40 [00:24<07:55, 12.52s/it]  8%|▊         | 3/40 [00:37<07:44, 12.54s/it] 10%|█         | 4/40 [00:49<07:24, 12.34s/it] 12%|█▎        | 5/40 [01:02<07:22, 12.65s/it] 15%|█▌        | 6/40 [01:15<07:10, 12.67s/it] 18%|█▊        | 7/40 [01:27<06:57, 12.64s/it] 20%|██        | 8/40 [01:41<06:51, 12.86s/it] 22%|██▎       | 9/40 [01:56<06:57, 13.47s/it] 25%|██▌       | 10/40 [02:08<06:35, 13.19s/it] 28%|██▊       | 11/40 [02:21<06:17, 13.01s/it] 30%|███       | 12/40 [02:34<06:05, 13.04s/it] 32%|███▎      | 13/40 [02:47<05:52, 13.05s/it] 35%|███▌      | 14/40 [02:59<05:34, 12.86s/it] 38%|███▊      | 15/40 [03:12<05:19, 12.78s/it] 40%|████      | 16/40 [03:25<05:08, 12.84s/it] 42%|████▎     | 17/40 [03:39<05:06, 13.31s/it] 45%|████▌     | 18/40 [03:52<04:48, 13.13s/it] 48%|████▊     | 19/40 [04:05<04:36, 13.15s/it] 50%|█████     | 20/40 [04:18<04:20, 13.05s/it] 52%|█████▎    | 21/40 [04:31<04:05, 12.93s/it] 55%|█████▌    | 22/40 [04:43<03:50, 12.80s/it] 57%|█████▊    | 23/40 [04:56<03:36, 12.72s/it] 60%|██████    | 24/40 [05:09<03:25, 12.86s/it] 62%|██████▎   | 25/40 [05:23<03:17, 13.15s/it] 65%|██████▌   | 26/40 [05:35<03:01, 12.94s/it] 68%|██████▊   | 27/40 [05:48<02:47, 12.91s/it] 70%|███████   | 28/40 [06:00<02:31, 12.61s/it] 72%|███████▎  | 29/40 [06:13<02:20, 12.74s/it] 75%|███████▌  | 30/40 [06:26<02:07, 12.76s/it] 78%|███████▊  | 31/40 [06:39<01:56, 12.90s/it] 80%|████████  | 32/40 [06:52<01:43, 12.95s/it] 82%|████████▎ | 33/40 [07:07<01:35, 13.62s/it] 85%|████████▌ | 34/40 [07:20<01:19, 13.30s/it] 88%|████████▊ | 35/40 [07:34<01:07, 13.43s/it] 90%|█████████ | 36/40 [07:46<00:52, 13.07s/it] 92%|█████████▎| 37/40 [07:58<00:38, 12.80s/it] 95%|█████████▌| 38/40 [08:11<00:25, 12.88s/it] 98%|█████████▊| 39/40 [08:24<00:12, 12.87s/it]100%|██████████| 40/40 [08:36<00:00, 12.53s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:36<00:00, 12.53s/it]100%|██████████| 40/40 [08:36<00:00, 12.90s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 516.0718, 'train_samples_per_second': 5.048, 'train_steps_per_second': 0.078, 'train_loss': 0.7157613277435303, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:16<10:59, 16.91s/it]  5%|▌         | 2/40 [00:36<11:44, 18.53s/it]  8%|▊         | 3/40 [00:55<11:32, 18.71s/it] 10%|█         | 4/40 [01:13<10:59, 18.31s/it] 12%|█▎        | 5/40 [01:32<10:59, 18.85s/it] 15%|█▌        | 6/40 [01:51<10:37, 18.75s/it] 18%|█▊        | 7/40 [02:10<10:20, 18.81s/it] 20%|██        | 8/40 [02:30<10:15, 19.24s/it] 22%|██▎       | 9/40 [02:52<10:25, 20.18s/it] 25%|██▌       | 10/40 [03:11<09:54, 19.81s/it] 28%|██▊       | 11/40 [03:30<09:22, 19.40s/it] 30%|███       | 12/40 [03:49<08:59, 19.28s/it] 32%|███▎      | 13/40 [04:08<08:40, 19.28s/it] 35%|███▌      | 14/40 [04:27<08:14, 19.01s/it] 38%|███▊      | 15/40 [04:45<07:52, 18.91s/it] 40%|████      | 16/40 [05:05<07:37, 19.06s/it] 42%|████▎     | 17/40 [05:25<07:29, 19.53s/it] 45%|████▌     | 18/40 [05:44<07:03, 19.24s/it] 48%|████▊     | 19/40 [06:04<06:48, 19.45s/it] 50%|█████     | 20/40 [06:23<06:27, 19.35s/it] 52%|█████▎    | 21/40 [06:42<06:06, 19.27s/it] 55%|█████▌    | 22/40 [07:00<05:40, 18.92s/it] 57%|█████▊    | 23/40 [07:19<05:20, 18.86s/it] 60%|██████    | 24/40 [07:38<05:02, 18.91s/it] 62%|██████▎   | 25/40 [07:59<04:51, 19.46s/it] 65%|██████▌   | 26/40 [08:17<04:26, 19.03s/it] 68%|██████▊   | 27/40 [08:36<04:08, 19.10s/it] 70%|███████   | 28/40 [08:53<03:42, 18.56s/it] 72%|███████▎  | 29/40 [09:13<03:27, 18.83s/it] 75%|███████▌  | 30/40 [09:32<03:08, 18.86s/it] 78%|███████▊  | 31/40 [09:51<02:51, 19.04s/it] 80%|████████  | 32/40 [10:10<02:33, 19.14s/it] 82%|████████▎ | 33/40 [10:33<02:21, 20.25s/it] 85%|████████▌ | 34/40 [10:52<01:59, 19.85s/it] 88%|████████▊ | 35/40 [11:13<01:40, 20.13s/it] 90%|█████████ | 36/40 [11:31<01:18, 19.59s/it] 92%|█████████▎| 37/40 [11:49<00:57, 19.03s/it] 95%|█████████▌| 38/40 [12:08<00:38, 19.07s/it] 98%|█████████▊| 39/40 [12:27<00:19, 19.01s/it]100%|██████████| 40/40 [12:44<00:00, 18.46s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:44<00:00, 18.46s/it]100%|██████████| 40/40 [12:44<00:00, 19.12s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 764.6688, 'train_samples_per_second': 3.407, 'train_steps_per_second': 0.052, 'train_loss': 0.6954577922821045, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:50, 18.21s/it]  5%|▌         | 2/40 [00:39<12:29, 19.74s/it]  8%|▊         | 3/40 [00:59<12:17, 19.93s/it] 10%|█         | 4/40 [01:17<11:38, 19.40s/it] 12%|█▎        | 5/40 [01:38<11:33, 19.82s/it] 15%|█▌        | 6/40 [01:58<11:15, 19.85s/it] 18%|█▊        | 7/40 [02:18<10:57, 19.92s/it] 20%|██        | 8/40 [02:39<10:45, 20.18s/it] 22%|██▎       | 9/40 [03:02<10:56, 21.17s/it] 25%|██▌       | 10/40 [03:22<10:23, 20.78s/it] 28%|██▊       | 11/40 [03:42<09:55, 20.53s/it] 30%|███       | 12/40 [04:02<09:31, 20.42s/it] 32%|███▎      | 13/40 [04:22<09:08, 20.30s/it] 35%|███▌      | 14/40 [04:41<08:39, 19.97s/it] 38%|███▊      | 15/40 [05:01<08:15, 19.84s/it] 40%|████      | 16/40 [05:21<08:00, 20.00s/it] 42%|████▎     | 17/40 [05:43<07:50, 20.44s/it] 45%|████▌     | 18/40 [06:02<07:23, 20.17s/it] 48%|████▊     | 19/40 [06:23<07:07, 20.35s/it] 50%|█████     | 20/40 [06:43<06:44, 20.21s/it] 52%|█████▎    | 21/40 [07:03<06:24, 20.25s/it] 55%|█████▌    | 22/40 [07:22<05:56, 19.83s/it] 57%|█████▊    | 23/40 [07:42<05:36, 19.82s/it] 60%|██████    | 24/40 [08:02<05:18, 19.91s/it] 62%|██████▎   | 25/40 [08:23<05:05, 20.36s/it] 65%|██████▌   | 26/40 [08:43<04:40, 20.04s/it] 68%|██████▊   | 27/40 [09:03<04:20, 20.07s/it] 70%|███████   | 28/40 [09:21<03:55, 19.63s/it] 72%|███████▎  | 29/40 [09:42<03:38, 19.86s/it] 75%|███████▌  | 30/40 [10:02<03:19, 19.93s/it] 78%|███████▊  | 31/40 [10:22<03:00, 20.05s/it] 80%|████████  | 32/40 [10:42<02:40, 20.10s/it] 82%|████████▎ | 33/40 [11:06<02:28, 21.21s/it] 85%|████████▌ | 34/40 [11:26<02:04, 20.80s/it] 88%|████████▊ | 35/40 [11:48<01:45, 21.06s/it] 90%|█████████ | 36/40 [12:07<01:21, 20.48s/it] 92%|█████████▎| 37/40 [12:26<00:59, 19.99s/it] 95%|█████████▌| 38/40 [12:45<00:39, 19.76s/it] 98%|█████████▊| 39/40 [13:05<00:19, 19.84s/it]100%|██████████| 40/40 [13:23<00:00, 19.41s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:23<00:00, 19.41s/it]100%|██████████| 40/40 [13:23<00:00, 20.09s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 803.7732, 'train_samples_per_second': 3.241, 'train_steps_per_second': 0.05, 'train_loss': 0.751339340209961, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:38, 11.76s/it]  5%|▌         | 2/40 [00:25<08:03, 12.72s/it]  8%|▊         | 3/40 [00:37<07:47, 12.64s/it] 10%|█         | 4/40 [00:50<07:32, 12.57s/it] 12%|█▎        | 5/40 [01:02<07:22, 12.66s/it] 15%|█▌        | 6/40 [01:15<07:12, 12.72s/it] 18%|█▊        | 7/40 [01:27<06:53, 12.54s/it] 20%|██        | 8/40 [01:40<06:44, 12.65s/it] 22%|██▎       | 9/40 [01:55<06:50, 13.24s/it] 25%|██▌       | 10/40 [02:07<06:23, 12.79s/it] 28%|██▊       | 11/40 [02:20<06:11, 12.83s/it] 30%|███       | 12/40 [02:32<05:56, 12.72s/it] 32%|███▎      | 13/40 [02:45<05:47, 12.86s/it] 35%|███▌      | 14/40 [02:58<05:32, 12.77s/it] 38%|███▊      | 15/40 [03:11<05:20, 12.83s/it] 40%|████      | 16/40 [03:24<05:09, 12.90s/it] 42%|████▎     | 17/40 [03:38<05:06, 13.31s/it] 45%|████▌     | 18/40 [03:52<04:54, 13.39s/it] 48%|████▊     | 19/40 [04:04<04:37, 13.21s/it] 50%|█████     | 20/40 [04:16<04:13, 12.68s/it] 52%|█████▎    | 21/40 [04:29<04:01, 12.69s/it] 55%|█████▌    | 22/40 [04:41<03:47, 12.65s/it] 57%|█████▊    | 23/40 [04:54<03:34, 12.63s/it] 60%|██████    | 24/40 [05:07<03:25, 12.85s/it] 62%|██████▎   | 25/40 [05:22<03:20, 13.39s/it] 65%|██████▌   | 26/40 [05:35<03:06, 13.31s/it] 68%|██████▊   | 27/40 [05:48<02:52, 13.26s/it] 70%|███████   | 28/40 [06:00<02:34, 12.84s/it] 72%|███████▎  | 29/40 [06:12<02:19, 12.73s/it] 75%|███████▌  | 30/40 [06:25<02:08, 12.81s/it] 78%|███████▊  | 31/40 [06:38<01:55, 12.86s/it] 80%|████████  | 32/40 [06:50<01:40, 12.58s/it] 82%|████████▎ | 33/40 [07:05<01:32, 13.25s/it] 85%|████████▌ | 34/40 [07:18<01:19, 13.18s/it] 88%|████████▊ | 35/40 [07:31<01:06, 13.22s/it] 90%|█████████ | 36/40 [07:44<00:51, 12.90s/it] 92%|█████████▎| 37/40 [07:56<00:38, 12.74s/it] 95%|█████████▌| 38/40 [08:08<00:25, 12.58s/it] 98%|█████████▊| 39/40 [08:21<00:12, 12.63s/it]100%|██████████| 40/40 [08:34<00:00, 12.71s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:34<00:00, 12.71s/it]100%|██████████| 40/40 [08:34<00:00, 12.86s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 514.3112, 'train_samples_per_second': 5.065, 'train_steps_per_second': 0.078, 'train_loss': 0.6991290092468262, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:28, 11.50s/it]  5%|▌         | 2/40 [00:24<08:01, 12.67s/it]  8%|▊         | 3/40 [00:37<07:48, 12.65s/it] 10%|█         | 4/40 [00:50<07:34, 12.62s/it] 12%|█▎        | 5/40 [01:03<07:26, 12.76s/it] 15%|█▌        | 6/40 [01:16<07:16, 12.85s/it] 18%|█▊        | 7/40 [01:28<06:57, 12.65s/it] 20%|██        | 8/40 [01:41<06:47, 12.73s/it] 22%|██▎       | 9/40 [01:55<06:52, 13.31s/it] 25%|██▌       | 10/40 [02:07<06:24, 12.83s/it] 28%|██▊       | 11/40 [02:20<06:12, 12.86s/it] 30%|███       | 12/40 [02:33<05:57, 12.75s/it] 32%|███▎      | 13/40 [02:46<05:46, 12.83s/it] 35%|███▌      | 14/40 [02:58<05:32, 12.79s/it] 38%|███▊      | 15/40 [03:11<05:21, 12.86s/it] 40%|████      | 16/40 [03:24<05:09, 12.89s/it] 42%|████▎     | 17/40 [03:39<05:06, 13.33s/it] 45%|████▌     | 18/40 [03:52<04:55, 13.45s/it] 48%|████▊     | 19/40 [04:05<04:38, 13.24s/it] 50%|█████     | 20/40 [04:17<04:14, 12.72s/it] 52%|█████▎    | 21/40 [04:29<04:01, 12.70s/it] 55%|█████▌    | 22/40 [04:42<03:47, 12.65s/it] 57%|█████▊    | 23/40 [04:54<03:34, 12.64s/it] 60%|██████    | 24/40 [05:08<03:25, 12.85s/it] 62%|██████▎   | 25/40 [05:22<03:20, 13.40s/it] 65%|██████▌   | 26/40 [05:36<03:06, 13.29s/it] 68%|██████▊   | 27/40 [05:49<02:52, 13.25s/it] 70%|███████   | 28/40 [06:01<02:33, 12.83s/it] 72%|███████▎  | 29/40 [06:13<02:19, 12.68s/it] 75%|███████▌  | 30/40 [06:26<02:07, 12.72s/it] 78%|███████▊  | 31/40 [06:38<01:54, 12.74s/it] 80%|████████  | 32/40 [06:50<01:39, 12.48s/it] 82%|████████▎ | 33/40 [07:05<01:32, 13.18s/it] 85%|████████▌ | 34/40 [07:18<01:18, 13.15s/it] 88%|████████▊ | 35/40 [07:32<01:06, 13.20s/it] 90%|█████████ | 36/40 [07:44<00:51, 12.94s/it] 92%|█████████▎| 37/40 [07:56<00:38, 12.69s/it] 95%|█████████▌| 38/40 [08:08<00:25, 12.55s/it] 98%|█████████▊| 39/40 [08:21<00:12, 12.60s/it]100%|██████████| 40/40 [08:34<00:00, 12.68s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:34<00:00, 12.68s/it]100%|██████████| 40/40 [08:34<00:00, 12.86s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 514.2752, 'train_samples_per_second': 5.065, 'train_steps_per_second': 0.078, 'train_loss': 0.7157057762145996, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:06, 17.10s/it]  5%|▌         | 2/40 [00:37<12:00, 18.97s/it]  8%|▊         | 3/40 [00:55<11:32, 18.72s/it] 10%|█         | 4/40 [01:14<11:09, 18.59s/it] 12%|█▎        | 5/40 [01:33<10:58, 18.82s/it] 15%|█▌        | 6/40 [01:52<10:43, 18.93s/it] 18%|█▊        | 7/40 [02:10<10:19, 18.77s/it] 20%|██        | 8/40 [02:30<10:04, 18.90s/it] 22%|██▎       | 9/40 [02:51<10:13, 19.79s/it] 25%|██▌       | 10/40 [03:09<09:31, 19.06s/it] 28%|██▊       | 11/40 [03:28<09:12, 19.05s/it] 30%|███       | 12/40 [03:46<08:49, 18.91s/it] 32%|███▎      | 13/40 [04:06<08:35, 19.09s/it] 35%|███▌      | 14/40 [04:25<08:11, 18.92s/it] 38%|███▊      | 15/40 [04:44<07:55, 19.03s/it] 40%|████      | 16/40 [05:04<07:43, 19.29s/it] 42%|████▎     | 17/40 [05:25<07:37, 19.91s/it] 45%|████▌     | 18/40 [05:45<07:21, 20.07s/it] 48%|████▊     | 19/40 [06:05<06:55, 19.77s/it] 50%|█████     | 20/40 [06:21<06:16, 18.80s/it] 52%|█████▎    | 21/40 [06:40<05:57, 18.83s/it] 55%|█████▌    | 22/40 [06:58<05:36, 18.71s/it] 57%|█████▊    | 23/40 [07:17<05:18, 18.71s/it] 60%|██████    | 24/40 [07:37<05:06, 19.13s/it] 62%|██████▎   | 25/40 [07:59<05:00, 20.04s/it] 65%|██████▌   | 26/40 [08:19<04:39, 19.94s/it] 68%|██████▊   | 27/40 [08:39<04:17, 19.84s/it] 70%|███████   | 28/40 [08:56<03:49, 19.15s/it] 72%|███████▎  | 29/40 [09:14<03:27, 18.85s/it] 75%|███████▌  | 30/40 [09:33<03:08, 18.86s/it] 78%|███████▊  | 31/40 [09:53<02:50, 18.97s/it] 80%|████████  | 32/40 [10:10<02:28, 18.62s/it] 82%|████████▎ | 33/40 [10:32<02:17, 19.60s/it] 85%|████████▌ | 34/40 [10:52<01:57, 19.52s/it] 88%|████████▊ | 35/40 [11:11<01:37, 19.41s/it] 90%|█████████ | 36/40 [11:29<01:16, 19.05s/it] 92%|█████████▎| 37/40 [11:47<00:56, 18.79s/it] 95%|█████████▌| 38/40 [12:05<00:36, 18.49s/it] 98%|█████████▊| 39/40 [12:24<00:18, 18.68s/it]100%|██████████| 40/40 [12:43<00:00, 18.79s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:43<00:00, 18.79s/it]100%|██████████| 40/40 [12:43<00:00, 19.09s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 763.5275, 'train_samples_per_second': 3.412, 'train_steps_per_second': 0.052, 'train_loss': 0.6952097892761231, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<11:46, 18.10s/it]  5%|▌         | 2/40 [00:39<12:37, 19.92s/it]  8%|▊         | 3/40 [00:58<12:11, 19.76s/it] 10%|█         | 4/40 [01:18<11:52, 19.79s/it] 12%|█▎        | 5/40 [01:39<11:39, 19.99s/it] 15%|█▌        | 6/40 [01:59<11:20, 20.02s/it] 18%|█▊        | 7/40 [02:18<10:51, 19.75s/it] 20%|██        | 8/40 [02:38<10:35, 19.85s/it] 22%|██▎       | 9/40 [03:01<10:44, 20.80s/it] 25%|██▌       | 10/40 [03:19<10:01, 20.04s/it] 28%|██▊       | 11/40 [03:39<09:42, 20.07s/it] 30%|███       | 12/40 [03:59<09:18, 19.95s/it] 32%|███▎      | 13/40 [04:19<09:02, 20.11s/it] 35%|███▌      | 14/40 [04:39<08:41, 20.04s/it] 38%|███▊      | 15/40 [05:00<08:24, 20.18s/it] 40%|████      | 16/40 [05:21<08:08, 20.37s/it] 42%|████▎     | 17/40 [05:43<08:01, 20.95s/it] 45%|████▌     | 18/40 [06:04<07:43, 21.08s/it] 48%|████▊     | 19/40 [06:24<07:16, 20.80s/it] 50%|█████     | 20/40 [06:42<06:36, 19.83s/it] 52%|█████▎    | 21/40 [07:02<06:17, 19.86s/it] 55%|█████▌    | 22/40 [07:21<05:54, 19.72s/it] 57%|█████▊    | 23/40 [07:41<05:35, 19.74s/it] 60%|██████    | 24/40 [08:02<05:21, 20.09s/it] 62%|██████▎   | 25/40 [08:25<05:14, 20.99s/it] 65%|██████▌   | 26/40 [08:46<04:53, 20.94s/it] 68%|██████▊   | 27/40 [09:07<04:30, 20.84s/it] 70%|███████   | 28/40 [09:25<04:02, 20.18s/it] 72%|███████▎  | 29/40 [09:45<03:39, 19.93s/it] 75%|███████▌  | 30/40 [10:04<03:19, 19.91s/it] 78%|███████▊  | 31/40 [10:25<03:00, 20.02s/it] 80%|████████  | 32/40 [10:43<02:37, 19.63s/it] 82%|████████▎ | 33/40 [11:07<02:25, 20.83s/it] 85%|████████▌ | 34/40 [11:27<02:04, 20.71s/it] 88%|████████▊ | 35/40 [11:48<01:42, 20.59s/it] 90%|█████████ | 36/40 [12:07<01:20, 20.14s/it] 92%|█████████▎| 37/40 [12:26<00:59, 19.86s/it] 95%|█████████▌| 38/40 [12:45<00:39, 19.54s/it] 98%|█████████▊| 39/40 [13:05<00:19, 19.69s/it]100%|██████████| 40/40 [13:25<00:00, 19.79s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:25<00:00, 19.79s/it]100%|██████████| 40/40 [13:25<00:00, 20.14s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'train_runtime': 805.4147, 'train_samples_per_second': 3.234, 'train_steps_per_second': 0.05, 'train_loss': 0.7511528015136719, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:09<45:49,  9.68s/it]  1%|          | 2/285 [00:18<42:39,  9.05s/it]  1%|          | 3/285 [00:28<44:02,  9.37s/it]  1%|▏         | 4/285 [00:37<44:46,  9.56s/it]  2%|▏         | 5/285 [00:48<45:51,  9.83s/it]  2%|▏         | 6/285 [00:58<45:59,  9.89s/it]  2%|▏         | 7/285 [01:07<44:48,  9.67s/it]  3%|▎         | 8/285 [01:17<45:14,  9.80s/it]  3%|▎         | 9/285 [01:26<44:12,  9.61s/it]  4%|▎         | 10/285 [01:37<45:12,  9.86s/it]  4%|▍         | 11/285 [01:46<44:16,  9.70s/it]  4%|▍         | 12/285 [01:56<44:20,  9.75s/it]  5%|▍         | 13/285 [02:06<44:25,  9.80s/it]  5%|▍         | 14/285 [02:17<46:15, 10.24s/it]  5%|▌         | 15/285 [02:27<46:10, 10.26s/it]  6%|▌         | 16/285 [02:37<45:13, 10.09s/it]  6%|▌         | 17/285 [02:48<46:53, 10.50s/it]  6%|▋         | 18/285 [03:00<48:01, 10.79s/it]  7%|▋         | 19/285 [03:11<48:31, 10.95s/it]  7%|▋         | 20/285 [03:23<49:00, 11.10s/it]  7%|▋         | 21/285 [03:35<50:21, 11.45s/it]  8%|▊         | 22/285 [03:45<48:50, 11.14s/it]  8%|▊         | 23/285 [03:57<49:19, 11.29s/it]  8%|▊         | 24/285 [04:08<48:53, 11.24s/it]  9%|▉         | 25/285 [04:20<49:12, 11.36s/it]  9%|▉         | 26/285 [04:30<48:14, 11.17s/it]  9%|▉         | 27/285 [04:41<47:39, 11.08s/it] 10%|▉         | 28/285 [04:53<48:21, 11.29s/it] 10%|█         | 29/285 [05:04<47:48, 11.21s/it] 11%|█         | 30/285 [05:16<48:06, 11.32s/it] 11%|█         | 31/285 [05:27<48:22, 11.43s/it] 11%|█         | 32/285 [05:39<47:52, 11.35s/it] 12%|█▏        | 33/285 [05:50<47:47, 11.38s/it] 12%|█▏        | 34/285 [06:02<48:19, 11.55s/it] 12%|█▏        | 35/285 [06:13<47:17, 11.35s/it] 13%|█▎        | 36/285 [06:24<47:08, 11.36s/it] 13%|█▎        | 37/285 [06:36<47:23, 11.46s/it] 13%|█▎        | 38/285 [06:47<47:15, 11.48s/it] 14%|█▎        | 39/285 [06:59<47:22, 11.56s/it] 14%|█▍        | 40/285 [07:11<47:02, 11.52s/it] 14%|█▍        | 41/285 [07:22<47:00, 11.56s/it] 15%|█▍        | 42/285 [07:34<46:38, 11.52s/it] 15%|█▌        | 43/285 [07:45<45:59, 11.40s/it] 15%|█▌        | 44/285 [07:56<45:56, 11.44s/it] 16%|█▌        | 45/285 [08:08<45:52, 11.47s/it] 16%|█▌        | 46/285 [08:19<44:59, 11.29s/it] 16%|█▋        | 47/285 [08:30<44:14, 11.15s/it] 17%|█▋        | 48/285 [08:41<43:59, 11.14s/it] 17%|█▋        | 49/285 [08:53<44:48, 11.39s/it] 18%|█▊        | 50/285 [09:05<45:33, 11.63s/it] 18%|█▊        | 51/285 [09:17<46:10, 11.84s/it] 18%|█▊        | 52/285 [09:29<45:28, 11.71s/it] 19%|█▊        | 53/285 [09:40<44:49, 11.59s/it] 19%|█▉        | 54/285 [09:52<45:15, 11.76s/it] 19%|█▉        | 55/285 [10:03<44:30, 11.61s/it] 20%|█▉        | 56/285 [10:16<45:20, 11.88s/it] 20%|██        | 57/285 [10:28<45:12, 11.90s/it] 20%|██        | 58/285 [10:40<45:02, 11.90s/it] 21%|██        | 59/285 [10:49<41:52, 11.12s/it] 21%|██        | 60/285 [10:58<39:48, 10.62s/it] 21%|██▏       | 61/285 [11:07<37:34, 10.07s/it] 22%|██▏       | 62/285 [11:16<36:06,  9.72s/it] 22%|██▏       | 63/285 [11:27<36:43,  9.93s/it] 22%|██▏       | 64/285 [11:36<36:12,  9.83s/it] 23%|██▎       | 65/285 [11:46<36:09,  9.86s/it] 23%|██▎       | 66/285 [11:57<36:37, 10.04s/it] 24%|██▎       | 67/285 [12:05<34:37,  9.53s/it] 24%|██▍       | 68/285 [12:15<34:49,  9.63s/it] 24%|██▍       | 69/285 [12:26<36:01, 10.01s/it] 25%|██▍       | 70/285 [12:36<35:51, 10.01s/it] 25%|██▍       | 71/285 [12:46<36:08, 10.13s/it] 25%|██▌       | 72/285 [12:58<37:27, 10.55s/it] 26%|██▌       | 73/285 [13:09<38:04, 10.78s/it] 26%|██▌       | 74/285 [13:19<37:38, 10.71s/it] 26%|██▋       | 75/285 [13:31<38:15, 10.93s/it] 27%|██▋       | 76/285 [13:42<38:37, 11.09s/it] 27%|██▋       | 77/285 [13:55<40:14, 11.61s/it] 27%|██▋       | 78/285 [14:06<39:25, 11.43s/it] 28%|██▊       | 79/285 [14:18<39:27, 11.49s/it] 28%|██▊       | 80/285 [14:28<38:20, 11.22s/it] 28%|██▊       | 81/285 [14:39<37:31, 11.04s/it] 29%|██▉       | 82/285 [14:51<38:15, 11.31s/it] 29%|██▉       | 83/285 [15:02<38:05, 11.32s/it] 29%|██▉       | 84/285 [15:13<37:46, 11.27s/it] 30%|██▉       | 85/285 [15:26<38:57, 11.69s/it] 30%|███       | 86/285 [15:37<38:24, 11.58s/it] 31%|███       | 87/285 [15:48<37:35, 11.39s/it] 31%|███       | 88/285 [16:00<37:06, 11.30s/it] 31%|███       | 89/285 [16:11<37:20, 11.43s/it] 32%|███▏      | 90/285 [16:23<37:13, 11.46s/it] 32%|███▏      | 91/285 [16:34<36:38, 11.33s/it] 32%|███▏      | 92/285 [16:46<37:13, 11.57s/it] 33%|███▎      | 93/285 [16:58<37:07, 11.60s/it] 33%|███▎      | 94/285 [17:10<37:43, 11.85s/it] 33%|███▎      | 95/285 [17:22<37:16, 11.77s/it] 34%|███▎      | 96/285 [17:33<36:44, 11.66s/it] 34%|███▍      | 97/285 [17:44<36:05, 11.52s/it] 34%|███▍      | 98/285 [17:56<35:57, 11.54s/it] 35%|███▍      | 99/285 [18:07<35:01, 11.30s/it] 35%|███▌      | 100/285 [18:18<35:14, 11.43s/it] 35%|███▌      | 101/285 [18:30<35:20, 11.52s/it] 36%|███▌      | 102/285 [18:41<35:00, 11.48s/it] 36%|███▌      | 103/285 [18:52<34:26, 11.36s/it] 36%|███▋      | 104/285 [19:04<34:14, 11.35s/it] 37%|███▋      | 105/285 [19:15<33:54, 11.30s/it] 37%|███▋      | 106/285 [19:27<34:11, 11.46s/it] 38%|███▊      | 107/285 [19:39<34:21, 11.58s/it] 38%|███▊      | 108/285 [19:50<34:15, 11.61s/it] 38%|███▊      | 109/285 [20:03<35:10, 11.99s/it] 39%|███▊      | 110/285 [20:15<35:12, 12.07s/it] 39%|███▉      | 111/285 [20:27<34:47, 12.00s/it] 39%|███▉      | 112/285 [20:40<34:53, 12.10s/it] 40%|███▉      | 113/285 [20:52<35:00, 12.21s/it] 40%|████      | 114/285 [21:03<33:42, 11.83s/it] 40%|████      | 115/285 [21:16<34:39, 12.23s/it] 41%|████      | 116/285 [21:28<34:18, 12.18s/it] 41%|████      | 117/285 [21:40<33:26, 11.94s/it] 41%|████▏     | 118/285 [21:51<32:39, 11.74s/it] 42%|████▏     | 119/285 [22:02<31:39, 11.44s/it] 42%|████▏     | 120/285 [22:13<31:26, 11.43s/it] 42%|████▏     | 121/285 [22:25<31:37, 11.57s/it] 43%|████▎     | 122/285 [22:37<31:28, 11.59s/it] 43%|████▎     | 123/285 [22:49<31:48, 11.78s/it] 44%|████▎     | 124/285 [23:00<31:20, 11.68s/it] 44%|████▍     | 125/285 [23:12<31:11, 11.70s/it] 44%|████▍     | 126/285 [23:23<30:38, 11.57s/it] 45%|████▍     | 127/285 [23:35<30:13, 11.48s/it] 45%|████▍     | 128/285 [23:45<29:18, 11.20s/it] 45%|████▌     | 129/285 [23:57<29:19, 11.28s/it] 46%|████▌     | 130/285 [24:08<29:32, 11.44s/it] 46%|████▌     | 131/285 [24:20<29:28, 11.48s/it] 46%|████▋     | 132/285 [24:31<29:16, 11.48s/it] 47%|████▋     | 133/285 [24:43<29:11, 11.52s/it] 47%|████▋     | 134/285 [24:55<29:28, 11.71s/it] 47%|████▋     | 135/285 [25:07<29:13, 11.69s/it] 48%|████▊     | 136/285 [25:17<28:11, 11.36s/it] 48%|████▊     | 137/285 [25:29<27:51, 11.29s/it] 48%|████▊     | 138/285 [25:40<27:25, 11.19s/it] 49%|████▉     | 139/285 [25:51<27:10, 11.17s/it] 49%|████▉     | 140/285 [26:02<27:13, 11.27s/it] 49%|████▉     | 141/285 [26:14<27:29, 11.46s/it] 50%|████▉     | 142/285 [26:25<26:57, 11.31s/it] 50%|█████     | 143/285 [26:36<26:26, 11.17s/it] 51%|█████     | 144/285 [26:47<26:10, 11.14s/it] 51%|█████     | 145/285 [26:59<26:48, 11.49s/it] 51%|█████     | 146/285 [27:10<26:18, 11.36s/it] 52%|█████▏    | 147/285 [27:21<26:00, 11.31s/it] 52%|█████▏    | 148/285 [27:33<26:07, 11.44s/it] 52%|█████▏    | 149/285 [27:45<25:54, 11.43s/it] 53%|█████▎    | 150/285 [27:56<25:34, 11.37s/it] 53%|█████▎    | 151/285 [28:08<25:34, 11.45s/it] 53%|█████▎    | 152/285 [28:19<25:26, 11.48s/it] 54%|█████▎    | 153/285 [28:31<25:49, 11.74s/it] 54%|█████▍    | 154/285 [28:43<25:29, 11.68s/it] 54%|█████▍    | 155/285 [28:54<25:00, 11.54s/it] 55%|█████▍    | 156/285 [29:06<25:09, 11.70s/it] 55%|█████▌    | 157/285 [29:17<24:39, 11.56s/it] 55%|█████▌    | 158/285 [29:29<24:26, 11.54s/it] 56%|█████▌    | 159/285 [29:41<24:20, 11.59s/it] 56%|█████▌    | 160/285 [29:52<23:51, 11.45s/it] 56%|█████▋    | 161/285 [30:04<24:08, 11.69s/it] 57%|█████▋    | 162/285 [30:16<23:58, 11.69s/it] 57%|█████▋    | 163/285 [30:28<24:06, 11.86s/it] 58%|█████▊    | 164/285 [30:40<24:01, 11.91s/it] 58%|█████▊    | 165/285 [30:53<24:16, 12.14s/it] 58%|█████▊    | 166/285 [31:04<23:19, 11.76s/it] 59%|█████▊    | 167/285 [31:15<22:46, 11.58s/it] 59%|█████▉    | 168/285 [31:26<22:23, 11.49s/it] 59%|█████▉    | 169/285 [31:37<22:12, 11.49s/it] 60%|█████▉    | 170/285 [31:49<21:55, 11.44s/it] 60%|██████    | 171/285 [32:00<21:45, 11.45s/it] 60%|██████    | 172/285 [32:12<21:37, 11.48s/it] 61%|██████    | 173/285 [32:23<21:15, 11.39s/it] 61%|██████    | 174/285 [32:34<20:57, 11.33s/it] 61%|██████▏   | 175/285 [32:45<20:41, 11.29s/it] 62%|██████▏   | 176/285 [32:57<20:51, 11.48s/it] 62%|██████▏   | 177/285 [33:08<20:09, 11.20s/it] 62%|██████▏   | 178/285 [33:18<19:18, 10.82s/it] 63%|██████▎   | 179/285 [33:28<18:37, 10.55s/it] 63%|██████▎   | 180/285 [33:37<17:56, 10.25s/it] 64%|██████▎   | 181/285 [33:48<17:50, 10.29s/it] 64%|██████▍   | 182/285 [33:58<17:38, 10.28s/it] 64%|██████▍   | 183/285 [34:07<17:05, 10.06s/it] 65%|██████▍   | 184/285 [34:18<17:08, 10.19s/it] 65%|██████▍   | 185/285 [34:27<16:39,  9.99s/it] 65%|██████▌   | 186/285 [34:38<16:44, 10.15s/it] 66%|██████▌   | 187/285 [34:47<16:04,  9.84s/it] 66%|██████▌   | 188/285 [34:57<16:01,  9.91s/it] 66%|██████▋   | 189/285 [35:07<15:38,  9.78s/it] 67%|██████▋   | 190/285 [35:17<15:54, 10.05s/it] 67%|██████▋   | 191/285 [35:29<16:38, 10.63s/it] 67%|██████▋   | 192/285 [35:42<17:24, 11.24s/it] 68%|██████▊   | 193/285 [35:54<17:44, 11.57s/it] 68%|██████▊   | 194/285 [36:06<17:44, 11.70s/it] 68%|██████▊   | 195/285 [36:19<17:53, 11.92s/it] 69%|██████▉   | 196/285 [36:30<17:18, 11.66s/it] 69%|██████▉   | 197/285 [36:41<16:40, 11.37s/it] 69%|██████▉   | 198/285 [36:52<16:21, 11.28s/it] 70%|██████▉   | 199/285 [37:03<16:07, 11.25s/it] 70%|███████   | 200/285 [37:14<15:54, 11.23s/it]                                                  70%|███████   | 200/285 [37:14<15:54, 11.23s/it] 71%|███████   | 201/285 [37:25<15:45, 11.26s/it] 71%|███████   | 202/285 [37:37<15:41, 11.35s/it] 71%|███████   | 203/285 [37:48<15:19, 11.22s/it] 72%|███████▏  | 204/285 [38:00<15:37, 11.58s/it] 72%|███████▏  | 205/285 [38:11<15:18, 11.48s/it] 72%|███████▏  | 206/285 [38:23<15:11, 11.54s/it] 73%|███████▎  | 207/285 [38:36<15:33, 11.97s/it] 73%|███████▎  | 208/285 [38:48<15:18, 11.93s/it] 73%|███████▎  | 209/285 [39:00<15:21, 12.12s/it] 74%|███████▎  | 210/285 [39:12<14:49, 11.85s/it] 74%|███████▍  | 211/285 [39:23<14:23, 11.67s/it] 74%|███████▍  | 212/285 [39:35<14:13, 11.69s/it] 75%|███████▍  | 213/285 [39:46<13:50, 11.53s/it] 75%|███████▌  | 214/285 [39:57<13:24, 11.33s/it] 75%|███████▌  | 215/285 [40:08<13:09, 11.28s/it] 76%|███████▌  | 216/285 [40:20<13:18, 11.57s/it] 76%|███████▌  | 217/285 [40:31<12:46, 11.27s/it] 76%|███████▋  | 218/285 [40:42<12:36, 11.29s/it] 77%|███████▋  | 219/285 [40:53<12:23, 11.26s/it] 77%|███████▋  | 220/285 [41:05<12:19, 11.37s/it] 78%|███████▊  | 221/285 [41:16<12:06, 11.36s/it] 78%|███████▊  | 222/285 [41:27<11:51, 11.30s/it] 78%|███████▊  | 223/285 [41:39<11:44, 11.36s/it] 79%|███████▊  | 224/285 [41:50<11:36, 11.42s/it] 79%|███████▉  | 225/285 [42:02<11:24, 11.41s/it] 79%|███████▉  | 226/285 [42:13<11:18, 11.50s/it] 80%|███████▉  | 227/285 [42:25<11:06, 11.49s/it] 80%|████████  | 228/285 [42:37<11:01, 11.61s/it] 80%|████████  | 229/285 [42:50<11:11, 11.98s/it] 81%|████████  | 230/285 [43:01<10:51, 11.84s/it] 81%|████████  | 231/285 [43:12<10:24, 11.56s/it] 81%|████████▏ | 232/285 [43:24<10:16, 11.62s/it] 82%|████████▏ | 233/285 [43:35<09:57, 11.49s/it] 82%|████████▏ | 234/285 [43:46<09:44, 11.46s/it] 82%|████████▏ | 235/285 [43:58<09:30, 11.42s/it] 83%|████████▎ | 236/285 [44:08<09:05, 11.14s/it] 83%|████████▎ | 237/285 [44:19<08:55, 11.15s/it] 84%|████████▎ | 238/285 [44:30<08:42, 11.12s/it] 84%|████████▍ | 239/285 [44:42<08:44, 11.40s/it] 84%|████████▍ | 240/285 [44:55<08:45, 11.69s/it] 85%|████████▍ | 241/285 [45:06<08:28, 11.55s/it] 85%|████████▍ | 242/285 [45:18<08:16, 11.56s/it] 85%|████████▌ | 243/285 [45:29<08:08, 11.64s/it] 86%|████████▌ | 244/285 [45:41<07:52, 11.52s/it] 86%|████████▌ | 245/285 [45:52<07:40, 11.51s/it] 86%|████████▋ | 246/285 [46:03<07:21, 11.32s/it] 87%|████████▋ | 247/285 [46:16<07:25, 11.73s/it] 87%|████████▋ | 248/285 [46:27<07:07, 11.57s/it] 87%|████████▋ | 249/285 [46:38<06:48, 11.36s/it] 88%|████████▊ | 250/285 [46:50<06:49, 11.69s/it] 88%|████████▊ | 251/285 [47:02<06:33, 11.58s/it] 88%|████████▊ | 252/285 [47:13<06:24, 11.65s/it] 89%|████████▉ | 253/285 [47:25<06:09, 11.55s/it] 89%|████████▉ | 254/285 [47:36<05:58, 11.56s/it] 89%|████████▉ | 255/285 [47:48<05:49, 11.65s/it] 90%|████████▉ | 256/285 [47:59<05:32, 11.48s/it] 90%|█████████ | 257/285 [48:12<05:31, 11.83s/it] 91%|█████████ | 258/285 [48:24<05:17, 11.76s/it] 91%|█████████ | 259/285 [48:35<05:02, 11.64s/it] 91%|█████████ | 260/285 [48:46<04:43, 11.33s/it] 92%|█████████▏| 261/285 [48:56<04:25, 11.04s/it] 92%|█████████▏| 262/285 [49:08<04:19, 11.30s/it] 92%|█████████▏| 263/285 [49:20<04:11, 11.45s/it] 93%|█████████▎| 264/285 [49:31<04:00, 11.45s/it] 93%|█████████▎| 265/285 [49:42<03:48, 11.40s/it] 93%|█████████▎| 266/285 [49:53<03:33, 11.22s/it] 94%|█████████▎| 267/285 [50:05<03:24, 11.37s/it] 94%|█████████▍| 268/285 [50:16<03:14, 11.42s/it] 94%|█████████▍| 269/285 [50:28<03:02, 11.42s/it] 95%|█████████▍| 270/285 [50:40<02:54, 11.61s/it] 95%|█████████▌| 271/285 [50:51<02:41, 11.52s/it] 95%|█████████▌| 272/285 [51:02<02:27, 11.37s/it] 96%|█████████▌| 273/285 [51:13<02:16, 11.35s/it] 96%|█████████▌| 274/285 [51:26<02:07, 11.60s/it] 96%|█████████▋| 275/285 [51:38<01:58, 11.81s/it] 97%|█████████▋| 276/285 [51:49<01:45, 11.70s/it] 97%|█████████▋| 277/285 [52:00<01:31, 11.39s/it] 98%|█████████▊| 278/285 [52:11<01:19, 11.36s/it] 98%|█████████▊| 279/285 [52:23<01:08, 11.37s/it] 98%|█████████▊| 280/285 [52:35<00:58, 11.64s/it] 99%|█████████▊| 281/285 [52:46<00:46, 11.58s/it] 99%|█████████▉| 282/285 [52:58<00:34, 11.57s/it] 99%|█████████▉| 283/285 [53:10<00:23, 11.77s/it]100%|█████████▉| 284/285 [53:21<00:11, 11.48s/it]100%|██████████| 285/285 [53:33<00:00, 11.60s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [53:33<00:00, 11.60s/it]100%|██████████| 285/285 [53:33<00:00, 11.28s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'loss': 0.361, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 3213.4349, 'train_samples_per_second': 5.686, 'train_steps_per_second': 0.089, 'train_loss': 0.284305609318248, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:10<50:47, 10.73s/it]  1%|          | 2/285 [00:21<50:17, 10.66s/it]  1%|          | 3/285 [00:32<52:11, 11.11s/it]  1%|▏         | 4/285 [00:44<52:47, 11.27s/it]  2%|▏         | 5/285 [00:56<53:46, 11.52s/it]  2%|▏         | 6/285 [01:08<54:02, 11.62s/it]  2%|▏         | 7/285 [01:19<53:09, 11.47s/it]  3%|▎         | 8/285 [01:30<52:36, 11.40s/it]  3%|▎         | 9/285 [01:41<51:26, 11.18s/it]  4%|▎         | 10/285 [01:53<52:08, 11.38s/it]  4%|▍         | 11/285 [02:04<51:25, 11.26s/it]  4%|▍         | 12/285 [02:15<51:20, 11.28s/it]  5%|▍         | 13/285 [02:27<51:44, 11.41s/it]  5%|▍         | 14/285 [02:39<53:16, 11.80s/it]  5%|▌         | 15/285 [02:51<52:58, 11.77s/it]  6%|▌         | 16/285 [03:02<52:12, 11.64s/it]  6%|▌         | 17/285 [03:14<51:43, 11.58s/it]  6%|▋         | 18/285 [03:25<51:22, 11.54s/it]  7%|▋         | 19/285 [03:37<50:55, 11.49s/it]  7%|▋         | 20/285 [03:48<50:48, 11.50s/it]  7%|▋         | 21/285 [04:00<51:21, 11.67s/it]  8%|▊         | 22/285 [04:11<50:21, 11.49s/it]  8%|▊         | 23/285 [04:23<50:17, 11.52s/it]  8%|▊         | 24/285 [04:35<50:11, 11.54s/it]  9%|▉         | 25/285 [04:46<50:04, 11.56s/it]  9%|▉         | 26/285 [04:57<49:02, 11.36s/it]  9%|▉         | 27/285 [05:08<48:03, 11.18s/it] 10%|▉         | 28/285 [05:20<48:35, 11.34s/it] 10%|█         | 29/285 [05:31<47:59, 11.25s/it] 11%|█         | 30/285 [05:42<48:13, 11.35s/it] 11%|█         | 31/285 [05:54<48:24, 11.43s/it] 11%|█         | 32/285 [06:05<47:53, 11.36s/it] 12%|█▏        | 33/285 [06:16<47:49, 11.39s/it] 12%|█▏        | 34/285 [06:28<48:17, 11.54s/it] 12%|█▏        | 35/285 [06:39<47:10, 11.32s/it] 13%|█▎        | 36/285 [06:51<47:01, 11.33s/it] 13%|█▎        | 37/285 [07:02<47:13, 11.43s/it] 13%|█▎        | 38/285 [07:14<47:08, 11.45s/it] 14%|█▎        | 39/285 [07:25<47:09, 11.50s/it] 14%|█▍        | 40/285 [07:37<46:51, 11.48s/it] 14%|█▍        | 41/285 [07:48<46:41, 11.48s/it] 15%|█▍        | 42/285 [07:59<46:04, 11.38s/it] 15%|█▌        | 43/285 [08:10<45:28, 11.28s/it] 15%|█▌        | 44/285 [08:22<45:25, 11.31s/it] 16%|█▌        | 45/285 [08:33<45:19, 11.33s/it] 16%|█▌        | 46/285 [08:44<44:36, 11.20s/it] 16%|█▋        | 47/285 [08:55<44:07, 11.13s/it] 17%|█▋        | 48/285 [09:07<44:26, 11.25s/it] 17%|█▋        | 49/285 [09:18<45:02, 11.45s/it] 18%|█▊        | 50/285 [09:31<45:39, 11.66s/it] 18%|█▊        | 51/285 [09:43<46:12, 11.85s/it] 18%|█▊        | 52/285 [09:54<45:29, 11.71s/it] 19%|█▊        | 53/285 [10:06<44:53, 11.61s/it] 19%|█▉        | 54/285 [10:18<45:02, 11.70s/it] 19%|█▉        | 55/285 [10:29<44:18, 11.56s/it] 20%|█▉        | 56/285 [10:41<45:12, 11.85s/it] 20%|██        | 57/285 [10:53<44:53, 11.81s/it] 20%|██        | 58/285 [11:07<46:37, 12.32s/it] 21%|██        | 59/285 [11:18<44:59, 11.94s/it] 21%|██        | 60/285 [11:29<43:45, 11.67s/it] 21%|██▏       | 61/285 [11:39<42:22, 11.35s/it] 22%|██▏       | 62/285 [11:50<41:25, 11.15s/it] 22%|██▏       | 63/285 [12:02<41:54, 11.33s/it] 22%|██▏       | 64/285 [12:13<41:40, 11.32s/it] 23%|██▎       | 65/285 [12:25<41:44, 11.39s/it] 23%|██▎       | 66/285 [12:37<42:23, 11.61s/it] 24%|██▎       | 67/285 [12:47<40:54, 11.26s/it] 24%|██▍       | 68/285 [12:59<41:07, 11.37s/it] 24%|██▍       | 69/285 [13:11<41:51, 11.63s/it] 25%|██▍       | 70/285 [13:23<41:40, 11.63s/it] 25%|██▍       | 71/285 [13:34<41:33, 11.65s/it] 25%|██▌       | 72/285 [13:46<41:20, 11.65s/it] 26%|██▌       | 73/285 [13:57<40:36, 11.49s/it] 26%|██▌       | 74/285 [14:08<39:25, 11.21s/it] 26%|██▋       | 75/285 [14:19<39:26, 11.27s/it] 27%|██▋       | 76/285 [14:30<39:23, 11.31s/it] 27%|██▋       | 77/285 [14:43<40:44, 11.75s/it] 27%|██▋       | 78/285 [14:54<39:44, 11.52s/it] 28%|██▊       | 79/285 [15:06<39:40, 11.56s/it] 28%|██▊       | 80/285 [15:16<38:29, 11.26s/it] 28%|██▊       | 81/285 [15:27<37:35, 11.06s/it] 29%|██▉       | 82/285 [15:39<38:20, 11.33s/it] 29%|██▉       | 83/285 [15:50<38:11, 11.34s/it] 29%|██▉       | 84/285 [16:02<37:51, 11.30s/it] 30%|██▉       | 85/285 [16:14<39:02, 11.71s/it] 30%|███       | 86/285 [16:26<38:27, 11.59s/it] 31%|███       | 87/285 [16:37<37:40, 11.41s/it] 31%|███       | 88/285 [16:48<37:12, 11.33s/it] 31%|███       | 89/285 [16:59<37:14, 11.40s/it] 32%|███▏      | 90/285 [17:11<37:00, 11.39s/it] 32%|███▏      | 91/285 [17:22<36:26, 11.27s/it] 32%|███▏      | 92/285 [17:33<36:51, 11.46s/it] 33%|███▎      | 93/285 [17:45<36:40, 11.46s/it] 33%|███▎      | 94/285 [17:57<37:07, 11.66s/it] 33%|███▎      | 95/285 [18:08<36:31, 11.54s/it] 34%|███▎      | 96/285 [18:19<35:57, 11.41s/it] 34%|███▍      | 97/285 [18:30<35:19, 11.27s/it] 34%|███▍      | 98/285 [18:42<35:22, 11.35s/it] 35%|███▍      | 99/285 [18:53<34:30, 11.13s/it] 35%|███▌      | 100/285 [19:04<34:40, 11.24s/it] 35%|███▌      | 101/285 [19:16<34:56, 11.39s/it] 36%|███▌      | 102/285 [19:27<34:44, 11.39s/it] 36%|███▌      | 103/285 [19:38<34:12, 11.28s/it] 36%|███▋      | 104/285 [19:49<34:00, 11.28s/it] 37%|███▋      | 105/285 [20:01<33:43, 11.24s/it] 37%|███▋      | 106/285 [20:12<33:46, 11.32s/it] 38%|███▊      | 107/285 [20:24<33:52, 11.42s/it] 38%|███▊      | 108/285 [20:35<33:40, 11.42s/it] 38%|███▊      | 109/285 [20:48<34:35, 11.80s/it] 39%|███▊      | 110/285 [21:00<34:41, 11.89s/it] 39%|███▉      | 111/285 [21:12<34:25, 11.87s/it] 39%|███▉      | 112/285 [21:24<34:32, 11.98s/it] 40%|███▉      | 113/285 [21:37<34:47, 12.14s/it] 40%|████      | 114/285 [21:47<33:33, 11.78s/it] 40%|████      | 115/285 [22:01<34:28, 12.17s/it] 41%|████      | 116/285 [22:13<34:07, 12.12s/it] 41%|████      | 117/285 [22:24<33:19, 11.90s/it] 41%|████▏     | 118/285 [22:35<32:35, 11.71s/it] 42%|████▏     | 119/285 [22:46<31:31, 11.40s/it] 42%|████▏     | 120/285 [22:57<31:19, 11.39s/it] 42%|████▏     | 121/285 [23:09<31:22, 11.48s/it] 43%|████▎     | 122/285 [23:20<31:13, 11.49s/it] 43%|████▎     | 123/285 [23:32<31:19, 11.60s/it] 44%|████▎     | 124/285 [23:44<30:53, 11.51s/it] 44%|████▍     | 125/285 [23:55<30:51, 11.57s/it] 44%|████▍     | 126/285 [24:06<30:20, 11.45s/it] 45%|████▍     | 127/285 [24:18<29:48, 11.32s/it] 45%|████▍     | 128/285 [24:28<28:59, 11.08s/it] 45%|████▌     | 129/285 [24:39<29:02, 11.17s/it] 46%|████▌     | 130/285 [24:51<29:16, 11.33s/it] 46%|████▌     | 131/285 [25:03<29:14, 11.39s/it] 46%|████▋     | 132/285 [25:14<29:07, 11.42s/it] 47%|████▋     | 133/285 [25:26<29:04, 11.48s/it] 47%|████▋     | 134/285 [25:38<29:15, 11.63s/it] 47%|████▋     | 135/285 [25:49<29:00, 11.61s/it] 48%|████▊     | 136/285 [26:00<28:01, 11.29s/it] 48%|████▊     | 137/285 [26:11<27:46, 11.26s/it] 48%|████▊     | 138/285 [26:22<27:17, 11.14s/it] 49%|████▉     | 139/285 [26:33<27:25, 11.27s/it] 49%|████▉     | 140/285 [26:45<27:30, 11.38s/it] 49%|████▉     | 141/285 [26:57<27:36, 11.50s/it] 50%|████▉     | 142/285 [27:08<27:00, 11.33s/it] 50%|█████     | 143/285 [27:18<26:19, 11.12s/it] 51%|█████     | 144/285 [27:29<25:34, 10.88s/it] 51%|█████     | 145/285 [27:41<26:01, 11.15s/it] 51%|█████     | 146/285 [27:51<25:28, 11.00s/it] 52%|█████▏    | 147/285 [28:02<25:11, 10.95s/it] 52%|█████▏    | 148/285 [28:13<25:19, 11.09s/it] 52%|█████▏    | 149/285 [28:24<25:01, 11.04s/it] 53%|█████▎    | 150/285 [28:35<24:38, 10.95s/it] 53%|█████▎    | 151/285 [28:47<24:49, 11.12s/it] 53%|█████▎    | 152/285 [28:59<25:13, 11.38s/it] 54%|█████▎    | 153/285 [29:11<25:53, 11.77s/it] 54%|█████▍    | 154/285 [29:23<25:23, 11.63s/it] 54%|█████▍    | 155/285 [29:34<24:49, 11.46s/it] 55%|█████▍    | 156/285 [29:46<24:55, 11.60s/it] 55%|█████▌    | 157/285 [29:57<24:20, 11.41s/it] 55%|█████▌    | 158/285 [30:08<24:01, 11.35s/it] 56%|█████▌    | 159/285 [30:19<23:52, 11.37s/it] 56%|█████▌    | 160/285 [30:30<23:18, 11.18s/it] 56%|█████▋    | 161/285 [30:42<23:44, 11.49s/it] 57%|█████▋    | 162/285 [30:54<23:42, 11.57s/it] 57%|█████▋    | 163/285 [31:06<23:57, 11.78s/it] 58%|█████▊    | 164/285 [31:18<23:52, 11.83s/it] 58%|█████▊    | 165/285 [31:31<24:06, 12.05s/it] 58%|█████▊    | 166/285 [31:41<23:09, 11.67s/it] 59%|█████▊    | 167/285 [31:53<22:39, 11.52s/it] 59%|█████▉    | 168/285 [32:04<22:17, 11.43s/it] 59%|█████▉    | 169/285 [32:15<21:58, 11.37s/it] 60%|█████▉    | 170/285 [32:26<21:37, 11.28s/it] 60%|██████    | 171/285 [32:37<21:26, 11.29s/it] 60%|██████    | 172/285 [32:49<21:22, 11.35s/it] 61%|██████    | 173/285 [33:00<20:55, 11.21s/it] 61%|██████    | 174/285 [33:10<20:03, 10.84s/it] 61%|██████▏   | 175/285 [33:19<19:05, 10.41s/it] 62%|██████▏   | 176/285 [33:29<18:41, 10.29s/it] 62%|██████▏   | 177/285 [33:39<18:20, 10.19s/it] 62%|██████▏   | 178/285 [33:49<18:02, 10.12s/it] 63%|██████▎   | 179/285 [33:59<17:47, 10.07s/it] 63%|██████▎   | 180/285 [34:09<17:22,  9.93s/it] 64%|██████▎   | 181/285 [34:19<17:28, 10.08s/it] 64%|██████▍   | 182/285 [34:31<18:23, 10.71s/it] 64%|██████▍   | 183/285 [34:43<18:28, 10.87s/it] 65%|██████▍   | 184/285 [34:54<18:40, 11.10s/it] 65%|██████▍   | 185/285 [35:05<18:31, 11.12s/it] 65%|██████▌   | 186/285 [35:17<18:45, 11.37s/it] 66%|██████▌   | 187/285 [35:28<18:23, 11.26s/it] 66%|██████▌   | 188/285 [35:40<18:22, 11.36s/it] 66%|██████▋   | 189/285 [35:51<18:01, 11.26s/it] 67%|██████▋   | 190/285 [36:02<17:52, 11.29s/it] 67%|██████▋   | 191/285 [36:14<17:47, 11.36s/it] 67%|██████▋   | 192/285 [36:26<18:05, 11.67s/it] 68%|██████▊   | 193/285 [36:39<18:12, 11.88s/it] 68%|██████▊   | 194/285 [36:51<18:05, 11.93s/it] 68%|██████▊   | 195/285 [37:03<18:09, 12.10s/it] 69%|██████▉   | 196/285 [37:14<17:31, 11.82s/it] 69%|██████▉   | 197/285 [37:26<17:07, 11.68s/it] 69%|██████▉   | 198/285 [37:37<16:39, 11.49s/it] 70%|██████▉   | 199/285 [37:48<16:16, 11.36s/it] 70%|███████   | 200/285 [37:59<16:02, 11.32s/it]                                                  70%|███████   | 200/285 [37:59<16:02, 11.32s/it] 71%|███████   | 201/285 [38:10<15:54, 11.36s/it] 71%|███████   | 202/285 [38:22<15:50, 11.45s/it] 71%|███████   | 203/285 [38:33<15:33, 11.38s/it] 72%|███████▏  | 204/285 [38:46<15:52, 11.75s/it] 72%|███████▏  | 205/285 [38:58<15:40, 11.76s/it] 72%|███████▏  | 206/285 [39:09<15:23, 11.69s/it] 73%|███████▎  | 207/285 [39:22<15:34, 11.98s/it] 73%|███████▎  | 208/285 [39:34<15:19, 11.94s/it] 73%|███████▎  | 209/285 [39:46<15:18, 12.08s/it] 74%|███████▎  | 210/285 [39:57<14:40, 11.73s/it] 74%|███████▍  | 211/285 [40:08<14:14, 11.54s/it] 74%|███████▍  | 212/285 [40:19<13:55, 11.45s/it] 75%|███████▍  | 213/285 [40:31<13:38, 11.36s/it] 75%|███████▌  | 214/285 [40:42<13:18, 11.25s/it] 75%|███████▌  | 215/285 [40:53<13:04, 11.20s/it] 76%|███████▌  | 216/285 [41:05<13:14, 11.52s/it] 76%|███████▌  | 217/285 [41:15<12:41, 11.20s/it] 76%|███████▋  | 218/285 [41:27<12:30, 11.20s/it] 77%|███████▋  | 219/285 [41:38<12:18, 11.19s/it] 77%|███████▋  | 220/285 [41:49<12:16, 11.33s/it] 78%|███████▊  | 221/285 [42:01<12:03, 11.31s/it] 78%|███████▊  | 222/285 [42:12<11:48, 11.24s/it] 78%|███████▊  | 223/285 [42:23<11:41, 11.32s/it] 79%|███████▊  | 224/285 [42:35<11:30, 11.32s/it] 79%|███████▉  | 225/285 [42:46<11:16, 11.27s/it] 79%|███████▉  | 226/285 [42:58<11:17, 11.48s/it] 80%|███████▉  | 227/285 [43:09<10:55, 11.30s/it] 80%|████████  | 228/285 [43:20<10:50, 11.41s/it] 80%|████████  | 229/285 [43:33<11:04, 11.87s/it] 81%|████████  | 230/285 [43:45<10:45, 11.73s/it] 81%|████████  | 231/285 [43:55<10:19, 11.48s/it] 81%|████████▏ | 232/285 [44:07<10:13, 11.58s/it] 82%|████████▏ | 233/285 [44:18<09:56, 11.46s/it] 82%|████████▏ | 234/285 [44:30<09:42, 11.42s/it] 82%|████████▏ | 235/285 [44:41<09:29, 11.39s/it] 83%|████████▎ | 236/285 [44:52<09:04, 11.11s/it] 83%|████████▎ | 237/285 [45:03<08:53, 11.10s/it] 84%|████████▎ | 238/285 [45:14<08:39, 11.05s/it] 84%|████████▍ | 239/285 [45:26<08:41, 11.34s/it] 84%|████████▍ | 240/285 [45:38<08:38, 11.53s/it] 85%|████████▍ | 241/285 [45:48<08:14, 11.23s/it] 85%|████████▍ | 242/285 [45:59<08:03, 11.25s/it] 85%|████████▌ | 243/285 [46:11<07:56, 11.35s/it] 86%|████████▌ | 244/285 [46:22<07:37, 11.16s/it] 86%|████████▌ | 245/285 [46:33<07:28, 11.20s/it] 86%|████████▋ | 246/285 [46:42<06:50, 10.54s/it] 87%|████████▋ | 247/285 [46:53<06:50, 10.79s/it] 87%|████████▋ | 248/285 [47:03<06:22, 10.34s/it] 87%|████████▋ | 249/285 [47:12<05:59,  9.99s/it] 88%|████████▊ | 250/285 [47:23<06:02, 10.35s/it] 88%|████████▊ | 251/285 [47:33<05:44, 10.14s/it] 88%|████████▊ | 252/285 [47:43<05:35, 10.18s/it] 89%|████████▉ | 253/285 [47:53<05:22, 10.07s/it] 89%|████████▉ | 254/285 [48:03<05:12, 10.07s/it] 89%|████████▉ | 255/285 [48:13<05:07, 10.24s/it] 90%|████████▉ | 256/285 [48:23<04:51, 10.04s/it] 90%|█████████ | 257/285 [48:34<04:49, 10.34s/it] 91%|█████████ | 258/285 [48:44<04:34, 10.15s/it] 91%|█████████ | 259/285 [48:54<04:26, 10.25s/it] 91%|█████████ | 260/285 [49:05<04:21, 10.47s/it] 92%|█████████▏| 261/285 [49:16<04:15, 10.64s/it] 92%|█████████▏| 262/285 [49:28<04:12, 10.98s/it] 92%|█████████▏| 263/285 [49:40<04:06, 11.20s/it] 93%|█████████▎| 264/285 [49:51<03:55, 11.24s/it] 93%|█████████▎| 265/285 [50:02<03:44, 11.21s/it] 93%|█████████▎| 266/285 [50:13<03:30, 11.06s/it] 94%|█████████▎| 267/285 [50:25<03:21, 11.22s/it] 94%|█████████▍| 268/285 [50:36<03:11, 11.29s/it] 94%|█████████▍| 269/285 [50:47<03:01, 11.32s/it] 95%|█████████▍| 270/285 [50:59<02:52, 11.53s/it] 95%|█████████▌| 271/285 [51:11<02:39, 11.41s/it] 95%|█████████▌| 272/285 [51:21<02:25, 11.21s/it] 96%|█████████▌| 273/285 [51:32<02:13, 11.09s/it] 96%|█████████▌| 274/285 [51:44<02:05, 11.37s/it] 96%|█████████▋| 275/285 [51:56<01:56, 11.65s/it] 97%|█████████▋| 276/285 [52:08<01:44, 11.58s/it] 97%|█████████▋| 277/285 [52:18<01:30, 11.30s/it] 98%|█████████▊| 278/285 [52:30<01:19, 11.29s/it] 98%|█████████▊| 279/285 [52:41<01:07, 11.32s/it] 98%|█████████▊| 280/285 [52:53<00:58, 11.60s/it] 99%|█████████▊| 281/285 [53:05<00:46, 11.56s/it] 99%|█████████▉| 282/285 [53:16<00:34, 11.50s/it] 99%|█████████▉| 283/285 [53:28<00:23, 11.72s/it]100%|█████████▉| 284/285 [53:39<00:11, 11.45s/it]100%|██████████| 285/285 [53:51<00:00, 11.57s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [53:51<00:00, 11.57s/it]100%|██████████| 285/285 [53:51<00:00, 11.34s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652359063
{'loss': 0.449, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 3231.617, 'train_samples_per_second': 5.654, 'train_steps_per_second': 0.088, 'train_loss': 0.345784732751679, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:15<1:12:42, 15.36s/it]  1%|          | 2/285 [00:30<1:11:48, 15.23s/it]  1%|          | 3/285 [00:46<1:13:53, 15.72s/it]  1%|▏         | 4/285 [01:03<1:14:58, 16.01s/it]  2%|▏         | 5/285 [01:20<1:16:41, 16.43s/it]  2%|▏         | 6/285 [01:37<1:16:49, 16.52s/it]  2%|▏         | 7/285 [01:52<1:15:32, 16.30s/it]  3%|▎         | 8/285 [02:10<1:16:19, 16.53s/it]  3%|▎         | 9/285 [02:25<1:14:29, 16.19s/it]  4%|▎         | 10/285 [02:42<1:15:51, 16.55s/it]  4%|▍         | 11/285 [02:59<1:15:12, 16.47s/it]  4%|▍         | 12/285 [03:15<1:15:12, 16.53s/it]  5%|▍         | 13/285 [03:32<1:15:12, 16.59s/it]  5%|▍         | 14/285 [03:50<1:17:21, 17.13s/it]  5%|▌         | 15/285 [04:08<1:17:08, 17.14s/it]  6%|▌         | 16/285 [04:24<1:15:34, 16.86s/it]  6%|▌         | 17/285 [04:40<1:14:17, 16.63s/it]  6%|▋         | 18/285 [04:56<1:13:57, 16.62s/it]  7%|▋         | 19/285 [05:13<1:13:22, 16.55s/it]  7%|▋         | 20/285 [05:29<1:13:05, 16.55s/it]  7%|▋         | 21/285 [05:47<1:14:21, 16.90s/it]  8%|▊         | 22/285 [06:03<1:12:42, 16.59s/it]  8%|▊         | 23/285 [06:20<1:12:34, 16.62s/it]  8%|▊         | 24/285 [06:37<1:12:37, 16.70s/it]  9%|▉         | 25/285 [06:53<1:12:18, 16.69s/it]  9%|▉         | 26/285 [07:09<1:10:22, 16.30s/it]  9%|▉         | 27/285 [07:24<1:08:41, 15.97s/it] 10%|▉         | 28/285 [07:41<1:10:01, 16.35s/it] 10%|█         | 29/285 [07:57<1:09:31, 16.29s/it] 11%|█         | 30/285 [08:14<1:10:06, 16.50s/it] 11%|█         | 31/285 [08:31<1:10:15, 16.60s/it] 11%|█         | 32/285 [08:47<1:09:44, 16.54s/it] 12%|█▏        | 33/285 [09:03<1:08:47, 16.38s/it] 12%|█▏        | 34/285 [09:21<1:09:29, 16.61s/it] 12%|█▏        | 35/285 [09:36<1:07:33, 16.21s/it] 13%|█▎        | 36/285 [09:52<1:07:07, 16.18s/it] 13%|█▎        | 37/285 [10:09<1:07:54, 16.43s/it] 13%|█▎        | 38/285 [10:25<1:07:29, 16.40s/it] 14%|█▎        | 39/285 [10:42<1:07:24, 16.44s/it] 14%|█▍        | 40/285 [10:58<1:07:01, 16.41s/it] 14%|█▍        | 41/285 [11:15<1:06:51, 16.44s/it] 15%|█▍        | 42/285 [11:31<1:06:00, 16.30s/it] 15%|█▌        | 43/285 [11:46<1:05:06, 16.14s/it] 15%|█▌        | 44/285 [12:03<1:04:54, 16.16s/it] 16%|█▌        | 45/285 [12:19<1:05:05, 16.27s/it] 16%|█▌        | 46/285 [12:35<1:03:48, 16.02s/it] 16%|█▋        | 47/285 [12:50<1:02:44, 15.82s/it] 17%|█▋        | 48/285 [13:07<1:03:32, 16.09s/it] 17%|█▋        | 49/285 [13:24<1:04:59, 16.52s/it] 18%|█▊        | 50/285 [13:41<1:05:36, 16.75s/it] 18%|█▊        | 51/285 [14:00<1:07:09, 17.22s/it] 18%|█▊        | 52/285 [14:16<1:05:56, 16.98s/it] 19%|█▊        | 53/285 [14:33<1:04:58, 16.80s/it] 19%|█▉        | 54/285 [14:50<1:05:16, 16.96s/it] 19%|█▉        | 55/285 [15:06<1:04:14, 16.76s/it] 20%|█▉        | 56/285 [15:25<1:05:56, 17.28s/it] 20%|██        | 57/285 [15:41<1:05:03, 17.12s/it] 20%|██        | 58/285 [16:01<1:07:34, 17.86s/it] 21%|██        | 59/285 [16:17<1:04:54, 17.23s/it] 21%|██        | 60/285 [16:33<1:03:12, 16.86s/it] 21%|██▏       | 61/285 [16:48<1:00:58, 16.33s/it] 22%|██▏       | 62/285 [17:03<59:34, 16.03s/it]   22%|██▏       | 63/285 [17:21<1:00:45, 16.42s/it] 22%|██▏       | 64/285 [17:37<1:00:17, 16.37s/it] 23%|██▎       | 65/285 [17:53<1:00:22, 16.47s/it] 23%|██▎       | 66/285 [18:11<1:01:14, 16.78s/it] 24%|██▎       | 67/285 [18:25<58:29, 16.10s/it]   24%|██▍       | 68/285 [18:42<58:38, 16.22s/it] 24%|██▍       | 69/285 [19:00<1:00:33, 16.82s/it] 25%|██▍       | 70/285 [19:17<1:00:16, 16.82s/it] 25%|██▍       | 71/285 [19:34<1:00:19, 16.91s/it] 25%|██▌       | 72/285 [19:51<1:00:16, 16.98s/it] 26%|██▌       | 73/285 [20:07<58:39, 16.60s/it]   26%|██▌       | 74/285 [20:21<56:00, 15.93s/it] 26%|██▋       | 75/285 [20:37<55:50, 15.95s/it] 27%|██▋       | 76/285 [20:54<56:13, 16.14s/it] 27%|██▋       | 77/285 [21:13<58:42, 16.93s/it] 27%|██▋       | 78/285 [21:28<56:51, 16.48s/it] 28%|██▊       | 79/285 [21:45<57:10, 16.65s/it] 28%|██▊       | 80/285 [22:00<55:03, 16.11s/it] 28%|██▊       | 81/285 [22:15<53:20, 15.69s/it] 29%|██▉       | 82/285 [22:32<54:55, 16.23s/it] 29%|██▉       | 83/285 [22:48<54:37, 16.23s/it] 29%|██▉       | 84/285 [23:05<54:24, 16.24s/it] 30%|██▉       | 85/285 [23:23<56:25, 16.93s/it] 30%|███       | 86/285 [23:40<55:45, 16.81s/it] 31%|███       | 87/285 [23:55<54:15, 16.44s/it] 31%|███       | 88/285 [24:12<53:48, 16.39s/it] 31%|███       | 89/285 [24:28<53:40, 16.43s/it] 32%|███▏      | 90/285 [24:44<53:11, 16.37s/it] 32%|███▏      | 91/285 [25:00<52:22, 16.20s/it] 32%|███▏      | 92/285 [25:17<53:00, 16.48s/it] 33%|███▎      | 93/285 [25:34<52:49, 16.51s/it] 33%|███▎      | 94/285 [25:52<53:38, 16.85s/it] 33%|███▎      | 95/285 [26:08<53:02, 16.75s/it] 34%|███▎      | 96/285 [26:24<52:16, 16.60s/it] 34%|███▍      | 97/285 [26:40<51:21, 16.39s/it] 34%|███▍      | 98/285 [26:57<51:36, 16.56s/it] 35%|███▍      | 99/285 [27:12<49:55, 16.10s/it] 35%|███▌      | 100/285 [27:29<50:39, 16.43s/it] 35%|███▌      | 101/285 [27:46<50:42, 16.54s/it] 36%|███▌      | 102/285 [28:02<50:05, 16.42s/it] 36%|███▌      | 103/285 [28:18<49:22, 16.28s/it] 36%|███▋      | 104/285 [28:35<49:17, 16.34s/it] 37%|███▋      | 105/285 [28:50<48:23, 16.13s/it] 37%|███▋      | 106/285 [29:08<49:09, 16.48s/it] 38%|███▊      | 107/285 [29:24<49:03, 16.54s/it] 38%|███▊      | 108/285 [29:41<48:59, 16.61s/it] 38%|███▊      | 109/285 [30:00<50:23, 17.18s/it] 39%|███▊      | 110/285 [30:17<50:17, 17.24s/it] 39%|███▉      | 111/285 [30:35<50:10, 17.30s/it] 39%|███▉      | 112/285 [30:53<50:44, 17.60s/it] 40%|███▉      | 113/285 [31:11<51:05, 17.82s/it] 40%|████      | 114/285 [31:27<49:07, 17.24s/it] 40%|████      | 115/285 [31:46<50:10, 17.71s/it] 41%|████      | 116/285 [32:04<49:52, 17.71s/it] 41%|████      | 117/285 [32:20<48:38, 17.37s/it] 41%|████▏     | 118/285 [32:36<47:29, 17.06s/it] 42%|████▏     | 119/285 [32:52<45:51, 16.57s/it] 42%|████▏     | 120/285 [33:09<45:38, 16.60s/it] 42%|████▏     | 121/285 [33:26<45:39, 16.70s/it] 43%|████▎     | 122/285 [33:42<45:35, 16.78s/it] 43%|████▎     | 123/285 [34:00<45:54, 17.01s/it] 44%|████▎     | 124/285 [34:17<45:16, 16.87s/it] 44%|████▍     | 125/285 [34:34<45:04, 16.90s/it] 44%|████▍     | 126/285 [34:49<43:48, 16.53s/it] 45%|████▍     | 127/285 [35:05<42:45, 16.24s/it] 45%|████▍     | 128/285 [35:20<41:26, 15.83s/it] 45%|████▌     | 129/285 [35:36<41:36, 16.00s/it] 46%|████▌     | 130/285 [35:53<41:54, 16.22s/it] 46%|████▌     | 131/285 [36:10<42:08, 16.42s/it] 46%|████▋     | 132/285 [36:27<42:13, 16.56s/it] 47%|████▋     | 133/285 [36:43<41:49, 16.51s/it] 47%|████▋     | 134/285 [37:01<42:44, 16.98s/it] 47%|████▋     | 135/285 [37:18<42:21, 16.94s/it] 48%|████▊     | 136/285 [37:33<40:35, 16.35s/it] 48%|████▊     | 137/285 [37:49<40:16, 16.33s/it] 48%|████▊     | 138/285 [38:05<39:32, 16.14s/it] 49%|████▉     | 139/285 [38:21<39:26, 16.21s/it] 49%|████▉     | 140/285 [38:39<40:00, 16.56s/it] 49%|████▉     | 141/285 [38:56<40:17, 16.79s/it] 50%|████▉     | 142/285 [39:12<39:20, 16.50s/it] 50%|█████     | 143/285 [39:27<38:22, 16.21s/it] 51%|█████     | 144/285 [39:43<37:38, 16.02s/it] 51%|█████     | 145/285 [40:01<38:53, 16.67s/it] 51%|█████     | 146/285 [40:17<38:04, 16.43s/it] 52%|█████▏    | 147/285 [40:33<37:33, 16.33s/it] 52%|█████▏    | 148/285 [40:50<37:48, 16.56s/it] 52%|█████▏    | 149/285 [41:06<37:15, 16.44s/it] 53%|█████▎    | 150/285 [41:23<37:01, 16.45s/it] 53%|█████▎    | 151/285 [41:40<37:24, 16.75s/it] 53%|█████▎    | 152/285 [41:57<36:56, 16.67s/it] 54%|█████▎    | 153/285 [42:15<37:35, 17.09s/it] 54%|█████▍    | 154/285 [42:31<37:05, 16.99s/it] 54%|█████▍    | 155/285 [42:48<36:18, 16.76s/it] 55%|█████▍    | 156/285 [43:05<36:30, 16.98s/it] 55%|█████▌    | 157/285 [43:21<35:20, 16.57s/it] 55%|█████▌    | 158/285 [43:37<34:44, 16.41s/it] 56%|█████▌    | 159/285 [43:53<34:31, 16.44s/it] 56%|█████▌    | 160/285 [44:09<33:39, 16.15s/it] 56%|█████▋    | 161/285 [44:27<34:22, 16.63s/it] 57%|█████▋    | 162/285 [44:44<34:20, 16.75s/it] 57%|█████▋    | 163/285 [45:01<34:33, 17.00s/it] 58%|█████▊    | 164/285 [45:19<34:32, 17.13s/it] 58%|█████▊    | 165/285 [45:37<35:04, 17.53s/it] 58%|█████▊    | 166/285 [45:53<33:45, 17.02s/it] 59%|█████▊    | 167/285 [46:09<32:50, 16.70s/it] 59%|█████▉    | 168/285 [46:25<32:06, 16.47s/it] 59%|█████▉    | 169/285 [46:41<31:53, 16.50s/it] 60%|█████▉    | 170/285 [46:58<31:31, 16.45s/it] 60%|██████    | 171/285 [47:14<31:21, 16.50s/it] 60%|██████    | 172/285 [47:31<31:03, 16.49s/it] 61%|██████    | 173/285 [47:46<30:17, 16.23s/it] 61%|██████    | 174/285 [48:02<29:39, 16.04s/it] 61%|██████▏   | 175/285 [48:18<29:20, 16.00s/it] 62%|██████▏   | 176/285 [48:35<29:30, 16.25s/it] 62%|██████▏   | 177/285 [48:51<29:29, 16.39s/it] 62%|██████▏   | 178/285 [49:08<29:31, 16.56s/it] 63%|██████▎   | 179/285 [49:25<29:30, 16.70s/it] 63%|██████▎   | 180/285 [49:42<29:03, 16.60s/it] 64%|██████▎   | 181/285 [49:59<29:09, 16.83s/it] 64%|██████▍   | 182/285 [50:16<29:06, 16.96s/it] 64%|██████▍   | 183/285 [50:33<28:28, 16.75s/it] 65%|██████▍   | 184/285 [50:50<28:31, 16.95s/it] 65%|██████▍   | 185/285 [51:06<27:52, 16.73s/it] 65%|██████▌   | 186/285 [51:24<28:03, 17.00s/it] 66%|██████▌   | 187/285 [51:40<27:07, 16.60s/it] 66%|██████▌   | 188/285 [51:57<26:58, 16.69s/it] 66%|██████▋   | 189/285 [52:12<26:20, 16.46s/it] 67%|██████▋   | 190/285 [52:29<26:11, 16.54s/it] 67%|██████▋   | 191/285 [52:46<26:04, 16.65s/it] 67%|██████▋   | 192/285 [53:05<26:46, 17.27s/it] 68%|██████▊   | 193/285 [53:23<26:53, 17.53s/it] 68%|██████▊   | 194/285 [53:40<26:23, 17.40s/it] 68%|██████▊   | 195/285 [53:58<26:24, 17.60s/it] 69%|██████▉   | 196/285 [54:14<25:24, 17.13s/it] 69%|██████▉   | 197/285 [54:29<24:13, 16.52s/it] 69%|██████▉   | 198/285 [54:45<23:34, 16.26s/it] 70%|██████▉   | 199/285 [55:01<23:09, 16.15s/it] 70%|███████   | 200/285 [55:17<22:48, 16.10s/it]                                                  70%|███████   | 200/285 [55:17<22:48, 16.10s/it] 71%|███████   | 201/285 [55:33<22:33, 16.11s/it] 71%|███████   | 202/285 [55:50<22:33, 16.31s/it] 71%|███████   | 203/285 [56:05<21:59, 16.09s/it] 72%|███████▏  | 204/285 [56:23<22:32, 16.70s/it] 72%|███████▏  | 205/285 [56:39<21:52, 16.41s/it] 72%|███████▏  | 206/285 [56:56<21:48, 16.56s/it] 73%|███████▎  | 207/285 [57:14<22:16, 17.13s/it] 73%|███████▎  | 208/285 [57:31<21:50, 17.02s/it] 73%|███████▎  | 209/285 [57:49<22:01, 17.38s/it] 74%|███████▎  | 210/285 [58:05<21:09, 16.93s/it] 74%|███████▍  | 211/285 [58:21<20:33, 16.67s/it] 74%|███████▍  | 212/285 [58:38<20:15, 16.66s/it] 75%|███████▍  | 213/285 [58:54<19:45, 16.46s/it] 75%|███████▌  | 214/285 [59:10<19:18, 16.31s/it] 75%|███████▌  | 215/285 [59:26<18:52, 16.18s/it] 76%|███████▌  | 216/285 [59:44<19:15, 16.74s/it] 76%|███████▌  | 217/285 [59:59<18:22, 16.22s/it] 76%|███████▋  | 218/285 [1:00:15<18:06, 16.22s/it] 77%|███████▋  | 219/285 [1:00:31<17:47, 16.18s/it] 77%|███████▋  | 220/285 [1:00:48<17:47, 16.42s/it] 78%|███████▊  | 221/285 [1:01:04<17:22, 16.30s/it] 78%|███████▊  | 222/285 [1:01:20<17:02, 16.23s/it] 78%|███████▊  | 223/285 [1:01:37<16:58, 16.42s/it] 79%|███████▊  | 224/285 [1:01:54<16:47, 16.52s/it] 79%|███████▉  | 225/285 [1:02:10<16:23, 16.40s/it] 79%|███████▉  | 226/285 [1:02:27<16:16, 16.55s/it] 80%|███████▉  | 227/285 [1:02:43<15:52, 16.42s/it] 80%|████████  | 228/285 [1:03:00<15:51, 16.69s/it] 80%|████████  | 229/285 [1:03:19<16:11, 17.36s/it] 81%|████████  | 230/285 [1:03:36<15:42, 17.13s/it] 81%|████████  | 231/285 [1:03:51<14:59, 16.65s/it] 81%|████████▏ | 232/285 [1:04:08<14:46, 16.73s/it] 82%|████████▏ | 233/285 [1:04:24<14:19, 16.53s/it] 82%|████████▏ | 234/285 [1:04:41<14:01, 16.51s/it] 82%|████████▏ | 235/285 [1:04:57<13:35, 16.31s/it] 83%|████████▎ | 236/285 [1:05:11<12:54, 15.80s/it] 83%|████████▎ | 237/285 [1:05:27<12:42, 15.89s/it] 84%|████████▎ | 238/285 [1:05:43<12:22, 15.81s/it] 84%|████████▍ | 239/285 [1:06:01<12:37, 16.47s/it] 84%|████████▍ | 240/285 [1:06:19<12:40, 16.90s/it] 85%|████████▍ | 241/285 [1:06:34<12:05, 16.50s/it] 85%|████████▍ | 242/285 [1:06:51<11:52, 16.57s/it] 85%|████████▌ | 243/285 [1:07:08<11:36, 16.57s/it] 86%|████████▌ | 244/285 [1:07:23<11:07, 16.29s/it] 86%|████████▌ | 245/285 [1:07:39<10:48, 16.22s/it] 86%|████████▋ | 246/285 [1:07:55<10:21, 15.94s/it] 87%|████████▋ | 247/285 [1:08:14<10:38, 16.80s/it] 87%|████████▋ | 248/285 [1:08:29<10:10, 16.51s/it] 87%|████████▋ | 249/285 [1:08:45<09:43, 16.20s/it] 88%|████████▊ | 250/285 [1:09:03<09:49, 16.85s/it] 88%|████████▊ | 251/285 [1:09:20<09:28, 16.72s/it] 88%|████████▊ | 252/285 [1:09:37<09:16, 16.88s/it] 89%|████████▉ | 253/285 [1:09:53<08:56, 16.76s/it] 89%|████████▉ | 254/285 [1:10:10<08:39, 16.77s/it] 89%|████████▉ | 255/285 [1:10:28<08:29, 16.99s/it] 90%|████████▉ | 256/285 [1:10:44<08:04, 16.70s/it] 90%|█████████ | 257/285 [1:11:02<08:00, 17.17s/it] 91%|█████████ | 258/285 [1:11:19<07:38, 16.98s/it] 91%|█████████ | 259/285 [1:11:35<07:15, 16.74s/it] 91%|█████████ | 260/285 [1:11:51<06:52, 16.49s/it] 92%|█████████▏| 261/285 [1:12:06<06:30, 16.27s/it] 92%|█████████▏| 262/285 [1:12:24<06:20, 16.53s/it] 92%|█████████▏| 263/285 [1:12:41<06:07, 16.71s/it] 93%|█████████▎| 264/285 [1:12:57<05:48, 16.59s/it] 93%|█████████▎| 265/285 [1:13:13<05:29, 16.50s/it] 93%|█████████▎| 266/285 [1:13:28<05:03, 15.96s/it] 94%|█████████▎| 267/285 [1:13:44<04:49, 16.09s/it] 94%|█████████▍| 268/285 [1:14:01<04:35, 16.22s/it] 94%|█████████▍| 269/285 [1:14:17<04:18, 16.18s/it] 95%|█████████▍| 270/285 [1:14:35<04:09, 16.62s/it] 95%|█████████▌| 271/285 [1:14:51<03:49, 16.42s/it] 95%|█████████▌| 272/285 [1:15:06<03:30, 16.15s/it] 96%|█████████▌| 273/285 [1:15:22<03:12, 16.07s/it] 96%|█████████▌| 274/285 [1:15:40<03:02, 16.59s/it] 96%|█████████▋| 275/285 [1:15:58<02:50, 17.08s/it] 97%|█████████▋| 276/285 [1:16:14<02:31, 16.88s/it] 97%|█████████▋| 277/285 [1:16:30<02:11, 16.40s/it] 98%|█████████▊| 278/285 [1:16:46<01:54, 16.33s/it] 98%|█████████▊| 279/285 [1:17:02<01:37, 16.33s/it] 98%|█████████▊| 280/285 [1:17:20<01:24, 16.85s/it] 99%|█████████▊| 281/285 [1:17:37<01:06, 16.71s/it] 99%|█████████▉| 282/285 [1:17:53<00:50, 16.67s/it] 99%|█████████▉| 283/285 [1:18:11<00:34, 17.11s/it]100%|█████████▉| 284/285 [1:18:27<00:16, 16.62s/it]100%|██████████| 285/285 [1:18:44<00:00, 16.86s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 285/285 [1:18:44<00:00, 16.86s/it]100%|██████████| 285/285 [1:18:44<00:00, 16.58s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652356503
{'loss': 0.2546, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 4724.735, 'train_samples_per_second': 3.867, 'train_steps_per_second': 0.06, 'train_loss': 0.18339498168543766, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:16<1:18:32, 16.59s/it]  1%|          | 2/285 [00:33<1:17:51, 16.51s/it]  1%|          | 3/285 [00:50<1:20:35, 17.15s/it]  1%|▏         | 4/285 [01:08<1:21:43, 17.45s/it]  2%|▏         | 5/285 [01:27<1:23:13, 17.83s/it]  2%|▏         | 6/285 [01:45<1:22:47, 17.81s/it]  2%|▏         | 7/285 [02:01<1:20:53, 17.46s/it]  3%|▎         | 8/285 [02:19<1:21:23, 17.63s/it]  3%|▎         | 9/285 [02:36<1:19:56, 17.38s/it]  4%|▎         | 10/285 [02:54<1:20:55, 17.66s/it]  4%|▍         | 11/285 [03:12<1:19:49, 17.48s/it]  4%|▍         | 12/285 [03:29<1:19:46, 17.53s/it]  5%|▍         | 13/285 [03:47<1:19:57, 17.64s/it]  5%|▍         | 14/285 [04:06<1:22:04, 18.17s/it]  5%|▌         | 15/285 [04:25<1:21:39, 18.15s/it]  6%|▌         | 16/285 [04:42<1:20:17, 17.91s/it]  6%|▌         | 17/285 [04:59<1:19:18, 17.76s/it]  6%|▋         | 18/285 [05:17<1:18:25, 17.62s/it]  7%|▋         | 19/285 [05:32<1:15:44, 17.08s/it]  7%|▋         | 20/285 [05:49<1:14:03, 16.77s/it]  7%|▋         | 21/285 [06:06<1:14:32, 16.94s/it]  8%|▊         | 22/285 [06:21<1:12:07, 16.45s/it]  8%|▊         | 23/285 [06:38<1:11:42, 16.42s/it]  8%|▊         | 24/285 [06:54<1:11:26, 16.42s/it]  9%|▉         | 25/285 [07:10<1:10:35, 16.29s/it]  9%|▉         | 26/285 [07:24<1:07:52, 15.73s/it]  9%|▉         | 27/285 [07:38<1:05:36, 15.26s/it] 10%|▉         | 28/285 [07:56<1:07:35, 15.78s/it] 10%|█         | 29/285 [08:11<1:07:04, 15.72s/it] 11%|█         | 30/285 [08:28<1:07:54, 15.98s/it] 11%|█         | 31/285 [08:44<1:08:11, 16.11s/it] 11%|█         | 32/285 [09:01<1:08:54, 16.34s/it] 12%|█▏        | 33/285 [09:18<1:09:51, 16.63s/it] 12%|█▏        | 34/285 [09:37<1:11:39, 17.13s/it] 12%|█▏        | 35/285 [09:53<1:10:36, 16.94s/it] 13%|█▎        | 36/285 [10:10<1:10:38, 17.02s/it] 13%|█▎        | 37/285 [10:28<1:11:28, 17.29s/it] 13%|█▎        | 38/285 [10:45<1:11:07, 17.28s/it] 14%|█▎        | 39/285 [11:03<1:11:05, 17.34s/it] 14%|█▍        | 40/285 [11:20<1:10:34, 17.29s/it] 14%|█▍        | 41/285 [11:38<1:10:52, 17.43s/it] 15%|█▍        | 42/285 [11:55<1:10:08, 17.32s/it] 15%|█▌        | 43/285 [12:12<1:09:16, 17.18s/it] 15%|█▌        | 44/285 [12:29<1:09:36, 17.33s/it] 16%|█▌        | 45/285 [12:47<1:09:37, 17.40s/it] 16%|█▌        | 46/285 [13:04<1:08:25, 17.18s/it] 16%|█▋        | 47/285 [13:20<1:07:31, 17.02s/it] 17%|█▋        | 48/285 [13:38<1:08:16, 17.28s/it] 17%|█▋        | 49/285 [13:57<1:09:38, 17.70s/it] 18%|█▊        | 50/285 [14:16<1:10:28, 17.99s/it] 18%|█▊        | 51/285 [14:35<1:11:40, 18.38s/it] 18%|█▊        | 52/285 [14:52<1:10:19, 18.11s/it] 19%|█▊        | 53/285 [15:09<1:08:36, 17.74s/it] 19%|█▉        | 54/285 [15:27<1:08:51, 17.89s/it] 19%|█▉        | 55/285 [15:45<1:07:56, 17.72s/it] 20%|█▉        | 56/285 [16:04<1:09:52, 18.31s/it] 20%|██        | 57/285 [16:23<1:09:17, 18.23s/it] 20%|██        | 58/285 [16:43<1:11:59, 19.03s/it] 21%|██        | 59/285 [17:00<1:09:27, 18.44s/it] 21%|██        | 60/285 [17:18<1:07:46, 18.07s/it] 21%|██▏       | 61/285 [17:34<1:05:48, 17.63s/it] 22%|██▏       | 62/285 [17:51<1:04:18, 17.30s/it] 22%|██▏       | 63/285 [18:09<1:05:07, 17.60s/it] 22%|██▏       | 64/285 [18:26<1:04:30, 17.51s/it] 23%|██▎       | 65/285 [18:44<1:04:32, 17.60s/it] 23%|██▎       | 66/285 [19:03<1:05:31, 17.95s/it] 24%|██▎       | 67/285 [19:19<1:02:48, 17.28s/it] 24%|██▍       | 68/285 [19:36<1:03:01, 17.42s/it] 24%|██▍       | 69/285 [19:56<1:04:34, 17.94s/it] 25%|██▍       | 70/285 [20:13<1:04:11, 17.92s/it] 25%|██▍       | 71/285 [20:32<1:04:09, 17.99s/it] 25%|██▌       | 72/285 [20:50<1:04:00, 18.03s/it] 26%|██▌       | 73/285 [21:07<1:02:30, 17.69s/it] 26%|██▌       | 74/285 [21:23<1:00:21, 17.16s/it] 26%|██▋       | 75/285 [21:40<1:00:17, 17.23s/it] 27%|██▋       | 76/285 [21:58<1:00:40, 17.42s/it] 27%|██▋       | 77/285 [22:18<1:02:53, 18.14s/it] 27%|██▋       | 78/285 [22:34<1:01:10, 17.73s/it] 28%|██▊       | 79/285 [22:52<1:01:11, 17.82s/it] 28%|██▊       | 80/285 [23:09<59:20, 17.37s/it]   28%|██▊       | 81/285 [23:25<58:02, 17.07s/it] 29%|██▉       | 82/285 [23:44<59:19, 17.53s/it] 29%|██▉       | 83/285 [24:01<59:02, 17.54s/it] 29%|██▉       | 84/285 [24:19<58:42, 17.53s/it] 30%|██▉       | 85/285 [24:38<1:00:31, 18.16s/it] 30%|███       | 86/285 [24:56<59:40, 17.99s/it]   31%|███       | 87/285 [25:13<58:17, 17.66s/it] 31%|███       | 88/285 [25:30<57:42, 17.58s/it] 31%|███       | 89/285 [25:48<57:42, 17.66s/it] 32%|███▏      | 90/285 [26:06<57:04, 17.56s/it] 32%|███▏      | 91/285 [26:22<56:10, 17.37s/it] 32%|███▏      | 92/285 [26:41<56:39, 17.61s/it] 33%|███▎      | 93/285 [26:58<56:18, 17.60s/it] 33%|███▎      | 94/285 [27:17<57:10, 17.96s/it] 33%|███▎      | 95/285 [27:35<56:31, 17.85s/it] 34%|███▎      | 96/285 [27:52<55:50, 17.73s/it] 34%|███▍      | 97/285 [28:09<54:50, 17.50s/it] 34%|███▍      | 98/285 [28:27<54:53, 17.61s/it] 35%|███▍      | 99/285 [28:43<53:24, 17.23s/it] 35%|███▌      | 100/285 [29:01<53:51, 17.47s/it] 35%|███▌      | 101/285 [29:19<54:13, 17.68s/it] 36%|███▌      | 102/285 [29:37<53:43, 17.61s/it] 36%|███▌      | 103/285 [29:54<52:56, 17.45s/it] 36%|███▋      | 104/285 [30:11<52:24, 17.37s/it] 37%|███▋      | 105/285 [30:28<51:42, 17.24s/it] 37%|███▋      | 106/285 [30:46<52:13, 17.50s/it] 38%|███▊      | 107/285 [31:04<52:15, 17.62s/it] 38%|███▊      | 108/285 [31:22<52:09, 17.68s/it] 38%|███▊      | 109/285 [31:42<53:39, 18.30s/it] 39%|███▊      | 110/285 [32:00<53:43, 18.42s/it] 39%|███▉      | 111/285 [32:19<53:29, 18.45s/it] 39%|███▉      | 112/285 [32:38<53:54, 18.69s/it] 40%|███▉      | 113/285 [32:58<54:13, 18.91s/it] 40%|████      | 114/285 [33:14<52:12, 18.32s/it] 40%|████      | 115/285 [33:35<53:25, 18.86s/it] 41%|████      | 116/285 [33:53<53:03, 18.84s/it] 41%|████      | 117/285 [34:11<51:38, 18.45s/it] 41%|████▏     | 118/285 [34:28<50:36, 18.18s/it] 42%|████▏     | 119/285 [34:46<49:30, 17.89s/it] 42%|████▏     | 120/285 [35:03<49:04, 17.85s/it] 42%|████▏     | 121/285 [35:22<48:59, 17.92s/it] 43%|████▎     | 122/285 [35:40<48:45, 17.95s/it] 43%|████▎     | 123/285 [35:58<48:59, 18.15s/it] 44%|████▎     | 124/285 [36:16<48:19, 18.01s/it] 44%|████▍     | 125/285 [36:34<48:07, 18.05s/it] 44%|████▍     | 126/285 [36:51<47:00, 17.74s/it] 45%|████▍     | 127/285 [37:08<46:05, 17.50s/it] 45%|████▍     | 128/285 [37:24<44:33, 17.03s/it] 45%|████▌     | 129/285 [37:41<44:42, 17.20s/it] 46%|████▌     | 130/285 [37:59<45:00, 17.42s/it] 46%|████▌     | 131/285 [38:17<45:07, 17.58s/it] 46%|████▋     | 132/285 [38:35<45:02, 17.67s/it] 47%|████▋     | 133/285 [38:53<44:35, 17.60s/it] 47%|████▋     | 134/285 [39:12<45:14, 17.98s/it] 47%|████▋     | 135/285 [39:29<44:48, 17.92s/it] 48%|████▊     | 136/285 [39:45<43:05, 17.35s/it] 48%|████▊     | 137/285 [40:03<42:46, 17.34s/it] 48%|████▊     | 138/285 [40:19<41:59, 17.14s/it] 49%|████▉     | 139/285 [40:37<41:58, 17.25s/it] 49%|████▉     | 140/285 [40:55<42:15, 17.49s/it] 49%|████▉     | 141/285 [41:13<42:37, 17.76s/it] 50%|████▉     | 142/285 [41:30<41:42, 17.50s/it] 50%|█████     | 143/285 [41:47<40:50, 17.26s/it] 51%|█████     | 144/285 [42:04<40:11, 17.10s/it] 51%|█████     | 145/285 [42:23<41:22, 17.73s/it] 51%|█████     | 146/285 [42:40<40:49, 17.62s/it] 52%|█████▏    | 147/285 [42:57<40:17, 17.52s/it] 52%|█████▏    | 148/285 [43:15<40:13, 17.62s/it] 52%|█████▏    | 149/285 [43:32<39:19, 17.35s/it] 53%|█████▎    | 150/285 [43:49<38:48, 17.25s/it] 53%|█████▎    | 151/285 [44:07<39:12, 17.55s/it] 53%|█████▎    | 152/285 [44:25<39:02, 17.61s/it] 54%|█████▎    | 153/285 [44:44<39:43, 18.06s/it] 54%|█████▍    | 154/285 [45:02<39:15, 17.98s/it] 54%|█████▍    | 155/285 [45:19<38:32, 17.79s/it] 55%|█████▍    | 156/285 [45:38<38:42, 18.00s/it] 55%|█████▌    | 157/285 [45:55<37:37, 17.63s/it] 55%|█████▌    | 158/285 [46:12<37:03, 17.50s/it] 56%|█████▌    | 159/285 [46:29<36:39, 17.45s/it] 56%|█████▌    | 160/285 [46:46<35:44, 17.15s/it] 56%|█████▋    | 161/285 [47:04<36:23, 17.61s/it] 57%|█████▋    | 162/285 [47:23<36:35, 17.85s/it] 57%|█████▋    | 163/285 [47:41<36:52, 18.13s/it] 58%|█████▊    | 164/285 [48:00<36:52, 18.29s/it] 58%|█████▊    | 165/285 [48:20<37:22, 18.69s/it] 58%|█████▊    | 166/285 [48:37<36:00, 18.16s/it] 59%|█████▊    | 167/285 [48:54<35:09, 17.88s/it] 59%|█████▉    | 168/285 [49:11<34:24, 17.65s/it] 59%|█████▉    | 169/285 [49:29<34:09, 17.67s/it] 60%|█████▉    | 170/285 [49:46<33:47, 17.63s/it] 60%|██████    | 171/285 [50:04<33:32, 17.66s/it] 60%|██████    | 172/285 [50:22<33:22, 17.72s/it] 61%|██████    | 173/285 [50:39<32:33, 17.44s/it] 61%|██████    | 174/285 [50:55<31:55, 17.26s/it] 61%|██████▏   | 175/285 [51:13<31:33, 17.21s/it] 62%|██████▏   | 176/285 [51:31<31:41, 17.45s/it] 62%|██████▏   | 177/285 [51:48<31:35, 17.55s/it] 62%|██████▏   | 178/285 [52:06<31:35, 17.71s/it] 63%|██████▎   | 179/285 [52:24<31:25, 17.79s/it] 63%|██████▎   | 180/285 [52:42<30:55, 17.67s/it] 64%|██████▎   | 181/285 [53:00<30:57, 17.86s/it] 64%|██████▍   | 182/285 [53:18<30:55, 18.02s/it] 64%|██████▍   | 183/285 [53:36<30:09, 17.74s/it] 65%|██████▍   | 184/285 [53:54<30:10, 17.93s/it] 65%|██████▍   | 185/285 [54:11<29:31, 17.72s/it] 65%|██████▌   | 186/285 [54:30<29:44, 18.02s/it] 66%|██████▌   | 187/285 [54:47<28:51, 17.67s/it] 66%|██████▌   | 188/285 [55:05<28:43, 17.77s/it] 66%|██████▋   | 189/285 [55:22<28:09, 17.60s/it] 67%|██████▋   | 190/285 [55:40<27:58, 17.67s/it] 67%|██████▋   | 191/285 [55:57<27:42, 17.69s/it] 67%|██████▋   | 192/285 [56:17<28:18, 18.26s/it] 68%|██████▊   | 193/285 [56:36<28:20, 18.49s/it] 68%|██████▊   | 194/285 [56:55<28:01, 18.48s/it] 68%|██████▊   | 195/285 [57:14<28:00, 18.67s/it] 69%|██████▉   | 196/285 [57:31<26:58, 18.19s/it] 69%|██████▉   | 197/285 [57:47<25:47, 17.59s/it] 69%|██████▉   | 198/285 [58:04<25:10, 17.36s/it] 70%|██████▉   | 199/285 [58:21<24:46, 17.29s/it] 70%|███████   | 200/285 [58:38<24:26, 17.25s/it]                                                  70%|███████   | 200/285 [58:38<24:26, 17.25s/it] 71%|███████   | 201/285 [58:55<24:04, 17.19s/it] 71%|███████   | 202/285 [59:13<24:04, 17.40s/it] 71%|███████   | 203/285 [59:30<23:31, 17.21s/it] 72%|███████▏  | 204/285 [59:49<24:03, 17.82s/it] 72%|███████▏  | 205/285 [1:00:07<23:39, 17.74s/it] 72%|███████▏  | 206/285 [1:00:25<23:32, 17.88s/it] 73%|███████▎  | 207/285 [1:00:45<23:58, 18.44s/it] 73%|███████▎  | 208/285 [1:01:02<23:25, 18.25s/it] 73%|███████▎  | 209/285 [1:01:22<23:31, 18.57s/it] 74%|███████▎  | 210/285 [1:01:39<22:41, 18.15s/it] 74%|███████▍  | 211/285 [1:01:56<22:01, 17.86s/it] 74%|███████▍  | 212/285 [1:02:14<21:46, 17.90s/it] 75%|███████▍  | 213/285 [1:02:31<21:14, 17.70s/it] 75%|███████▌  | 214/285 [1:02:48<20:42, 17.50s/it] 75%|███████▌  | 215/285 [1:03:05<20:15, 17.37s/it] 76%|███████▌  | 216/285 [1:03:24<20:32, 17.86s/it] 76%|███████▌  | 217/285 [1:03:41<19:41, 17.38s/it] 76%|███████▋  | 218/285 [1:03:58<19:20, 17.33s/it] 77%|███████▋  | 219/285 [1:04:15<19:03, 17.33s/it] 77%|███████▋  | 220/285 [1:04:33<19:01, 17.56s/it] 78%|███████▊  | 221/285 [1:04:50<18:33, 17.40s/it] 78%|███████▊  | 222/285 [1:05:07<18:10, 17.31s/it] 78%|███████▊  | 223/285 [1:05:25<18:04, 17.49s/it] 79%|███████▊  | 224/285 [1:05:43<17:49, 17.53s/it] 79%|███████▉  | 225/285 [1:06:00<17:26, 17.44s/it] 79%|███████▉  | 226/285 [1:06:18<17:18, 17.60s/it] 80%|███████▉  | 227/285 [1:06:35<16:54, 17.49s/it] 80%|████████  | 228/285 [1:06:54<16:51, 17.74s/it] 80%|████████  | 229/285 [1:07:14<17:14, 18.47s/it] 81%|████████  | 230/285 [1:07:32<16:46, 18.30s/it] 81%|████████  | 231/285 [1:07:48<16:02, 17.83s/it] 81%|████████▏ | 232/285 [1:08:06<15:48, 17.90s/it] 82%|████████▏ | 233/285 [1:08:23<15:16, 17.63s/it] 82%|████████▏ | 234/285 [1:08:41<14:55, 17.56s/it] 82%|████████▏ | 235/285 [1:08:58<14:32, 17.44s/it] 83%|████████▎ | 236/285 [1:09:14<13:50, 16.94s/it] 83%|████████▎ | 237/285 [1:09:31<13:37, 17.02s/it] 84%|████████▎ | 238/285 [1:09:48<13:16, 16.94s/it] 84%|████████▍ | 239/285 [1:10:07<13:27, 17.55s/it] 84%|████████▍ | 240/285 [1:10:26<13:28, 17.96s/it] 85%|████████▍ | 241/285 [1:10:42<12:53, 17.58s/it] 85%|████████▍ | 242/285 [1:11:00<12:38, 17.63s/it] 85%|████████▌ | 243/285 [1:11:18<12:25, 17.74s/it] 86%|████████▌ | 244/285 [1:11:35<11:54, 17.43s/it] 86%|████████▌ | 245/285 [1:11:52<11:36, 17.40s/it] 86%|████████▋ | 246/285 [1:12:08<11:05, 17.08s/it] 87%|████████▋ | 247/285 [1:12:28<11:18, 17.86s/it] 87%|████████▋ | 248/285 [1:12:45<10:52, 17.63s/it] 87%|████████▋ | 249/285 [1:13:02<10:23, 17.32s/it] 88%|████████▊ | 250/285 [1:13:21<10:28, 17.95s/it] 88%|████████▊ | 251/285 [1:13:39<10:06, 17.85s/it] 88%|████████▊ | 252/285 [1:13:57<09:54, 18.01s/it] 89%|████████▉ | 253/285 [1:14:15<09:32, 17.89s/it] 89%|████████▉ | 254/285 [1:14:33<09:15, 17.92s/it] 89%|████████▉ | 255/285 [1:14:52<09:03, 18.13s/it] 90%|████████▉ | 256/285 [1:15:09<08:36, 17.82s/it] 90%|█████████ | 257/285 [1:15:28<08:33, 18.33s/it] 91%|█████████ | 258/285 [1:15:46<08:10, 18.15s/it] 91%|█████████ | 259/285 [1:16:03<07:47, 17.99s/it] 91%|█████████ | 260/285 [1:16:21<07:24, 17.77s/it] 92%|█████████▏| 261/285 [1:16:38<07:00, 17.54s/it] 92%|█████████▏| 262/285 [1:16:56<06:50, 17.83s/it] 92%|█████████▏| 263/285 [1:17:15<06:35, 17.99s/it] 93%|█████████▎| 264/285 [1:17:32<06:14, 17.85s/it] 93%|█████████▎| 265/285 [1:17:49<05:53, 17.68s/it] 93%|█████████▎| 266/285 [1:18:06<05:28, 17.28s/it] 94%|█████████▎| 267/285 [1:18:24<05:13, 17.44s/it] 94%|█████████▍| 268/285 [1:18:41<04:56, 17.41s/it] 94%|█████████▍| 269/285 [1:18:58<04:37, 17.35s/it] 95%|█████████▍| 270/285 [1:19:17<04:26, 17.73s/it] 95%|█████████▌| 271/285 [1:19:34<04:06, 17.59s/it] 95%|█████████▌| 272/285 [1:19:49<03:38, 16.78s/it] 96%|█████████▌| 273/285 [1:20:04<03:16, 16.33s/it] 96%|█████████▌| 274/285 [1:20:22<03:03, 16.69s/it] 96%|█████████▋| 275/285 [1:20:41<02:54, 17.45s/it] 97%|█████████▋| 276/285 [1:20:59<02:38, 17.66s/it] 97%|█████████▋| 277/285 [1:21:16<02:19, 17.39s/it] 98%|█████████▊| 278/285 [1:21:34<02:02, 17.47s/it] 98%|█████████▊| 279/285 [1:21:51<01:45, 17.50s/it] 98%|█████████▊| 280/285 [1:22:10<01:29, 17.91s/it] 99%|█████████▊| 281/285 [1:22:28<01:11, 17.82s/it] 99%|█████████▉| 282/285 [1:22:46<00:53, 17.87s/it] 99%|█████████▉| 283/285 [1:23:05<00:36, 18.33s/it]100%|█████████▉| 284/285 [1:23:22<00:17, 18.00s/it]100%|██████████| 285/285 [1:23:41<00:00, 18.25s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 285/285 [1:23:41<00:00, 18.25s/it]100%|██████████| 285/285 [1:23:41<00:00, 17.62s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'loss': 0.3809, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 5021.5018, 'train_samples_per_second': 3.638, 'train_steps_per_second': 0.057, 'train_loss': 0.27523944252415705, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:12<08:02, 12.37s/it]  5%|▌         | 2/40 [00:25<08:09, 12.89s/it]  8%|▊         | 3/40 [00:37<07:40, 12.45s/it] 10%|█         | 4/40 [00:50<07:37, 12.70s/it] 12%|█▎        | 5/40 [01:03<07:24, 12.69s/it] 15%|█▌        | 6/40 [01:15<07:01, 12.39s/it] 18%|█▊        | 7/40 [01:27<06:44, 12.26s/it] 20%|██        | 8/40 [01:40<06:43, 12.61s/it] 22%|██▎       | 9/40 [01:54<06:43, 13.02s/it] 25%|██▌       | 10/40 [02:06<06:22, 12.75s/it] 28%|██▊       | 11/40 [02:18<06:05, 12.60s/it] 30%|███       | 12/40 [02:30<05:45, 12.34s/it] 32%|███▎      | 13/40 [02:43<05:34, 12.39s/it] 35%|███▌      | 14/40 [02:56<05:26, 12.57s/it] 38%|███▊      | 15/40 [03:09<05:19, 12.77s/it] 40%|████      | 16/40 [03:22<05:08, 12.83s/it] 42%|████▎     | 17/40 [03:36<05:02, 13.15s/it] 45%|████▌     | 18/40 [03:49<04:48, 13.12s/it] 48%|████▊     | 19/40 [04:01<04:28, 12.77s/it] 50%|█████     | 20/40 [04:13<04:12, 12.63s/it] 52%|█████▎    | 21/40 [04:25<03:58, 12.54s/it] 55%|█████▌    | 22/40 [04:38<03:44, 12.48s/it] 57%|█████▊    | 23/40 [04:50<03:29, 12.34s/it] 60%|██████    | 24/40 [05:03<03:22, 12.65s/it] 62%|██████▎   | 25/40 [05:18<03:18, 13.26s/it] 65%|██████▌   | 26/40 [05:30<03:03, 13.09s/it] 68%|██████▊   | 27/40 [05:43<02:49, 13.01s/it] 70%|███████   | 28/40 [05:55<02:32, 12.74s/it] 72%|███████▎  | 29/40 [06:08<02:18, 12.63s/it] 75%|███████▌  | 30/40 [06:20<02:06, 12.68s/it] 78%|███████▊  | 31/40 [06:33<01:52, 12.50s/it] 80%|████████  | 32/40 [06:45<01:38, 12.34s/it] 82%|████████▎ | 33/40 [06:59<01:31, 13.02s/it] 85%|████████▌ | 34/40 [07:13<01:19, 13.18s/it] 88%|████████▊ | 35/40 [07:26<01:05, 13.14s/it] 90%|█████████ | 36/40 [07:38<00:51, 12.93s/it] 92%|█████████▎| 37/40 [07:50<00:37, 12.52s/it] 95%|█████████▌| 38/40 [08:02<00:25, 12.58s/it] 98%|█████████▊| 39/40 [08:15<00:12, 12.50s/it]100%|██████████| 40/40 [08:27<00:00, 12.37s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:27<00:00, 12.37s/it]100%|██████████| 40/40 [08:27<00:00, 12.68s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 507.3544, 'train_samples_per_second': 5.134, 'train_steps_per_second': 0.079, 'train_loss': 0.6989149570465087, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:39, 11.78s/it]  5%|▌         | 2/40 [00:25<08:00, 12.65s/it]  8%|▊         | 3/40 [00:36<07:34, 12.28s/it] 10%|█         | 4/40 [00:49<07:33, 12.59s/it] 12%|█▎        | 5/40 [01:02<07:22, 12.64s/it] 15%|█▌        | 6/40 [01:14<06:57, 12.27s/it] 18%|█▊        | 7/40 [01:26<06:40, 12.14s/it] 20%|██        | 8/40 [01:39<06:41, 12.54s/it] 22%|██▎       | 9/40 [01:53<06:41, 12.96s/it] 25%|██▌       | 10/40 [02:05<06:20, 12.67s/it] 28%|██▊       | 11/40 [02:17<06:04, 12.57s/it] 30%|███       | 12/40 [02:29<05:45, 12.34s/it] 32%|███▎      | 13/40 [02:42<05:35, 12.44s/it] 35%|███▌      | 14/40 [02:55<05:26, 12.56s/it] 38%|███▊      | 15/40 [03:08<05:20, 12.81s/it] 40%|████      | 16/40 [03:21<05:08, 12.87s/it] 42%|████▎     | 17/40 [03:35<05:03, 13.18s/it] 45%|████▌     | 18/40 [03:48<04:49, 13.16s/it] 48%|████▊     | 19/40 [04:00<04:29, 12.85s/it] 50%|█████     | 20/40 [04:12<04:13, 12.69s/it] 52%|█████▎    | 21/40 [04:25<03:59, 12.60s/it] 55%|█████▌    | 22/40 [04:37<03:46, 12.57s/it] 57%|█████▊    | 23/40 [04:49<03:30, 12.41s/it] 60%|██████    | 24/40 [05:03<03:23, 12.71s/it] 62%|██████▎   | 25/40 [05:17<03:19, 13.31s/it] 65%|██████▌   | 26/40 [05:30<03:03, 13.11s/it] 68%|██████▊   | 27/40 [05:43<02:49, 13.02s/it] 70%|███████   | 28/40 [05:55<02:33, 12.77s/it] 72%|███████▎  | 29/40 [06:08<02:19, 12.68s/it] 75%|███████▌  | 30/40 [06:20<02:07, 12.70s/it] 78%|███████▊  | 31/40 [06:33<01:52, 12.55s/it] 80%|████████  | 32/40 [06:45<01:39, 12.43s/it] 82%|████████▎ | 33/40 [06:59<01:30, 13.00s/it] 85%|████████▌ | 34/40 [07:12<01:18, 13.11s/it] 88%|████████▊ | 35/40 [07:25<01:04, 12.90s/it] 90%|█████████ | 36/40 [07:37<00:51, 12.76s/it] 92%|█████████▎| 37/40 [07:49<00:37, 12.46s/it] 95%|█████████▌| 38/40 [08:02<00:25, 12.57s/it] 98%|█████████▊| 39/40 [08:14<00:12, 12.51s/it]100%|██████████| 40/40 [08:26<00:00, 12.39s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:26<00:00, 12.39s/it]100%|██████████| 40/40 [08:26<00:00, 12.67s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 506.7807, 'train_samples_per_second': 5.14, 'train_steps_per_second': 0.079, 'train_loss': 0.7158811569213868, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:15, 17.33s/it]  5%|▌         | 2/40 [00:37<11:55, 18.84s/it]  8%|▊         | 3/40 [00:54<11:12, 18.16s/it] 10%|█         | 4/40 [01:13<11:09, 18.61s/it] 12%|█▎        | 5/40 [01:32<10:54, 18.69s/it] 15%|█▌        | 6/40 [01:50<10:19, 18.22s/it] 18%|█▊        | 7/40 [02:07<09:56, 18.07s/it] 20%|██        | 8/40 [02:27<09:57, 18.68s/it] 22%|██▎       | 9/40 [02:48<09:59, 19.34s/it] 25%|██▌       | 10/40 [03:06<09:28, 18.96s/it] 28%|██▊       | 11/40 [03:23<08:55, 18.45s/it] 30%|███       | 12/40 [03:40<08:21, 17.91s/it] 32%|███▎      | 13/40 [03:58<08:03, 17.90s/it] 35%|███▌      | 14/40 [04:17<07:55, 18.28s/it] 38%|███▊      | 15/40 [04:37<07:49, 18.77s/it] 40%|████      | 16/40 [04:57<07:35, 19.00s/it] 42%|████▎     | 17/40 [05:17<07:28, 19.49s/it] 45%|████▌     | 18/40 [05:37<07:09, 19.51s/it] 48%|████▊     | 19/40 [05:55<06:41, 19.11s/it] 50%|█████     | 20/40 [06:13<06:17, 18.88s/it] 52%|█████▎    | 21/40 [06:32<05:59, 18.90s/it] 55%|█████▌    | 22/40 [06:51<05:37, 18.76s/it] 57%|█████▊    | 23/40 [07:09<05:14, 18.47s/it] 60%|██████    | 24/40 [07:28<05:01, 18.86s/it] 62%|██████▎   | 25/40 [07:50<04:55, 19.71s/it] 65%|██████▌   | 26/40 [08:08<04:29, 19.26s/it] 68%|██████▊   | 27/40 [08:27<04:09, 19.20s/it] 70%|███████   | 28/40 [08:45<03:46, 18.85s/it] 72%|███████▎  | 29/40 [09:04<03:26, 18.78s/it] 75%|███████▌  | 30/40 [09:23<03:08, 18.87s/it] 78%|███████▊  | 31/40 [09:41<02:47, 18.66s/it] 80%|████████  | 32/40 [09:59<02:27, 18.49s/it] 82%|████████▎ | 33/40 [10:20<02:15, 19.31s/it] 85%|████████▌ | 34/40 [10:41<01:57, 19.54s/it] 88%|████████▊ | 35/40 [10:59<01:36, 19.35s/it] 90%|█████████ | 36/40 [11:18<01:16, 19.19s/it] 92%|█████████▎| 37/40 [11:35<00:55, 18.50s/it] 95%|█████████▌| 38/40 [11:54<00:37, 18.72s/it] 98%|█████████▊| 39/40 [12:13<00:18, 18.61s/it]100%|██████████| 40/40 [12:31<00:00, 18.40s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:31<00:00, 18.40s/it]100%|██████████| 40/40 [12:31<00:00, 18.78s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 751.1554, 'train_samples_per_second': 3.468, 'train_steps_per_second': 0.053, 'train_loss': 0.6954928874969483, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:18<12:01, 18.50s/it]  5%|▌         | 2/40 [00:39<12:37, 19.94s/it]  8%|▊         | 3/40 [00:57<11:53, 19.28s/it] 10%|█         | 4/40 [01:18<11:49, 19.70s/it] 12%|█▎        | 5/40 [01:38<11:34, 19.83s/it] 15%|█▌        | 6/40 [01:56<10:58, 19.36s/it] 18%|█▊        | 7/40 [02:15<10:33, 19.20s/it] 20%|██        | 8/40 [02:36<10:34, 19.81s/it] 22%|██▎       | 9/40 [02:58<10:34, 20.48s/it] 25%|██▌       | 10/40 [03:17<10:03, 20.10s/it] 28%|██▊       | 11/40 [03:37<09:35, 19.85s/it] 30%|███       | 12/40 [03:55<09:02, 19.39s/it] 32%|███▎      | 13/40 [04:15<08:45, 19.46s/it] 35%|███▌      | 14/40 [04:35<08:32, 19.71s/it] 38%|███▊      | 15/40 [04:56<08:22, 20.11s/it] 40%|████      | 16/40 [05:16<08:03, 20.16s/it] 42%|████▎     | 17/40 [05:38<07:54, 20.63s/it] 45%|████▌     | 18/40 [05:58<07:32, 20.56s/it] 48%|████▊     | 19/40 [06:18<07:02, 20.14s/it] 50%|█████     | 20/40 [06:37<06:37, 19.86s/it] 52%|█████▎    | 21/40 [06:57<06:16, 19.82s/it] 55%|█████▌    | 22/40 [07:16<05:54, 19.69s/it] 57%|█████▊    | 23/40 [07:35<05:29, 19.39s/it] 60%|██████    | 24/40 [07:56<05:18, 19.89s/it] 62%|██████▎   | 25/40 [08:19<05:13, 20.90s/it] 65%|██████▌   | 26/40 [08:39<04:47, 20.53s/it] 68%|██████▊   | 27/40 [08:58<04:24, 20.34s/it] 70%|███████   | 28/40 [09:17<03:59, 19.93s/it] 72%|███████▎  | 29/40 [09:37<03:38, 19.83s/it] 75%|███████▌  | 30/40 [09:57<03:18, 19.86s/it] 78%|███████▊  | 31/40 [10:16<02:56, 19.63s/it] 80%|████████  | 32/40 [10:35<02:35, 19.46s/it] 82%|████████▎ | 33/40 [10:58<02:22, 20.40s/it] 85%|████████▌ | 34/40 [11:19<02:04, 20.68s/it] 88%|████████▊ | 35/40 [11:39<01:42, 20.49s/it] 90%|█████████ | 36/40 [11:59<01:21, 20.26s/it] 92%|█████████▎| 37/40 [12:17<00:58, 19.61s/it] 95%|█████████▌| 38/40 [12:37<00:39, 19.74s/it] 98%|█████████▊| 39/40 [12:56<00:19, 19.60s/it]100%|██████████| 40/40 [13:15<00:00, 19.35s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [13:15<00:00, 19.35s/it]100%|██████████| 40/40 [13:15<00:00, 19.89s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-2_full_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 795.5445, 'train_samples_per_second': 3.274, 'train_steps_per_second': 0.05, 'train_loss': 0.7516088485717773, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:24, 11.38s/it]  5%|▌         | 2/40 [00:23<07:20, 11.60s/it]  8%|▊         | 3/40 [00:34<07:01, 11.38s/it] 10%|█         | 4/40 [00:45<06:54, 11.51s/it] 12%|█▎        | 5/40 [00:57<06:44, 11.55s/it] 15%|█▌        | 6/40 [01:09<06:36, 11.67s/it] 18%|█▊        | 7/40 [01:20<06:19, 11.50s/it] 20%|██        | 8/40 [01:32<06:09, 11.54s/it] 22%|██▎       | 9/40 [01:52<07:18, 14.15s/it] 25%|██▌       | 10/40 [02:03<06:36, 13.23s/it] 28%|██▊       | 11/40 [02:14<06:03, 12.53s/it] 30%|███       | 12/40 [02:25<05:44, 12.29s/it] 32%|███▎      | 13/40 [02:37<05:21, 11.91s/it] 35%|███▌      | 14/40 [02:48<05:06, 11.80s/it] 38%|███▊      | 15/40 [02:59<04:49, 11.58s/it] 40%|████      | 16/40 [03:11<04:40, 11.68s/it] 42%|████▎     | 17/40 [03:32<05:35, 14.59s/it] 45%|████▌     | 18/40 [03:44<04:59, 13.62s/it] 48%|████▊     | 19/40 [03:56<04:34, 13.05s/it] 50%|█████     | 20/40 [04:08<04:15, 12.76s/it] 52%|█████▎    | 21/40 [04:18<03:50, 12.12s/it] 55%|█████▌    | 22/40 [04:30<03:38, 12.12s/it] 57%|█████▊    | 23/40 [04:42<03:21, 11.84s/it] 60%|██████    | 24/40 [04:53<03:05, 11.58s/it] 62%|██████▎   | 25/40 [05:12<03:29, 13.98s/it] 65%|██████▌   | 26/40 [05:23<03:03, 13.12s/it] 68%|██████▊   | 27/40 [05:35<02:45, 12.75s/it] 70%|███████   | 28/40 [05:46<02:26, 12.24s/it] 72%|███████▎  | 29/40 [05:58<02:13, 12.16s/it] 75%|███████▌  | 30/40 [06:10<01:59, 11.99s/it] 78%|███████▊  | 31/40 [06:21<01:45, 11.70s/it] 80%|████████  | 32/40 [06:33<01:35, 11.92s/it] 82%|████████▎ | 33/40 [06:53<01:39, 14.26s/it] 85%|████████▌ | 34/40 [07:04<01:20, 13.36s/it] 88%|████████▊ | 35/40 [07:16<01:04, 12.86s/it] 90%|█████████ | 36/40 [07:28<00:50, 12.57s/it] 92%|█████████▎| 37/40 [07:39<00:36, 12.13s/it] 95%|█████████▌| 38/40 [07:50<00:23, 11.99s/it] 98%|█████████▊| 39/40 [08:03<00:12, 12.13s/it]100%|██████████| 40/40 [08:14<00:00, 11.88s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:14<00:00, 11.88s/it]100%|██████████| 40/40 [08:14<00:00, 12.37s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT_full_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 494.7532, 'train_samples_per_second': 5.669, 'train_steps_per_second': 0.081, 'train_loss': 0.7465334415435791, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:24, 11.39s/it]  5%|▌         | 2/40 [00:22<07:16, 11.49s/it]  8%|▊         | 3/40 [00:34<07:00, 11.36s/it] 10%|█         | 4/40 [00:46<06:56, 11.58s/it] 12%|█▎        | 5/40 [00:57<06:46, 11.62s/it] 15%|█▌        | 6/40 [01:09<06:40, 11.77s/it] 18%|█▊        | 7/40 [01:21<06:22, 11.60s/it] 20%|██        | 8/40 [01:32<06:13, 11.68s/it] 22%|██▎       | 9/40 [01:52<07:23, 14.30s/it] 25%|██▌       | 10/40 [02:04<06:41, 13.37s/it] 28%|██▊       | 11/40 [02:15<06:11, 12.81s/it] 30%|███       | 12/40 [02:27<05:52, 12.57s/it] 32%|███▎      | 13/40 [02:39<05:28, 12.18s/it] 35%|███▌      | 14/40 [02:50<05:13, 12.04s/it] 38%|███▊      | 15/40 [03:02<04:56, 11.85s/it] 40%|████      | 16/40 [03:14<04:43, 11.82s/it] 42%|████▎     | 17/40 [03:35<05:37, 14.69s/it] 45%|████▌     | 18/40 [03:46<05:01, 13.69s/it] 48%|████▊     | 19/40 [03:58<04:35, 13.12s/it] 50%|█████     | 20/40 [04:10<04:16, 12.80s/it] 52%|█████▎    | 21/40 [04:21<03:52, 12.23s/it] 55%|█████▌    | 22/40 [04:33<03:39, 12.19s/it] 57%|█████▊    | 23/40 [04:44<03:22, 11.92s/it] 60%|██████    | 24/40 [04:55<03:06, 11.64s/it] 62%|██████▎   | 25/40 [05:15<03:29, 13.99s/it] 65%|██████▌   | 26/40 [05:26<03:04, 13.18s/it] 68%|██████▊   | 27/40 [05:38<02:46, 12.77s/it] 70%|███████   | 28/40 [05:49<02:26, 12.24s/it] 72%|███████▎  | 29/40 [06:01<02:13, 12.18s/it] 75%|███████▌  | 30/40 [06:13<02:00, 12.04s/it] 78%|███████▊  | 31/40 [06:24<01:47, 11.92s/it] 80%|████████  | 32/40 [06:37<01:36, 12.02s/it] 82%|████████▎ | 33/40 [06:56<01:39, 14.26s/it] 85%|████████▌ | 34/40 [07:07<01:20, 13.35s/it] 88%|████████▊ | 35/40 [07:19<01:04, 12.83s/it] 90%|█████████ | 36/40 [07:31<00:50, 12.54s/it] 92%|█████████▎| 37/40 [07:42<00:36, 12.14s/it] 95%|█████████▌| 38/40 [07:54<00:23, 11.99s/it] 98%|█████████▊| 39/40 [08:06<00:12, 12.14s/it]100%|██████████| 40/40 [08:17<00:00, 11.86s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [08:17<00:00, 11.86s/it]100%|██████████| 40/40 [08:17<00:00, 12.45s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_full_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 497.8404, 'train_samples_per_second': 5.634, 'train_steps_per_second': 0.08, 'train_loss': 0.7506145477294922, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:16<11:02, 17.00s/it]  5%|▌         | 2/40 [00:34<10:46, 17.02s/it]  8%|▊         | 3/40 [00:49<10:11, 16.53s/it] 10%|█         | 4/40 [01:07<10:03, 16.75s/it] 12%|█▎        | 5/40 [01:24<09:50, 16.88s/it] 15%|█▌        | 6/40 [01:42<09:47, 17.28s/it] 18%|█▊        | 7/40 [01:58<09:21, 17.02s/it] 20%|██        | 8/40 [02:15<09:05, 17.05s/it] 22%|██▎       | 9/40 [02:44<10:42, 20.73s/it] 25%|██▌       | 10/40 [03:00<09:40, 19.36s/it] 28%|██▊       | 11/40 [03:18<09:00, 18.66s/it] 30%|███       | 12/40 [03:36<08:38, 18.50s/it] 32%|███▎      | 13/40 [03:52<08:01, 17.82s/it] 35%|███▌      | 14/40 [04:09<07:38, 17.64s/it] 38%|███▊      | 15/40 [04:25<07:08, 17.14s/it] 40%|████      | 16/40 [04:43<06:57, 17.38s/it] 42%|████▎     | 17/40 [05:15<08:20, 21.76s/it] 45%|████▌     | 18/40 [05:32<07:26, 20.28s/it] 48%|████▊     | 19/40 [05:49<06:46, 19.35s/it] 50%|█████     | 20/40 [06:07<06:18, 18.92s/it] 52%|█████▎    | 21/40 [06:23<05:41, 17.95s/it] 55%|█████▌    | 22/40 [06:41<05:24, 18.05s/it] 57%|█████▊    | 23/40 [06:57<04:58, 17.56s/it] 60%|██████    | 24/40 [07:13<04:32, 17.02s/it] 62%|██████▎   | 25/40 [07:41<05:06, 20.42s/it] 65%|██████▌   | 26/40 [07:58<04:28, 19.20s/it] 68%|██████▊   | 27/40 [08:15<04:02, 18.67s/it] 70%|███████   | 28/40 [08:31<03:33, 17.82s/it] 72%|███████▎  | 29/40 [08:49<03:17, 17.94s/it] 75%|███████▌  | 30/40 [09:06<02:56, 17.65s/it] 78%|███████▊  | 31/40 [09:22<02:34, 17.14s/it] 80%|████████  | 32/40 [09:41<02:20, 17.56s/it] 82%|████████▎ | 33/40 [10:09<02:25, 20.76s/it] 85%|████████▌ | 34/40 [10:25<01:56, 19.46s/it] 88%|████████▊ | 35/40 [10:42<01:33, 18.75s/it] 90%|█████████ | 36/40 [10:59<01:12, 18.13s/it] 92%|█████████▎| 37/40 [11:15<00:52, 17.39s/it] 95%|█████████▌| 38/40 [11:31<00:34, 17.10s/it] 98%|█████████▊| 39/40 [11:49<00:17, 17.40s/it]100%|██████████| 40/40 [12:05<00:00, 16.94s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:05<00:00, 16.94s/it]100%|██████████| 40/40 [12:05<00:00, 18.14s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-1b_full_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 725.7131, 'train_samples_per_second': 3.865, 'train_steps_per_second': 0.055, 'train_loss': 0.7548487186431885, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:17<11:32, 17.75s/it]  5%|▌         | 2/40 [00:35<11:22, 17.97s/it]  8%|▊         | 3/40 [00:53<10:54, 17.70s/it] 10%|█         | 4/40 [01:11<10:49, 18.05s/it] 12%|█▎        | 5/40 [01:30<10:34, 18.14s/it] 15%|█▌        | 6/40 [01:49<10:27, 18.46s/it] 18%|█▊        | 7/40 [02:07<10:02, 18.27s/it] 20%|██        | 8/40 [02:24<09:40, 18.15s/it] 22%|██▎       | 9/40 [02:55<11:20, 21.94s/it] 25%|██▌       | 10/40 [03:12<10:19, 20.64s/it] 28%|██▊       | 11/40 [03:30<09:32, 19.74s/it] 30%|███       | 12/40 [03:49<09:05, 19.47s/it] 32%|███▎      | 13/40 [04:06<08:26, 18.76s/it] 35%|███▌      | 14/40 [04:24<08:01, 18.51s/it] 38%|███▊      | 15/40 [04:41<07:30, 18.03s/it] 40%|████      | 16/40 [05:00<07:18, 18.25s/it] 42%|████▎     | 17/40 [05:34<08:51, 23.09s/it] 45%|████▌     | 18/40 [05:52<07:51, 21.43s/it] 48%|████▊     | 19/40 [06:10<07:11, 20.54s/it] 50%|█████     | 20/40 [06:29<06:40, 20.02s/it] 52%|█████▎    | 21/40 [06:46<06:01, 19.02s/it] 55%|█████▌    | 22/40 [07:05<05:42, 19.01s/it] 57%|█████▊    | 23/40 [07:22<05:16, 18.61s/it] 60%|██████    | 24/40 [07:39<04:50, 18.15s/it] 62%|██████▎   | 25/40 [08:10<05:28, 21.89s/it] 65%|██████▌   | 26/40 [08:27<04:47, 20.50s/it] 68%|██████▊   | 27/40 [08:46<04:19, 19.96s/it] 70%|███████   | 28/40 [09:03<03:48, 19.04s/it] 72%|███████▎  | 29/40 [09:22<03:29, 19.02s/it] 75%|███████▌  | 30/40 [09:40<03:06, 18.66s/it] 78%|███████▊  | 31/40 [09:57<02:44, 18.32s/it] 80%|████████  | 32/40 [10:17<02:29, 18.63s/it] 82%|████████▎ | 33/40 [10:47<02:35, 22.19s/it] 85%|████████▌ | 34/40 [11:04<02:04, 20.76s/it] 88%|████████▊ | 35/40 [11:23<01:40, 20.00s/it] 90%|█████████ | 36/40 [11:41<01:18, 19.58s/it] 92%|█████████▎| 37/40 [11:59<00:56, 18.97s/it] 95%|█████████▌| 38/40 [12:17<00:37, 18.84s/it] 98%|█████████▊| 39/40 [12:37<00:19, 19.08s/it]100%|██████████| 40/40 [12:54<00:00, 18.54s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [12:54<00:00, 18.54s/it]100%|██████████| 40/40 [12:54<00:00, 19.37s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-2_full_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
{'train_runtime': 774.7917, 'train_samples_per_second': 3.62, 'train_steps_per_second': 0.052, 'train_loss': 0.7796206474304199, 'epoch': 4.91}
