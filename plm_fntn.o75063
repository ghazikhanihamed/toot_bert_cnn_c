Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
/home/h_ghazik/python_venv/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:15,  6.56s/it]  5%|▌         | 2/40 [00:12<03:49,  6.03s/it]  8%|▊         | 3/40 [00:18<03:40,  5.95s/it] 10%|█         | 4/40 [00:24<03:34,  5.96s/it] 12%|█▎        | 5/40 [00:30<03:28,  5.96s/it] 15%|█▌        | 6/40 [00:35<03:21,  5.92s/it] 18%|█▊        | 7/40 [00:41<03:13,  5.85s/it] 20%|██        | 8/40 [00:47<03:07,  5.85s/it] 22%|██▎       | 9/40 [00:57<03:43,  7.20s/it] 25%|██▌       | 10/40 [01:03<03:27,  6.92s/it] 28%|██▊       | 11/40 [01:09<03:11,  6.62s/it] 30%|███       | 12/40 [01:15<03:01,  6.46s/it] 32%|███▎      | 13/40 [01:21<02:50,  6.32s/it] 35%|███▌      | 14/40 [01:27<02:39,  6.14s/it] 38%|███▊      | 15/40 [01:33<02:28,  5.95s/it] 40%|████      | 16/40 [01:39<02:22,  5.94s/it] 42%|████▎     | 17/40 [01:49<02:46,  7.24s/it] 45%|████▌     | 18/40 [01:55<02:30,  6.85s/it] 48%|████▊     | 19/40 [02:00<02:15,  6.46s/it] 50%|█████     | 20/40 [02:06<02:05,  6.28s/it] 52%|█████▎    | 21/40 [02:12<01:55,  6.07s/it] 55%|█████▌    | 22/40 [02:18<01:47,  6.00s/it] 57%|█████▊    | 23/40 [02:23<01:39,  5.84s/it] 60%|██████    | 24/40 [02:29<01:32,  5.81s/it] 62%|██████▎   | 25/40 [02:39<01:46,  7.10s/it] 65%|██████▌   | 26/40 [02:44<01:33,  6.65s/it] 68%|██████▊   | 27/40 [02:50<01:23,  6.42s/it] 70%|███████   | 28/40 [02:56<01:14,  6.18s/it] 72%|███████▎  | 29/40 [03:02<01:06,  6.06s/it] 75%|███████▌  | 30/40 [03:08<00:59,  5.99s/it] 78%|███████▊  | 31/40 [03:13<00:52,  5.80s/it] 80%|████████  | 32/40 [03:19<00:46,  5.85s/it] 82%|████████▎ | 33/40 [03:29<00:49,  7.03s/it] 85%|████████▌ | 34/40 [03:35<00:40,  6.73s/it] 88%|████████▊ | 35/40 [03:41<00:32,  6.53s/it] 90%|█████████ | 36/40 [03:47<00:25,  6.45s/it] 92%|█████████▎| 37/40 [03:53<00:18,  6.33s/it] 95%|█████████▌| 38/40 [03:59<00:12,  6.25s/it] 98%|█████████▊| 39/40 [04:05<00:06,  6.18s/it]100%|██████████| 40/40 [04:11<00:00,  6.10s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:11<00:00,  6.10s/it]100%|██████████| 40/40 [04:11<00:00,  6.29s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 251.6233, 'train_samples_per_second': 11.148, 'train_steps_per_second': 0.159, 'train_loss': 0.7462671279907227, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:59,  6.15s/it]  5%|▌         | 2/40 [00:12<03:47,  5.99s/it]  8%|▊         | 3/40 [00:18<03:44,  6.06s/it] 10%|█         | 4/40 [00:24<03:45,  6.26s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.34s/it] 15%|█▌        | 6/40 [00:37<03:33,  6.27s/it] 18%|█▊        | 7/40 [00:43<03:22,  6.14s/it] 20%|██        | 8/40 [00:49<03:15,  6.10s/it] 22%|██▎       | 9/40 [00:59<03:49,  7.39s/it] 25%|██▌       | 10/40 [01:05<03:31,  7.06s/it] 28%|██▊       | 11/40 [01:11<03:15,  6.74s/it] 30%|███       | 12/40 [01:17<03:01,  6.47s/it] 32%|███▎      | 13/40 [01:23<02:50,  6.33s/it] 35%|███▌      | 14/40 [01:29<02:42,  6.25s/it] 38%|███▊      | 15/40 [01:35<02:32,  6.09s/it] 40%|████      | 16/40 [01:41<02:26,  6.11s/it] 42%|████▎     | 17/40 [01:52<02:52,  7.49s/it] 45%|████▌     | 18/40 [01:58<02:35,  7.08s/it] 48%|████▊     | 19/40 [02:04<02:21,  6.73s/it] 50%|█████     | 20/40 [02:10<02:11,  6.58s/it] 52%|█████▎    | 21/40 [02:16<02:01,  6.38s/it] 55%|█████▌    | 22/40 [02:22<01:52,  6.25s/it] 57%|█████▊    | 23/40 [02:28<01:43,  6.07s/it] 60%|██████    | 24/40 [02:33<01:35,  5.98s/it] 62%|██████▎   | 25/40 [02:44<01:48,  7.25s/it] 65%|██████▌   | 26/40 [02:49<01:34,  6.77s/it] 68%|██████▊   | 27/40 [02:55<01:24,  6.52s/it] 70%|███████   | 28/40 [03:01<01:15,  6.27s/it] 72%|███████▎  | 29/40 [03:07<01:07,  6.16s/it] 75%|███████▌  | 30/40 [03:13<01:00,  6.08s/it] 78%|███████▊  | 31/40 [03:18<00:52,  5.89s/it] 80%|████████  | 32/40 [03:24<00:47,  5.93s/it] 82%|████████▎ | 33/40 [03:34<00:49,  7.12s/it] 85%|████████▌ | 34/40 [03:40<00:40,  6.80s/it] 88%|████████▊ | 35/40 [03:46<00:32,  6.48s/it] 90%|█████████ | 36/40 [03:51<00:25,  6.25s/it] 92%|█████████▎| 37/40 [03:57<00:18,  6.14s/it] 95%|█████████▌| 38/40 [04:03<00:12,  6.03s/it] 98%|█████████▊| 39/40 [04:09<00:05,  5.93s/it]100%|██████████| 40/40 [04:14<00:00,  5.84s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:14<00:00,  5.84s/it]100%|██████████| 40/40 [04:14<00:00,  6.37s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 254.9696, 'train_samples_per_second': 11.001, 'train_steps_per_second': 0.157, 'train_loss': 0.7508064270019531, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:23,  8.29s/it]  5%|▌         | 2/40 [00:16<05:09,  8.15s/it]  8%|▊         | 3/40 [00:24<05:05,  8.25s/it] 10%|█         | 4/40 [00:33<05:06,  8.52s/it] 12%|█▎        | 5/40 [00:42<05:01,  8.63s/it] 15%|█▌        | 6/40 [00:51<04:53,  8.62s/it] 18%|█▊        | 7/40 [00:59<04:43,  8.59s/it] 20%|██        | 8/40 [01:08<04:34,  8.56s/it] 22%|██▎       | 9/40 [01:22<05:24, 10.47s/it] 25%|██▌       | 10/40 [01:32<05:02, 10.09s/it] 28%|██▊       | 11/40 [01:40<04:38,  9.62s/it] 30%|███       | 12/40 [01:49<04:19,  9.26s/it] 32%|███▎      | 13/40 [01:57<04:06,  9.14s/it] 35%|███▌      | 14/40 [02:06<03:53,  8.97s/it] 38%|███▊      | 15/40 [02:14<03:40,  8.80s/it] 40%|████      | 16/40 [02:23<03:32,  8.85s/it] 42%|████▎     | 17/40 [02:39<04:10, 10.88s/it] 45%|████▌     | 18/40 [02:48<03:45, 10.23s/it] 48%|████▊     | 19/40 [02:56<03:23,  9.69s/it] 50%|█████     | 20/40 [03:05<03:07,  9.38s/it] 52%|█████▎    | 21/40 [03:13<02:51,  9.05s/it] 55%|█████▌    | 22/40 [03:22<02:40,  8.91s/it] 57%|█████▊    | 23/40 [03:30<02:27,  8.67s/it] 60%|██████    | 24/40 [03:38<02:18,  8.64s/it] 62%|██████▎   | 25/40 [03:53<02:38, 10.57s/it] 65%|██████▌   | 26/40 [04:02<02:18,  9.89s/it] 68%|██████▊   | 27/40 [04:11<02:05,  9.69s/it] 70%|███████   | 28/40 [04:19<01:51,  9.26s/it] 72%|███████▎  | 29/40 [04:28<01:39,  9.09s/it] 75%|███████▌  | 30/40 [04:37<01:29,  8.97s/it] 78%|███████▊  | 31/40 [04:44<01:17,  8.62s/it] 80%|████████  | 32/40 [04:53<01:09,  8.69s/it] 82%|████████▎ | 33/40 [05:08<01:12, 10.42s/it] 85%|████████▌ | 34/40 [05:16<00:59,  9.93s/it] 88%|████████▊ | 35/40 [05:25<00:47,  9.44s/it] 90%|█████████ | 36/40 [05:33<00:36,  9.14s/it] 92%|█████████▎| 37/40 [05:42<00:27,  9.01s/it] 95%|█████████▌| 38/40 [05:50<00:17,  8.86s/it] 98%|█████████▊| 39/40 [05:59<00:08,  8.72s/it]100%|██████████| 40/40 [06:07<00:00,  8.56s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:07<00:00,  8.56s/it]100%|██████████| 40/40 [06:07<00:00,  9.19s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 367.431, 'train_samples_per_second': 7.634, 'train_steps_per_second': 0.109, 'train_loss': 0.7579294204711914, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:59,  9.21s/it]  5%|▌         | 2/40 [00:18<05:51,  9.24s/it]  8%|▊         | 3/40 [00:28<05:57,  9.66s/it] 10%|█         | 4/40 [00:38<05:50,  9.75s/it] 12%|█▎        | 5/40 [00:48<05:40,  9.72s/it] 15%|█▌        | 6/40 [00:57<05:28,  9.65s/it] 18%|█▊        | 7/40 [01:07<05:15,  9.56s/it] 20%|██        | 8/40 [01:16<05:06,  9.58s/it] 22%|██▎       | 9/40 [01:32<06:00, 11.63s/it] 25%|██▌       | 10/40 [01:42<05:32, 11.10s/it] 28%|██▊       | 11/40 [01:52<05:06, 10.58s/it] 30%|███       | 12/40 [02:01<04:46, 10.22s/it] 32%|███▎      | 13/40 [02:11<04:31, 10.07s/it] 35%|███▌      | 14/40 [02:20<04:16,  9.88s/it] 38%|███▊      | 15/40 [02:29<04:00,  9.63s/it] 40%|████      | 16/40 [02:39<03:50,  9.59s/it] 42%|████▎     | 17/40 [02:55<04:27, 11.63s/it] 45%|████▌     | 18/40 [03:05<04:01, 10.97s/it] 48%|████▊     | 19/40 [03:14<03:37, 10.37s/it] 50%|█████     | 20/40 [03:23<03:21, 10.06s/it] 52%|█████▎    | 21/40 [03:32<03:04,  9.73s/it] 55%|█████▌    | 22/40 [03:41<02:52,  9.61s/it] 57%|█████▊    | 23/40 [03:50<02:39,  9.35s/it] 60%|██████    | 24/40 [03:59<02:28,  9.31s/it] 62%|██████▎   | 25/40 [04:15<02:50, 11.38s/it] 65%|██████▌   | 26/40 [04:24<02:29, 10.65s/it] 68%|██████▊   | 27/40 [04:34<02:13, 10.29s/it] 70%|███████   | 28/40 [04:43<01:58,  9.88s/it] 72%|███████▎  | 29/40 [04:52<01:47,  9.73s/it] 75%|███████▌  | 30/40 [05:01<01:35,  9.60s/it] 78%|███████▊  | 31/40 [05:10<01:23,  9.26s/it] 80%|████████  | 32/40 [05:19<01:14,  9.36s/it] 82%|████████▎ | 33/40 [05:35<01:18, 11.22s/it] 85%|████████▌ | 34/40 [05:44<01:04, 10.70s/it] 88%|████████▊ | 35/40 [05:54<00:51, 10.21s/it] 90%|█████████ | 36/40 [06:03<00:39,  9.85s/it] 92%|█████████▎| 37/40 [06:12<00:29,  9.69s/it] 95%|█████████▌| 38/40 [06:21<00:19,  9.52s/it] 98%|█████████▊| 39/40 [06:30<00:09,  9.36s/it]100%|██████████| 40/40 [06:39<00:00,  9.21s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:39<00:00,  9.21s/it]100%|██████████| 40/40 [06:39<00:00,  9.98s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 399.3026, 'train_samples_per_second': 7.025, 'train_steps_per_second': 0.1, 'train_loss': 0.7833407402038575, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:47,  5.83s/it]  5%|▌         | 2/40 [00:11<03:39,  5.78s/it]  8%|▊         | 3/40 [00:17<03:35,  5.84s/it] 10%|█         | 4/40 [00:23<03:28,  5.80s/it] 12%|█▎        | 5/40 [00:29<03:29,  5.97s/it] 15%|█▌        | 6/40 [00:35<03:20,  5.90s/it] 18%|█▊        | 7/40 [00:41<03:13,  5.86s/it] 20%|██        | 8/40 [00:46<03:07,  5.87s/it] 22%|██▎       | 9/40 [00:56<03:40,  7.12s/it] 25%|██▌       | 10/40 [01:02<03:22,  6.74s/it] 28%|██▊       | 11/40 [01:08<03:05,  6.38s/it] 30%|███       | 12/40 [01:13<02:53,  6.18s/it] 32%|███▎      | 13/40 [01:19<02:43,  6.06s/it] 35%|███▌      | 14/40 [01:25<02:37,  6.05s/it] 38%|███▊      | 15/40 [01:31<02:27,  5.91s/it] 40%|████      | 16/40 [01:37<02:23,  5.96s/it] 42%|████▎     | 17/40 [01:47<02:48,  7.32s/it] 45%|████▌     | 18/40 [01:53<02:30,  6.83s/it] 48%|████▊     | 19/40 [01:59<02:16,  6.49s/it] 50%|█████     | 20/40 [02:05<02:05,  6.29s/it] 52%|█████▎    | 21/40 [02:10<01:56,  6.14s/it] 55%|█████▌    | 22/40 [02:16<01:49,  6.09s/it] 57%|█████▊    | 23/40 [02:22<01:41,  5.99s/it] 60%|██████    | 24/40 [02:28<01:34,  5.89s/it] 62%|██████▎   | 25/40 [02:39<01:51,  7.41s/it] 65%|██████▌   | 26/40 [02:45<01:38,  7.00s/it] 68%|██████▊   | 27/40 [02:51<01:27,  6.70s/it] 70%|███████   | 28/40 [02:57<01:18,  6.57s/it] 72%|███████▎  | 29/40 [03:03<01:10,  6.40s/it] 75%|███████▌  | 30/40 [03:09<01:03,  6.32s/it] 78%|███████▊  | 31/40 [03:15<00:55,  6.13s/it] 80%|████████  | 32/40 [03:21<00:48,  6.11s/it] 82%|████████▎ | 33/40 [03:31<00:51,  7.37s/it] 85%|████████▌ | 34/40 [03:38<00:42,  7.05s/it] 88%|████████▊ | 35/40 [03:44<00:33,  6.73s/it] 90%|█████████ | 36/40 [03:49<00:25,  6.48s/it] 92%|█████████▎| 37/40 [03:55<00:18,  6.32s/it] 95%|█████████▌| 38/40 [04:01<00:12,  6.15s/it] 98%|█████████▊| 39/40 [04:07<00:06,  6.15s/it]100%|██████████| 40/40 [04:13<00:00,  6.11s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:13<00:00,  6.11s/it]100%|██████████| 40/40 [04:13<00:00,  6.35s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 253.8011, 'train_samples_per_second': 11.052, 'train_steps_per_second': 0.158, 'train_loss': 0.745904541015625, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:46,  5.81s/it]  5%|▌         | 2/40 [00:11<03:42,  5.85s/it]  8%|▊         | 3/40 [00:17<03:39,  5.95s/it] 10%|█         | 4/40 [00:23<03:33,  5.93s/it] 12%|█▎        | 5/40 [00:30<03:34,  6.13s/it] 15%|█▌        | 6/40 [00:36<03:27,  6.11s/it] 18%|█▊        | 7/40 [00:42<03:20,  6.07s/it] 20%|██        | 8/40 [00:48<03:13,  6.06s/it] 22%|██▎       | 9/40 [00:58<03:48,  7.38s/it] 25%|██▌       | 10/40 [01:04<03:29,  7.00s/it] 28%|██▊       | 11/40 [01:10<03:12,  6.64s/it] 30%|███       | 12/40 [01:16<02:59,  6.41s/it] 32%|███▎      | 13/40 [01:22<02:51,  6.34s/it] 35%|███▌      | 14/40 [01:28<02:42,  6.26s/it] 38%|███▊      | 15/40 [01:34<02:33,  6.16s/it] 40%|████      | 16/40 [01:40<02:29,  6.23s/it] 42%|████▎     | 17/40 [01:51<02:54,  7.58s/it] 45%|████▌     | 18/40 [01:57<02:34,  7.03s/it] 48%|████▊     | 19/40 [02:03<02:19,  6.64s/it] 50%|█████     | 20/40 [02:08<02:07,  6.38s/it] 52%|█████▎    | 21/40 [02:14<01:58,  6.21s/it] 55%|█████▌    | 22/40 [02:20<01:51,  6.19s/it] 57%|█████▊    | 23/40 [02:26<01:44,  6.15s/it] 60%|██████    | 24/40 [02:32<01:36,  6.00s/it] 62%|██████▎   | 25/40 [02:43<01:50,  7.36s/it] 65%|██████▌   | 26/40 [02:48<01:36,  6.89s/it] 68%|██████▊   | 27/40 [02:54<01:24,  6.50s/it] 70%|███████   | 28/40 [03:00<01:17,  6.48s/it] 72%|███████▎  | 29/40 [03:07<01:09,  6.36s/it] 75%|███████▌  | 30/40 [03:13<01:03,  6.34s/it] 78%|███████▊  | 31/40 [03:19<00:55,  6.20s/it] 80%|████████  | 32/40 [03:25<00:49,  6.19s/it] 82%|████████▎ | 33/40 [03:35<00:52,  7.47s/it] 85%|████████▌ | 34/40 [03:42<00:42,  7.14s/it] 88%|████████▊ | 35/40 [03:48<00:33,  6.78s/it] 90%|█████████ | 36/40 [03:54<00:26,  6.52s/it] 92%|█████████▎| 37/40 [03:59<00:19,  6.34s/it] 95%|█████████▌| 38/40 [04:05<00:12,  6.16s/it] 98%|█████████▊| 39/40 [04:11<00:06,  6.16s/it]100%|██████████| 40/40 [04:17<00:00,  6.09s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:17<00:00,  6.09s/it]100%|██████████| 40/40 [04:17<00:00,  6.44s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 257.7654, 'train_samples_per_second': 10.882, 'train_steps_per_second': 0.155, 'train_loss': 0.750439453125, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:31,  8.49s/it]  5%|▌         | 2/40 [00:17<05:23,  8.51s/it]  8%|▊         | 3/40 [00:25<05:18,  8.60s/it] 10%|█         | 4/40 [00:34<05:10,  8.64s/it] 12%|█▎        | 5/40 [00:43<05:11,  8.89s/it] 15%|█▌        | 6/40 [00:52<04:58,  8.78s/it] 18%|█▊        | 7/40 [01:00<04:45,  8.64s/it] 20%|██        | 8/40 [01:09<04:35,  8.61s/it] 22%|██▎       | 9/40 [01:23<05:19, 10.31s/it] 25%|██▌       | 10/40 [01:31<04:52,  9.76s/it] 28%|██▊       | 11/40 [01:39<04:28,  9.26s/it] 30%|███       | 12/40 [01:48<04:10,  8.94s/it] 32%|███▎      | 13/40 [01:56<03:57,  8.78s/it] 35%|███▌      | 14/40 [02:05<03:48,  8.79s/it] 38%|███▊      | 15/40 [02:13<03:33,  8.53s/it] 40%|████      | 16/40 [02:21<03:25,  8.57s/it] 42%|████▎     | 17/40 [02:37<04:02, 10.53s/it] 45%|████▌     | 18/40 [02:45<03:35,  9.82s/it] 48%|████▊     | 19/40 [02:53<03:16,  9.36s/it] 50%|█████     | 20/40 [03:01<03:00,  9.04s/it] 52%|█████▎    | 21/40 [03:10<02:49,  8.92s/it] 55%|█████▌    | 22/40 [03:19<02:39,  8.86s/it] 57%|█████▊    | 23/40 [03:27<02:28,  8.71s/it] 60%|██████    | 24/40 [03:35<02:15,  8.46s/it] 62%|██████▎   | 25/40 [03:50<02:37, 10.52s/it] 65%|██████▌   | 26/40 [03:59<02:18,  9.88s/it] 68%|██████▊   | 27/40 [04:06<02:00,  9.28s/it] 70%|███████   | 28/40 [04:15<01:48,  9.06s/it] 72%|███████▎  | 29/40 [04:23<01:37,  8.86s/it] 75%|███████▌  | 30/40 [04:32<01:27,  8.78s/it] 78%|███████▊  | 31/40 [04:40<01:16,  8.49s/it] 80%|████████  | 32/40 [04:48<01:08,  8.51s/it] 82%|████████▎ | 33/40 [05:03<01:11, 10.23s/it] 85%|████████▌ | 34/40 [05:12<00:59,  9.85s/it] 88%|████████▊ | 35/40 [05:20<00:46,  9.37s/it] 90%|█████████ | 36/40 [05:28<00:36,  9.01s/it] 92%|█████████▎| 37/40 [05:36<00:26,  8.84s/it] 95%|█████████▌| 38/40 [05:44<00:17,  8.59s/it] 98%|█████████▊| 39/40 [05:53<00:08,  8.71s/it]100%|██████████| 40/40 [06:02<00:00,  8.62s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:02<00:00,  8.62s/it]100%|██████████| 40/40 [06:02<00:00,  9.06s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 362.3497, 'train_samples_per_second': 7.741, 'train_steps_per_second': 0.11, 'train_loss': 0.757661247253418, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:49,  8.97s/it]  5%|▌         | 2/40 [00:17<05:41,  8.99s/it]  8%|▊         | 3/40 [00:27<05:38,  9.14s/it] 10%|█         | 4/40 [00:36<05:28,  9.13s/it] 12%|█▎        | 5/40 [00:46<05:29,  9.42s/it] 15%|█▌        | 6/40 [00:55<05:17,  9.34s/it] 18%|█▊        | 7/40 [01:04<05:04,  9.23s/it] 20%|██        | 8/40 [01:13<04:55,  9.25s/it] 22%|██▎       | 9/40 [01:29<05:53, 11.41s/it] 25%|██▌       | 10/40 [01:39<05:27, 10.90s/it] 28%|██▊       | 11/40 [01:48<05:01, 10.39s/it] 30%|███       | 12/40 [01:58<04:42, 10.08s/it] 32%|███▎      | 13/40 [02:07<04:28,  9.93s/it] 35%|███▌      | 14/40 [02:17<04:17,  9.89s/it] 38%|███▊      | 15/40 [02:27<04:02,  9.71s/it] 40%|████      | 16/40 [02:36<03:54,  9.76s/it] 42%|████▎     | 17/40 [02:54<04:35, 11.97s/it] 45%|████▌     | 18/40 [03:03<04:03, 11.08s/it] 48%|████▊     | 19/40 [03:12<03:40, 10.49s/it] 50%|█████     | 20/40 [03:21<03:21, 10.08s/it] 52%|█████▎    | 21/40 [03:30<03:07,  9.84s/it] 55%|█████▌    | 22/40 [03:40<02:56,  9.78s/it] 57%|█████▊    | 23/40 [03:49<02:44,  9.68s/it] 60%|██████    | 24/40 [03:58<02:33,  9.57s/it] 62%|██████▎   | 25/40 [04:16<02:58, 11.88s/it] 65%|██████▌   | 26/40 [04:25<02:36, 11.18s/it] 68%|██████▊   | 27/40 [04:34<02:17, 10.59s/it] 70%|███████   | 28/40 [04:44<02:03, 10.32s/it] 72%|███████▎  | 29/40 [04:54<01:50, 10.08s/it] 75%|███████▌  | 30/40 [05:03<01:39,  9.98s/it] 78%|███████▊  | 31/40 [05:12<01:26,  9.65s/it] 80%|████████  | 32/40 [05:22<01:17,  9.64s/it] 82%|████████▎ | 33/40 [05:38<01:20, 11.50s/it] 85%|████████▌ | 34/40 [05:48<01:05, 10.99s/it] 88%|████████▊ | 35/40 [05:57<00:52, 10.43s/it] 90%|█████████ | 36/40 [06:06<00:40, 10.04s/it] 92%|█████████▎| 37/40 [06:15<00:29,  9.80s/it] 95%|█████████▌| 38/40 [06:24<00:19,  9.61s/it] 98%|█████████▊| 39/40 [06:34<00:09,  9.65s/it]100%|██████████| 40/40 [06:43<00:00,  9.57s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:43<00:00,  9.57s/it]100%|██████████| 40/40 [06:43<00:00, 10.10s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 403.8635, 'train_samples_per_second': 6.945, 'train_steps_per_second': 0.099, 'train_loss': 0.7830683708190918, 'epoch': 4.91}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:05,  6.31s/it]  5%|▌         | 2/40 [00:12<03:52,  6.13s/it]  8%|▊         | 3/40 [00:18<03:49,  6.19s/it] 10%|█         | 4/40 [00:25<03:46,  6.29s/it] 12%|█▎        | 5/40 [00:31<03:42,  6.35s/it] 15%|█▌        | 6/40 [00:37<03:33,  6.27s/it] 18%|█▊        | 7/40 [00:43<03:27,  6.29s/it] 20%|██        | 8/40 [00:49<03:18,  6.20s/it] 22%|██▎       | 9/40 [00:56<03:18,  6.41s/it] 25%|██▌       | 10/40 [01:02<03:09,  6.31s/it] 28%|██▊       | 11/40 [01:09<03:02,  6.29s/it] 30%|███       | 12/40 [01:15<02:54,  6.24s/it] 32%|███▎      | 13/40 [01:21<02:50,  6.33s/it] 35%|███▌      | 14/40 [01:28<02:44,  6.32s/it] 38%|███▊      | 15/40 [01:34<02:41,  6.48s/it] 40%|████      | 16/40 [01:41<02:34,  6.43s/it] 42%|████▎     | 17/40 [01:48<02:35,  6.76s/it] 45%|████▌     | 18/40 [01:55<02:27,  6.71s/it] 48%|████▊     | 19/40 [02:01<02:17,  6.54s/it] 50%|█████     | 20/40 [02:07<02:08,  6.43s/it] 52%|█████▎    | 21/40 [02:13<02:00,  6.36s/it] 55%|█████▌    | 22/40 [02:20<01:53,  6.29s/it] 57%|█████▊    | 23/40 [02:26<01:47,  6.32s/it] 60%|██████    | 24/40 [02:32<01:39,  6.25s/it] 62%|██████▎   | 25/40 [02:39<01:37,  6.50s/it] 65%|██████▌   | 26/40 [02:45<01:30,  6.44s/it] 68%|██████▊   | 27/40 [02:51<01:21,  6.29s/it] 70%|███████   | 28/40 [02:58<01:16,  6.35s/it] 72%|███████▎  | 29/40 [03:04<01:09,  6.33s/it] 75%|███████▌  | 30/40 [03:10<01:02,  6.24s/it] 78%|███████▊  | 31/40 [03:17<00:56,  6.31s/it] 80%|████████  | 32/40 [03:23<00:50,  6.29s/it] 82%|████████▎ | 33/40 [03:30<00:46,  6.62s/it] 85%|████████▌ | 34/40 [03:36<00:38,  6.48s/it] 88%|████████▊ | 35/40 [03:43<00:32,  6.49s/it] 90%|█████████ | 36/40 [03:49<00:25,  6.40s/it] 92%|█████████▎| 37/40 [03:56<00:19,  6.44s/it] 95%|█████████▌| 38/40 [04:02<00:12,  6.47s/it] 98%|█████████▊| 39/40 [04:08<00:06,  6.41s/it]100%|██████████| 40/40 [04:15<00:00,  6.38s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:15<00:00,  6.38s/it]100%|██████████| 40/40 [04:15<00:00,  6.38s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_iontransporters_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 255.2079, 'train_samples_per_second': 10.227, 'train_steps_per_second': 0.157, 'train_loss': 0.6997037887573242, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:06,  6.32s/it]  5%|▌         | 2/40 [00:12<03:55,  6.20s/it]  8%|▊         | 3/40 [00:18<03:50,  6.24s/it] 10%|█         | 4/40 [00:25<03:47,  6.33s/it] 12%|█▎        | 5/40 [00:31<03:44,  6.42s/it] 15%|█▌        | 6/40 [00:37<03:35,  6.34s/it] 18%|█▊        | 7/40 [00:44<03:29,  6.36s/it] 20%|██        | 8/40 [00:50<03:20,  6.26s/it] 22%|██▎       | 9/40 [00:57<03:20,  6.48s/it] 25%|██▌       | 10/40 [01:03<03:11,  6.40s/it] 28%|██▊       | 11/40 [01:10<03:06,  6.42s/it] 30%|███       | 12/40 [01:16<02:58,  6.36s/it] 32%|███▎      | 13/40 [01:22<02:53,  6.42s/it] 35%|███▌      | 14/40 [01:29<02:46,  6.40s/it] 38%|███▊      | 15/40 [01:35<02:40,  6.42s/it] 40%|████      | 16/40 [01:41<02:33,  6.39s/it] 42%|████▎     | 17/40 [01:49<02:34,  6.71s/it] 45%|████▌     | 18/40 [01:55<02:26,  6.66s/it] 48%|████▊     | 19/40 [02:02<02:17,  6.57s/it] 50%|█████     | 20/40 [02:08<02:09,  6.50s/it] 52%|█████▎    | 21/40 [02:15<02:02,  6.46s/it] 55%|█████▌    | 22/40 [02:21<01:55,  6.40s/it] 57%|█████▊    | 23/40 [02:27<01:49,  6.46s/it] 60%|██████    | 24/40 [02:34<01:42,  6.44s/it] 62%|██████▎   | 25/40 [02:41<01:40,  6.69s/it] 65%|██████▌   | 26/40 [02:47<01:32,  6.60s/it] 68%|██████▊   | 27/40 [02:53<01:23,  6.41s/it] 70%|███████   | 28/40 [03:00<01:17,  6.44s/it] 72%|███████▎  | 29/40 [03:06<01:10,  6.39s/it] 75%|███████▌  | 30/40 [03:12<01:02,  6.30s/it] 78%|███████▊  | 31/40 [03:19<00:57,  6.35s/it] 80%|████████  | 32/40 [03:25<00:50,  6.32s/it] 82%|████████▎ | 33/40 [03:32<00:46,  6.64s/it] 85%|████████▌ | 34/40 [03:39<00:39,  6.50s/it] 88%|████████▊ | 35/40 [03:45<00:32,  6.50s/it] 90%|█████████ | 36/40 [03:51<00:25,  6.40s/it] 92%|█████████▎| 37/40 [03:58<00:19,  6.39s/it] 95%|█████████▌| 38/40 [04:04<00:12,  6.36s/it] 98%|█████████▊| 39/40 [04:10<00:06,  6.28s/it]100%|██████████| 40/40 [04:16<00:00,  6.22s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:16<00:00,  6.22s/it]100%|██████████| 40/40 [04:16<00:00,  6.41s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_iontransporters_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 256.5421, 'train_samples_per_second': 10.174, 'train_steps_per_second': 0.156, 'train_loss': 0.7170417785644532, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:09,  9.48s/it]  5%|▌         | 2/40 [00:18<05:51,  9.25s/it]  8%|▊         | 3/40 [00:27<05:42,  9.26s/it] 10%|█         | 4/40 [00:37<05:36,  9.34s/it] 12%|█▎        | 5/40 [00:46<05:27,  9.37s/it] 15%|█▌        | 6/40 [00:55<05:12,  9.20s/it] 18%|█▊        | 7/40 [01:04<05:04,  9.24s/it] 20%|██        | 8/40 [01:13<04:50,  9.09s/it] 22%|██▎       | 9/40 [01:23<04:50,  9.38s/it] 25%|██▌       | 10/40 [01:32<04:36,  9.22s/it] 28%|██▊       | 11/40 [01:41<04:26,  9.19s/it] 30%|███       | 12/40 [01:50<04:15,  9.14s/it] 32%|███▎      | 13/40 [02:00<04:09,  9.26s/it] 35%|███▌      | 14/40 [02:09<04:01,  9.30s/it] 38%|███▊      | 15/40 [02:19<03:55,  9.44s/it] 40%|████      | 16/40 [02:28<03:45,  9.40s/it] 42%|████▎     | 17/40 [02:39<03:47,  9.88s/it] 45%|████▌     | 18/40 [02:49<03:34,  9.73s/it] 48%|████▊     | 19/40 [02:58<03:20,  9.55s/it] 50%|█████     | 20/40 [03:07<03:08,  9.45s/it] 52%|█████▎    | 21/40 [03:16<02:58,  9.37s/it] 55%|█████▌    | 22/40 [03:25<02:47,  9.31s/it] 57%|█████▊    | 23/40 [03:35<02:39,  9.39s/it] 60%|██████    | 24/40 [03:44<02:29,  9.34s/it] 62%|██████▎   | 25/40 [03:55<02:26,  9.76s/it] 65%|██████▌   | 26/40 [04:04<02:15,  9.67s/it] 68%|██████▊   | 27/40 [04:13<02:03,  9.47s/it] 70%|███████   | 28/40 [04:23<01:54,  9.51s/it] 72%|███████▎  | 29/40 [04:32<01:44,  9.47s/it] 75%|███████▌  | 30/40 [04:41<01:33,  9.39s/it] 78%|███████▊  | 31/40 [04:51<01:25,  9.44s/it] 80%|████████  | 32/40 [05:00<01:15,  9.41s/it] 82%|████████▎ | 33/40 [05:11<01:09,  9.88s/it] 85%|████████▌ | 34/40 [05:21<00:58,  9.71s/it] 88%|████████▊ | 35/40 [05:30<00:48,  9.74s/it] 90%|█████████ | 36/40 [05:40<00:38,  9.56s/it] 92%|█████████▎| 37/40 [05:49<00:28,  9.52s/it] 95%|█████████▌| 38/40 [05:58<00:18,  9.48s/it] 98%|█████████▊| 39/40 [06:07<00:09,  9.35s/it]100%|██████████| 40/40 [06:16<00:00,  9.20s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:16<00:00,  9.20s/it]100%|██████████| 40/40 [06:16<00:00,  9.42s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_iontransporters_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 376.8013, 'train_samples_per_second': 6.927, 'train_steps_per_second': 0.106, 'train_loss': 0.6958465576171875, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:31, 10.05s/it]  5%|▌         | 2/40 [00:19<06:18,  9.96s/it]  8%|▊         | 3/40 [00:30<06:13, 10.10s/it] 10%|█         | 4/40 [00:40<06:07, 10.22s/it] 12%|█▎        | 5/40 [00:51<06:01, 10.33s/it] 15%|█▌        | 6/40 [01:01<05:46, 10.18s/it] 18%|█▊        | 7/40 [01:11<05:37, 10.24s/it] 20%|██        | 8/40 [01:21<05:22, 10.09s/it] 22%|██▎       | 9/40 [01:32<05:23, 10.45s/it] 25%|██▌       | 10/40 [01:42<05:08, 10.27s/it] 28%|██▊       | 11/40 [01:52<04:57, 10.27s/it] 30%|███       | 12/40 [02:02<04:44, 10.17s/it] 32%|███▎      | 13/40 [02:13<04:37, 10.29s/it] 35%|███▌      | 14/40 [02:23<04:25, 10.22s/it] 38%|███▊      | 15/40 [02:33<04:16, 10.27s/it] 40%|████      | 16/40 [02:43<04:06, 10.26s/it] 42%|████▎     | 17/40 [02:55<04:09, 10.86s/it] 45%|████▌     | 18/40 [03:06<03:55, 10.72s/it] 48%|████▊     | 19/40 [03:16<03:40, 10.51s/it] 50%|█████     | 20/40 [03:26<03:27, 10.39s/it] 52%|█████▎    | 21/40 [03:36<03:15, 10.31s/it] 55%|█████▌    | 22/40 [03:46<03:04, 10.23s/it] 57%|█████▊    | 23/40 [03:57<02:55, 10.34s/it] 60%|██████    | 24/40 [04:07<02:42, 10.18s/it] 62%|██████▎   | 25/40 [04:18<02:39, 10.61s/it] 65%|██████▌   | 26/40 [04:28<02:26, 10.45s/it] 68%|██████▊   | 27/40 [04:38<02:12, 10.16s/it] 70%|███████   | 28/40 [04:48<02:02, 10.22s/it] 72%|███████▎  | 29/40 [04:58<01:51, 10.15s/it] 75%|███████▌  | 30/40 [05:08<01:41, 10.13s/it] 78%|███████▊  | 31/40 [05:19<01:32, 10.27s/it] 80%|████████  | 32/40 [05:29<01:22, 10.30s/it] 82%|████████▎ | 33/40 [05:41<01:15, 10.86s/it] 85%|████████▌ | 34/40 [05:52<01:04, 10.68s/it] 88%|████████▊ | 35/40 [06:02<00:53, 10.67s/it] 90%|█████████ | 36/40 [06:12<00:41, 10.47s/it] 92%|█████████▎| 37/40 [06:23<00:31, 10.44s/it] 95%|█████████▌| 38/40 [06:33<00:20, 10.37s/it] 98%|█████████▊| 39/40 [06:43<00:10, 10.21s/it]100%|██████████| 40/40 [06:52<00:00, 10.04s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:52<00:00, 10.04s/it]100%|██████████| 40/40 [06:52<00:00, 10.32s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_iontransporters_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'train_runtime': 412.791, 'train_samples_per_second': 6.323, 'train_steps_per_second': 0.097, 'train_loss': 0.7543684005737304, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:05<26:47,  5.66s/it]  1%|          | 2/285 [00:11<26:19,  5.58s/it]  1%|          | 3/285 [00:16<26:31,  5.64s/it]  1%|▏         | 4/285 [00:22<26:36,  5.68s/it]  2%|▏         | 5/285 [00:28<26:37,  5.70s/it]  2%|▏         | 6/285 [00:34<26:28,  5.70s/it]  2%|▏         | 7/285 [00:39<26:05,  5.63s/it]  3%|▎         | 8/285 [00:44<25:30,  5.52s/it]  3%|▎         | 9/285 [00:50<25:26,  5.53s/it]  4%|▎         | 10/285 [00:56<25:39,  5.60s/it]  4%|▍         | 11/285 [01:01<25:31,  5.59s/it]  4%|▍         | 12/285 [01:07<25:56,  5.70s/it]  5%|▍         | 13/285 [01:13<25:39,  5.66s/it]  5%|▍         | 14/285 [01:18<25:29,  5.64s/it]  5%|▌         | 15/285 [01:24<24:53,  5.53s/it]  6%|▌         | 16/285 [01:29<24:33,  5.48s/it]  6%|▌         | 17/285 [01:35<24:55,  5.58s/it]  6%|▋         | 18/285 [01:41<25:02,  5.63s/it]  7%|▋         | 19/285 [01:46<25:06,  5.66s/it]  7%|▋         | 20/285 [01:52<25:13,  5.71s/it]  7%|▋         | 21/285 [01:58<25:20,  5.76s/it]  8%|▊         | 22/285 [02:04<25:37,  5.85s/it]  8%|▊         | 23/285 [02:09<24:59,  5.72s/it]  8%|▊         | 24/285 [02:15<24:38,  5.66s/it]  9%|▉         | 25/285 [02:21<24:39,  5.69s/it]  9%|▉         | 26/285 [02:27<24:53,  5.77s/it]  9%|▉         | 27/285 [02:32<24:50,  5.78s/it] 10%|▉         | 28/285 [02:38<24:57,  5.82s/it] 10%|█         | 29/285 [02:44<24:48,  5.81s/it] 11%|█         | 30/285 [02:50<24:49,  5.84s/it] 11%|█         | 31/285 [02:56<24:46,  5.85s/it] 11%|█         | 32/285 [03:02<25:08,  5.96s/it] 12%|█▏        | 33/285 [03:08<24:43,  5.89s/it] 12%|█▏        | 34/285 [03:14<24:22,  5.83s/it] 12%|█▏        | 35/285 [03:20<24:25,  5.86s/it] 13%|█▎        | 36/285 [03:25<24:16,  5.85s/it] 13%|█▎        | 37/285 [03:32<24:34,  5.94s/it] 13%|█▎        | 38/285 [03:37<24:12,  5.88s/it] 14%|█▎        | 39/285 [03:43<24:05,  5.88s/it] 14%|█▍        | 40/285 [03:49<24:01,  5.88s/it] 14%|█▍        | 41/285 [03:55<23:46,  5.85s/it] 15%|█▍        | 42/285 [04:01<23:51,  5.89s/it] 15%|█▌        | 43/285 [04:07<24:01,  5.96s/it] 15%|█▌        | 44/285 [04:13<23:42,  5.90s/it] 16%|█▌        | 45/285 [04:18<23:24,  5.85s/it] 16%|█▌        | 46/285 [04:25<23:53,  6.00s/it] 16%|█▋        | 47/285 [04:30<23:23,  5.90s/it] 17%|█▋        | 48/285 [04:37<23:39,  5.99s/it] 17%|█▋        | 49/285 [04:43<23:28,  5.97s/it] 18%|█▊        | 50/285 [04:49<23:30,  6.00s/it] 18%|█▊        | 51/285 [04:55<23:39,  6.07s/it] 18%|█▊        | 52/285 [05:01<23:31,  6.06s/it] 19%|█▊        | 53/285 [05:07<23:09,  5.99s/it] 19%|█▉        | 54/285 [05:13<23:11,  6.02s/it] 19%|█▉        | 55/285 [05:19<23:06,  6.03s/it] 20%|█▉        | 56/285 [05:25<22:48,  5.98s/it] 20%|██        | 57/285 [05:31<22:38,  5.96s/it] 20%|██        | 58/285 [05:41<27:38,  7.31s/it] 21%|██        | 59/285 [05:47<26:02,  6.92s/it] 21%|██        | 60/285 [05:53<24:54,  6.64s/it] 21%|██▏       | 61/285 [05:59<24:03,  6.44s/it] 22%|██▏       | 62/285 [06:05<23:02,  6.20s/it] 22%|██▏       | 63/285 [06:11<22:48,  6.17s/it] 22%|██▏       | 64/285 [06:17<22:44,  6.17s/it] 23%|██▎       | 65/285 [06:23<22:31,  6.14s/it] 23%|██▎       | 66/285 [06:29<22:01,  6.03s/it] 24%|██▎       | 67/285 [06:35<21:56,  6.04s/it] 24%|██▍       | 68/285 [06:41<22:02,  6.09s/it] 24%|██▍       | 69/285 [06:47<21:24,  5.95s/it] 25%|██▍       | 70/285 [06:52<21:07,  5.90s/it] 25%|██▍       | 71/285 [06:58<21:08,  5.93s/it] 25%|██▌       | 72/285 [07:04<21:03,  5.93s/it] 26%|██▌       | 73/285 [07:10<20:38,  5.84s/it] 26%|██▌       | 74/285 [07:16<20:30,  5.83s/it] 26%|██▋       | 75/285 [07:22<20:26,  5.84s/it] 27%|██▋       | 76/285 [07:28<20:33,  5.90s/it] 27%|██▋       | 77/285 [07:33<20:17,  5.85s/it] 27%|██▋       | 78/285 [07:40<20:23,  5.91s/it] 28%|██▊       | 79/285 [07:46<20:33,  5.99s/it] 28%|██▊       | 80/285 [07:52<20:29,  6.00s/it] 28%|██▊       | 81/285 [07:58<20:14,  5.96s/it] 29%|██▉       | 82/285 [08:03<20:05,  5.94s/it] 29%|██▉       | 83/285 [08:09<19:53,  5.91s/it] 29%|██▉       | 84/285 [08:15<19:48,  5.91s/it] 30%|██▉       | 85/285 [08:21<19:28,  5.84s/it] 30%|███       | 86/285 [08:27<19:31,  5.89s/it] 31%|███       | 87/285 [08:33<19:17,  5.85s/it] 31%|███       | 88/285 [08:39<19:20,  5.89s/it] 31%|███       | 89/285 [08:45<19:18,  5.91s/it] 32%|███▏      | 90/285 [08:51<19:20,  5.95s/it] 32%|███▏      | 91/285 [08:57<19:16,  5.96s/it] 32%|███▏      | 92/285 [09:02<18:52,  5.87s/it] 33%|███▎      | 93/285 [09:08<18:48,  5.88s/it] 33%|███▎      | 94/285 [09:14<18:52,  5.93s/it] 33%|███▎      | 95/285 [09:20<18:29,  5.84s/it] 34%|███▎      | 96/285 [09:26<18:28,  5.87s/it] 34%|███▍      | 97/285 [09:32<18:30,  5.91s/it] 34%|███▍      | 98/285 [09:38<18:19,  5.88s/it] 35%|███▍      | 99/285 [09:43<18:00,  5.81s/it] 35%|███▌      | 100/285 [09:49<17:56,  5.82s/it] 35%|███▌      | 101/285 [09:55<18:07,  5.91s/it] 36%|███▌      | 102/285 [10:01<17:57,  5.89s/it] 36%|███▌      | 103/285 [10:07<17:40,  5.83s/it] 36%|███▋      | 104/285 [10:13<17:41,  5.86s/it] 37%|███▋      | 105/285 [10:18<17:30,  5.84s/it] 37%|███▋      | 106/285 [10:25<17:42,  5.94s/it] 38%|███▊      | 107/285 [10:31<17:36,  5.94s/it] 38%|███▊      | 108/285 [10:37<17:31,  5.94s/it] 38%|███▊      | 109/285 [10:42<17:17,  5.89s/it] 39%|███▊      | 110/285 [10:48<17:14,  5.91s/it] 39%|███▉      | 111/285 [10:54<17:12,  5.93s/it] 39%|███▉      | 112/285 [11:00<17:10,  5.96s/it] 40%|███▉      | 113/285 [11:06<16:53,  5.89s/it] 40%|████      | 114/285 [11:12<17:00,  5.97s/it] 40%|████      | 115/285 [11:23<20:48,  7.35s/it] 41%|████      | 116/285 [11:29<19:32,  6.94s/it] 41%|████      | 117/285 [11:35<18:38,  6.66s/it] 41%|████▏     | 118/285 [11:41<17:51,  6.42s/it] 42%|████▏     | 119/285 [11:46<17:14,  6.23s/it] 42%|████▏     | 120/285 [11:52<16:55,  6.16s/it] 42%|████▏     | 121/285 [11:58<16:30,  6.04s/it] 43%|████▎     | 122/285 [12:04<16:13,  5.97s/it] 43%|████▎     | 123/285 [12:10<16:10,  5.99s/it] 44%|████▎     | 124/285 [12:16<15:58,  5.95s/it] 44%|████▍     | 125/285 [12:22<15:54,  5.96s/it] 44%|████▍     | 126/285 [12:28<16:03,  6.06s/it] 45%|████▍     | 127/285 [12:34<16:00,  6.08s/it] 45%|████▍     | 128/285 [12:40<15:48,  6.04s/it] 45%|████▌     | 129/285 [12:46<15:26,  5.94s/it] 46%|████▌     | 130/285 [12:52<15:15,  5.91s/it] 46%|████▌     | 131/285 [12:58<15:24,  6.00s/it] 46%|████▋     | 132/285 [13:04<15:14,  5.97s/it] 47%|████▋     | 133/285 [13:10<14:55,  5.89s/it] 47%|████▋     | 134/285 [13:16<15:03,  5.99s/it] 47%|████▋     | 135/285 [13:21<14:43,  5.89s/it] 48%|████▊     | 136/285 [13:27<14:25,  5.81s/it] 48%|████▊     | 137/285 [13:33<14:18,  5.80s/it] 48%|████▊     | 138/285 [13:39<14:17,  5.83s/it] 49%|████▉     | 139/285 [13:44<14:03,  5.78s/it] 49%|████▉     | 140/285 [13:50<13:56,  5.77s/it] 49%|████▉     | 141/285 [13:56<13:50,  5.76s/it] 50%|████▉     | 142/285 [14:02<13:42,  5.75s/it] 50%|█████     | 143/285 [14:07<13:20,  5.64s/it] 51%|█████     | 144/285 [14:13<13:25,  5.71s/it] 51%|█████     | 145/285 [14:19<13:32,  5.80s/it] 51%|█████     | 146/285 [14:25<13:26,  5.80s/it] 52%|█████▏    | 147/285 [14:30<13:16,  5.77s/it] 52%|█████▏    | 148/285 [14:36<13:07,  5.75s/it] 52%|█████▏    | 149/285 [14:42<13:13,  5.84s/it] 53%|█████▎    | 150/285 [14:48<13:15,  5.89s/it] 53%|█████▎    | 151/285 [14:54<13:04,  5.85s/it] 53%|█████▎    | 152/285 [15:00<12:56,  5.84s/it] 54%|█████▎    | 153/285 [15:05<12:46,  5.80s/it] 54%|█████▍    | 154/285 [15:11<12:49,  5.87s/it] 54%|█████▍    | 155/285 [15:17<12:43,  5.88s/it] 55%|█████▍    | 156/285 [15:23<12:40,  5.89s/it] 55%|█████▌    | 157/285 [15:29<12:37,  5.92s/it] 55%|█████▌    | 158/285 [15:35<12:27,  5.88s/it] 56%|█████▌    | 159/285 [15:41<12:28,  5.94s/it] 56%|█████▌    | 160/285 [15:47<12:11,  5.85s/it] 56%|█████▋    | 161/285 [15:53<12:12,  5.91s/it] 57%|█████▋    | 162/285 [15:59<12:03,  5.88s/it] 57%|█████▋    | 163/285 [16:04<11:52,  5.84s/it] 58%|█████▊    | 164/285 [16:10<11:37,  5.76s/it] 58%|█████▊    | 165/285 [16:16<11:52,  5.94s/it] 58%|█████▊    | 166/285 [16:22<11:38,  5.87s/it] 59%|█████▊    | 167/285 [16:28<11:32,  5.87s/it] 59%|█████▉    | 168/285 [16:33<11:18,  5.80s/it] 59%|█████▉    | 169/285 [16:39<11:08,  5.76s/it] 60%|█████▉    | 170/285 [16:45<11:07,  5.81s/it] 60%|██████    | 171/285 [16:51<10:59,  5.78s/it] 60%|██████    | 172/285 [17:00<12:55,  6.86s/it] 61%|██████    | 173/285 [17:06<12:09,  6.51s/it] 61%|██████    | 174/285 [17:12<11:42,  6.33s/it] 61%|██████▏   | 175/285 [17:17<11:14,  6.13s/it] 62%|██████▏   | 176/285 [17:24<11:07,  6.12s/it] 62%|██████▏   | 177/285 [17:30<10:58,  6.10s/it] 62%|██████▏   | 178/285 [17:35<10:34,  5.93s/it] 63%|██████▎   | 179/285 [17:41<10:23,  5.88s/it] 63%|██████▎   | 180/285 [17:46<10:08,  5.80s/it] 64%|██████▎   | 181/285 [17:53<10:15,  5.92s/it] 64%|██████▍   | 182/285 [17:58<10:02,  5.85s/it] 64%|██████▍   | 183/285 [18:04<09:50,  5.79s/it] 65%|██████▍   | 184/285 [18:10<09:38,  5.73s/it] 65%|██████▍   | 185/285 [18:16<09:43,  5.83s/it] 65%|██████▌   | 186/285 [18:22<09:36,  5.83s/it] 66%|██████▌   | 187/285 [18:27<09:27,  5.79s/it] 66%|██████▌   | 188/285 [18:33<09:16,  5.73s/it] 66%|██████▋   | 189/285 [18:39<09:12,  5.76s/it] 67%|██████▋   | 190/285 [18:44<08:59,  5.68s/it] 67%|██████▋   | 191/285 [18:50<08:56,  5.71s/it] 67%|██████▋   | 192/285 [18:56<08:50,  5.70s/it] 68%|██████▊   | 193/285 [19:01<08:41,  5.67s/it] 68%|██████▊   | 194/285 [19:07<08:32,  5.63s/it] 68%|██████▊   | 195/285 [19:13<08:37,  5.75s/it] 69%|██████▉   | 196/285 [19:18<08:23,  5.66s/it] 69%|██████▉   | 197/285 [19:24<08:21,  5.70s/it] 69%|██████▉   | 198/285 [19:30<08:17,  5.72s/it] 70%|██████▉   | 199/285 [19:35<08:06,  5.65s/it] 70%|███████   | 200/285 [19:41<08:08,  5.75s/it]                                                  70%|███████   | 200/285 [19:41<08:08,  5.75s/it] 71%|███████   | 201/285 [19:47<08:12,  5.87s/it] 71%|███████   | 202/285 [19:53<08:03,  5.83s/it] 71%|███████   | 203/285 [19:59<08:03,  5.89s/it] 72%|███████▏  | 204/285 [20:05<07:59,  5.92s/it] 72%|███████▏  | 205/285 [20:11<07:57,  5.96s/it] 72%|███████▏  | 206/285 [20:17<07:42,  5.85s/it] 73%|███████▎  | 207/285 [20:23<07:39,  5.89s/it] 73%|███████▎  | 208/285 [20:28<07:22,  5.75s/it] 73%|███████▎  | 209/285 [20:34<07:12,  5.69s/it] 74%|███████▎  | 210/285 [20:39<07:04,  5.66s/it] 74%|███████▍  | 211/285 [20:45<06:59,  5.67s/it] 74%|███████▍  | 212/285 [20:51<06:50,  5.62s/it] 75%|███████▍  | 213/285 [20:56<06:47,  5.66s/it] 75%|███████▌  | 214/285 [21:02<06:39,  5.63s/it] 75%|███████▌  | 215/285 [21:07<06:34,  5.63s/it] 76%|███████▌  | 216/285 [21:13<06:27,  5.62s/it] 76%|███████▌  | 217/285 [21:19<06:22,  5.63s/it] 76%|███████▋  | 218/285 [21:24<06:14,  5.58s/it] 77%|███████▋  | 219/285 [21:30<06:05,  5.54s/it] 77%|███████▋  | 220/285 [21:36<06:06,  5.64s/it] 78%|███████▊  | 221/285 [21:41<06:05,  5.72s/it] 78%|███████▊  | 222/285 [21:47<05:53,  5.61s/it] 78%|███████▊  | 223/285 [21:52<05:47,  5.60s/it] 79%|███████▊  | 224/285 [21:58<05:46,  5.68s/it] 79%|███████▉  | 225/285 [22:04<05:37,  5.62s/it] 79%|███████▉  | 226/285 [22:09<05:34,  5.67s/it] 80%|███████▉  | 227/285 [22:15<05:33,  5.75s/it] 80%|████████  | 228/285 [22:21<05:29,  5.77s/it] 80%|████████  | 229/285 [22:31<06:37,  7.11s/it] 81%|████████  | 230/285 [22:38<06:13,  6.80s/it] 81%|████████  | 231/285 [22:44<06:01,  6.70s/it] 81%|████████▏ | 232/285 [22:50<05:48,  6.58s/it] 82%|████████▏ | 233/285 [22:56<05:28,  6.31s/it] 82%|████████▏ | 234/285 [23:02<05:13,  6.15s/it] 82%|████████▏ | 235/285 [23:07<04:58,  5.98s/it] 83%|████████▎ | 236/285 [23:13<04:47,  5.87s/it] 83%|████████▎ | 237/285 [23:18<04:34,  5.72s/it] 84%|████████▎ | 238/285 [23:24<04:29,  5.74s/it] 84%|████████▍ | 239/285 [23:30<04:21,  5.68s/it] 84%|████████▍ | 240/285 [23:35<04:16,  5.69s/it] 85%|████████▍ | 241/285 [23:41<04:10,  5.69s/it] 85%|████████▍ | 242/285 [23:47<04:02,  5.64s/it] 85%|████████▌ | 243/285 [23:52<03:52,  5.53s/it] 86%|████████▌ | 244/285 [23:57<03:44,  5.48s/it] 86%|████████▌ | 245/285 [24:03<03:42,  5.57s/it] 86%|████████▋ | 246/285 [24:08<03:34,  5.51s/it] 87%|████████▋ | 247/285 [24:14<03:33,  5.62s/it] 87%|████████▋ | 248/285 [24:20<03:28,  5.64s/it] 87%|████████▋ | 249/285 [24:26<03:26,  5.73s/it] 88%|████████▊ | 250/285 [24:32<03:20,  5.73s/it] 88%|████████▊ | 251/285 [24:37<03:11,  5.63s/it] 88%|████████▊ | 252/285 [24:43<03:08,  5.72s/it] 89%|████████▉ | 253/285 [24:48<03:01,  5.67s/it] 89%|████████▉ | 254/285 [24:54<02:57,  5.72s/it] 89%|████████▉ | 255/285 [25:00<02:52,  5.77s/it] 90%|████████▉ | 256/285 [25:06<02:45,  5.69s/it] 90%|█████████ | 257/285 [25:11<02:38,  5.65s/it] 91%|█████████ | 258/285 [25:17<02:32,  5.65s/it] 91%|█████████ | 259/285 [25:23<02:27,  5.66s/it] 91%|█████████ | 260/285 [25:28<02:20,  5.63s/it] 92%|█████████▏| 261/285 [25:34<02:16,  5.70s/it] 92%|█████████▏| 262/285 [25:40<02:09,  5.65s/it] 92%|█████████▏| 263/285 [25:45<02:05,  5.68s/it] 93%|█████████▎| 264/285 [25:51<01:59,  5.71s/it] 93%|█████████▎| 265/285 [25:57<01:52,  5.63s/it] 93%|█████████▎| 266/285 [26:03<01:48,  5.73s/it] 94%|█████████▎| 267/285 [26:08<01:43,  5.73s/it] 94%|█████████▍| 268/285 [26:14<01:36,  5.71s/it] 94%|█████████▍| 269/285 [26:20<01:32,  5.76s/it] 95%|█████████▍| 270/285 [26:25<01:25,  5.70s/it] 95%|█████████▌| 271/285 [26:31<01:20,  5.72s/it] 95%|█████████▌| 272/285 [26:37<01:14,  5.73s/it] 96%|█████████▌| 273/285 [26:43<01:08,  5.70s/it] 96%|█████████▌| 274/285 [26:48<01:02,  5.69s/it] 96%|█████████▋| 275/285 [26:54<00:56,  5.64s/it] 97%|█████████▋| 276/285 [26:59<00:50,  5.58s/it] 97%|█████████▋| 277/285 [27:05<00:45,  5.65s/it] 98%|█████████▊| 278/285 [27:11<00:39,  5.64s/it] 98%|█████████▊| 279/285 [27:16<00:34,  5.69s/it] 98%|█████████▊| 280/285 [27:22<00:28,  5.75s/it] 99%|█████████▊| 281/285 [27:28<00:22,  5.69s/it] 99%|█████████▉| 282/285 [27:34<00:17,  5.70s/it] 99%|█████████▉| 283/285 [27:39<00:11,  5.74s/it]100%|█████████▉| 284/285 [27:45<00:05,  5.80s/it]100%|██████████| 285/285 [27:51<00:00,  5.70s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [27:51<00:00,  5.70s/it]100%|██████████| 285/285 [27:51<00:00,  5.86s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'loss': 0.3619, 'learning_rate': 9.85e-06, 'epoch': 3.5}
{'train_runtime': 1671.308, 'train_samples_per_second': 11.051, 'train_steps_per_second': 0.171, 'train_loss': 0.2911775555527001, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:05<26:19,  5.56s/it]  1%|          | 2/285 [00:11<26:15,  5.57s/it]  1%|          | 3/285 [00:16<26:36,  5.66s/it]  1%|▏         | 4/285 [00:22<26:43,  5.70s/it]  2%|▏         | 5/285 [00:28<26:47,  5.74s/it]  2%|▏         | 6/285 [00:34<26:40,  5.74s/it]  2%|▏         | 7/285 [00:39<26:15,  5.67s/it]  3%|▎         | 8/285 [00:45<25:40,  5.56s/it]  3%|▎         | 9/285 [00:50<25:22,  5.51s/it]  4%|▎         | 10/285 [00:56<25:34,  5.58s/it]  4%|▍         | 11/285 [01:01<25:27,  5.58s/it]  4%|▍         | 12/285 [01:07<25:55,  5.70s/it]  5%|▍         | 13/285 [01:13<25:41,  5.67s/it]  5%|▍         | 14/285 [01:19<25:56,  5.74s/it]  5%|▌         | 15/285 [01:24<25:21,  5.64s/it]  6%|▌         | 16/285 [01:30<25:20,  5.65s/it]  6%|▌         | 17/285 [01:36<25:37,  5.74s/it]  6%|▋         | 18/285 [01:42<25:38,  5.76s/it]  7%|▋         | 19/285 [01:47<25:41,  5.79s/it]  7%|▋         | 20/285 [01:53<25:38,  5.81s/it]  7%|▋         | 21/285 [01:59<25:20,  5.76s/it]  8%|▊         | 22/285 [02:05<25:09,  5.74s/it]  8%|▊         | 23/285 [02:10<24:36,  5.64s/it]  8%|▊         | 24/285 [02:16<24:21,  5.60s/it]  9%|▉         | 25/285 [02:21<24:11,  5.58s/it]  9%|▉         | 26/285 [02:27<24:11,  5.60s/it]  9%|▉         | 27/285 [02:32<24:08,  5.61s/it] 10%|▉         | 28/285 [02:38<24:05,  5.62s/it] 10%|█         | 29/285 [02:44<23:50,  5.59s/it] 11%|█         | 30/285 [02:49<23:48,  5.60s/it] 11%|█         | 31/285 [02:55<23:41,  5.60s/it] 11%|█         | 32/285 [03:01<24:03,  5.70s/it] 12%|█▏        | 33/285 [03:06<23:41,  5.64s/it] 12%|█▏        | 34/285 [03:12<23:12,  5.55s/it] 12%|█▏        | 35/285 [03:17<23:18,  5.60s/it] 13%|█▎        | 36/285 [03:23<23:08,  5.58s/it] 13%|█▎        | 37/285 [03:29<23:27,  5.68s/it] 13%|█▎        | 38/285 [03:34<23:03,  5.60s/it] 14%|█▎        | 39/285 [03:40<22:50,  5.57s/it] 14%|█▍        | 40/285 [03:45<22:42,  5.56s/it] 14%|█▍        | 41/285 [03:51<22:27,  5.52s/it] 15%|█▍        | 42/285 [03:56<22:40,  5.60s/it] 15%|█▌        | 43/285 [04:02<22:48,  5.66s/it] 15%|█▌        | 44/285 [04:08<22:32,  5.61s/it] 16%|█▌        | 45/285 [04:13<22:15,  5.57s/it] 16%|█▌        | 46/285 [04:19<22:52,  5.74s/it] 16%|█▋        | 47/285 [04:25<22:32,  5.68s/it] 17%|█▋        | 48/285 [04:31<22:52,  5.79s/it] 17%|█▋        | 49/285 [04:37<22:40,  5.77s/it] 18%|█▊        | 50/285 [04:42<22:37,  5.78s/it] 18%|█▊        | 51/285 [04:48<22:45,  5.83s/it] 18%|█▊        | 52/285 [04:54<22:25,  5.77s/it] 19%|█▊        | 53/285 [05:00<22:11,  5.74s/it] 19%|█▉        | 54/285 [05:05<21:54,  5.69s/it] 19%|█▉        | 55/285 [05:11<21:51,  5.70s/it] 20%|█▉        | 56/285 [05:17<21:38,  5.67s/it] 20%|██        | 57/285 [05:22<21:25,  5.64s/it] 20%|██        | 58/285 [05:32<26:18,  6.95s/it] 21%|██        | 59/285 [05:38<24:52,  6.60s/it] 21%|██        | 60/285 [05:44<23:47,  6.34s/it] 21%|██▏       | 61/285 [05:49<22:56,  6.15s/it] 22%|██▏       | 62/285 [05:55<21:51,  5.88s/it] 22%|██▏       | 63/285 [06:00<21:39,  5.86s/it] 22%|██▏       | 64/285 [06:06<21:35,  5.86s/it] 23%|██▎       | 65/285 [06:12<21:28,  5.86s/it] 23%|██▎       | 66/285 [06:18<21:00,  5.76s/it] 24%|██▎       | 67/285 [06:24<21:07,  5.82s/it] 24%|██▍       | 68/285 [06:29<21:08,  5.85s/it] 24%|██▍       | 69/285 [06:35<20:24,  5.67s/it] 25%|██▍       | 70/285 [06:40<20:05,  5.61s/it] 25%|██▍       | 71/285 [06:46<20:06,  5.64s/it] 25%|██▌       | 72/285 [06:52<20:08,  5.68s/it] 26%|██▌       | 73/285 [06:57<19:37,  5.55s/it] 26%|██▌       | 74/285 [07:02<19:23,  5.52s/it] 26%|██▋       | 75/285 [07:08<19:29,  5.57s/it] 27%|██▋       | 76/285 [07:14<19:44,  5.67s/it] 27%|██▋       | 77/285 [07:19<19:25,  5.60s/it] 27%|██▋       | 78/285 [07:25<19:18,  5.60s/it] 28%|██▊       | 79/285 [07:31<19:29,  5.68s/it] 28%|██▊       | 80/285 [07:37<19:29,  5.71s/it] 28%|██▊       | 81/285 [07:43<19:34,  5.76s/it] 29%|██▉       | 82/285 [07:48<19:22,  5.73s/it] 29%|██▉       | 83/285 [07:54<19:11,  5.70s/it] 29%|██▉       | 84/285 [07:59<19:01,  5.68s/it] 30%|██▉       | 85/285 [08:05<18:33,  5.57s/it] 30%|███       | 86/285 [08:10<18:38,  5.62s/it] 31%|███       | 87/285 [08:16<18:22,  5.57s/it] 31%|███       | 88/285 [08:22<18:24,  5.61s/it] 31%|███       | 89/285 [08:27<18:20,  5.62s/it] 32%|███▏      | 90/285 [08:33<18:23,  5.66s/it] 32%|███▏      | 91/285 [08:39<18:16,  5.65s/it] 32%|███▏      | 92/285 [08:44<17:52,  5.56s/it] 33%|███▎      | 93/285 [08:50<18:04,  5.65s/it] 33%|███▎      | 94/285 [08:56<18:21,  5.77s/it] 33%|███▎      | 95/285 [09:01<18:02,  5.70s/it] 34%|███▎      | 96/285 [09:07<17:47,  5.65s/it] 34%|███▍      | 97/285 [09:13<17:41,  5.64s/it] 34%|███▍      | 98/285 [09:18<17:26,  5.60s/it] 35%|███▍      | 99/285 [09:24<17:18,  5.59s/it] 35%|███▌      | 100/285 [09:29<17:16,  5.60s/it] 35%|███▌      | 101/285 [09:35<17:32,  5.72s/it] 36%|███▌      | 102/285 [09:41<17:08,  5.62s/it] 36%|███▌      | 103/285 [09:46<16:54,  5.57s/it] 36%|███▋      | 104/285 [09:52<16:54,  5.60s/it] 37%|███▋      | 105/285 [09:57<16:44,  5.58s/it] 37%|███▋      | 106/285 [10:03<16:57,  5.68s/it] 38%|███▊      | 107/285 [10:09<16:50,  5.68s/it] 38%|███▊      | 108/285 [10:15<16:43,  5.67s/it] 38%|███▊      | 109/285 [10:20<16:27,  5.61s/it] 39%|███▊      | 110/285 [10:26<16:23,  5.62s/it] 39%|███▉      | 111/285 [10:31<16:21,  5.64s/it] 39%|███▉      | 112/285 [10:37<16:21,  5.67s/it] 40%|███▉      | 113/285 [10:43<16:03,  5.60s/it] 40%|████      | 114/285 [10:48<16:10,  5.68s/it] 40%|████      | 115/285 [10:58<19:47,  6.99s/it] 41%|████      | 116/285 [11:04<18:34,  6.59s/it] 41%|████      | 117/285 [11:10<17:43,  6.33s/it] 41%|████▏     | 118/285 [11:16<17:24,  6.25s/it] 42%|████▏     | 119/285 [11:21<16:38,  6.01s/it] 42%|████▏     | 120/285 [11:27<16:24,  5.97s/it] 42%|████▏     | 121/285 [11:33<16:08,  5.91s/it] 43%|████▎     | 122/285 [11:39<15:54,  5.86s/it] 43%|████▎     | 123/285 [11:45<15:54,  5.89s/it] 44%|████▎     | 124/285 [11:51<15:42,  5.86s/it] 44%|████▍     | 125/285 [11:56<15:41,  5.89s/it] 44%|████▍     | 126/285 [12:02<15:35,  5.88s/it] 45%|████▍     | 127/285 [12:09<15:42,  5.96s/it] 45%|████▍     | 128/285 [12:14<15:25,  5.89s/it] 45%|████▌     | 129/285 [12:20<15:02,  5.79s/it] 46%|████▌     | 130/285 [12:25<14:46,  5.72s/it] 46%|████▌     | 131/285 [12:31<14:48,  5.77s/it] 46%|████▋     | 132/285 [12:37<14:39,  5.75s/it] 47%|████▋     | 133/285 [12:42<14:25,  5.69s/it] 47%|████▋     | 134/285 [12:48<14:30,  5.76s/it] 47%|████▋     | 135/285 [12:54<14:09,  5.66s/it] 48%|████▊     | 136/285 [12:59<13:55,  5.61s/it] 48%|████▊     | 137/285 [13:05<13:52,  5.63s/it] 48%|████▊     | 138/285 [13:11<13:51,  5.66s/it] 49%|████▉     | 139/285 [13:16<13:38,  5.61s/it] 49%|████▉     | 140/285 [13:22<13:33,  5.61s/it] 49%|████▉     | 141/285 [13:28<13:32,  5.64s/it] 50%|████▉     | 142/285 [13:33<13:27,  5.65s/it] 50%|█████     | 143/285 [13:39<13:23,  5.66s/it] 51%|█████     | 144/285 [13:45<13:28,  5.73s/it] 51%|█████     | 145/285 [13:51<13:45,  5.89s/it] 51%|█████     | 146/285 [13:57<13:46,  5.95s/it] 52%|█████▏    | 147/285 [14:03<13:55,  6.05s/it] 52%|█████▏    | 148/285 [14:10<13:56,  6.11s/it] 52%|█████▏    | 149/285 [14:16<13:47,  6.08s/it] 53%|█████▎    | 150/285 [14:22<13:38,  6.06s/it] 53%|█████▎    | 151/285 [14:27<13:13,  5.92s/it] 53%|█████▎    | 152/285 [14:33<13:01,  5.87s/it] 54%|█████▎    | 153/285 [14:39<12:47,  5.81s/it] 54%|█████▍    | 154/285 [14:45<12:50,  5.88s/it] 54%|█████▍    | 155/285 [14:51<12:44,  5.88s/it] 55%|█████▍    | 156/285 [14:57<12:49,  5.97s/it] 55%|█████▌    | 157/285 [15:03<12:46,  5.99s/it] 55%|█████▌    | 158/285 [15:09<12:35,  5.95s/it] 56%|█████▌    | 159/285 [15:15<12:40,  6.04s/it] 56%|█████▌    | 160/285 [15:21<12:26,  5.97s/it] 56%|█████▋    | 161/285 [15:27<12:37,  6.11s/it] 57%|█████▋    | 162/285 [15:33<12:24,  6.06s/it] 57%|█████▋    | 163/285 [15:39<12:15,  6.03s/it] 58%|█████▊    | 164/285 [15:45<11:55,  5.91s/it] 58%|█████▊    | 165/285 [15:51<12:05,  6.04s/it] 58%|█████▊    | 166/285 [15:57<11:46,  5.94s/it] 59%|█████▊    | 167/285 [16:03<11:43,  5.96s/it] 59%|█████▉    | 168/285 [16:08<11:27,  5.88s/it] 59%|█████▉    | 169/285 [16:14<11:25,  5.91s/it] 60%|█████▉    | 170/285 [16:20<11:19,  5.91s/it] 60%|██████    | 171/285 [16:26<11:06,  5.84s/it] 60%|██████    | 172/285 [16:35<12:53,  6.85s/it] 61%|██████    | 173/285 [16:41<12:03,  6.46s/it] 61%|██████    | 174/285 [16:46<11:31,  6.23s/it] 61%|██████▏   | 175/285 [16:52<11:04,  6.04s/it] 62%|██████▏   | 176/285 [16:58<10:38,  5.85s/it] 62%|██████▏   | 177/285 [17:03<10:29,  5.83s/it] 62%|██████▏   | 178/285 [17:09<10:11,  5.71s/it] 63%|██████▎   | 179/285 [17:14<09:59,  5.66s/it] 63%|██████▎   | 180/285 [17:20<09:49,  5.61s/it] 64%|██████▎   | 181/285 [17:26<09:57,  5.75s/it] 64%|██████▍   | 182/285 [17:31<09:46,  5.70s/it] 64%|██████▍   | 183/285 [17:37<09:37,  5.66s/it] 65%|██████▍   | 184/285 [17:42<09:23,  5.58s/it] 65%|██████▍   | 185/285 [17:48<09:29,  5.69s/it] 65%|██████▌   | 186/285 [17:54<09:24,  5.70s/it] 66%|██████▌   | 187/285 [18:00<09:16,  5.67s/it] 66%|██████▌   | 188/285 [18:05<09:06,  5.63s/it] 66%|██████▋   | 189/285 [18:11<09:03,  5.66s/it] 67%|██████▋   | 190/285 [18:16<08:50,  5.58s/it] 67%|██████▋   | 191/285 [18:22<08:47,  5.61s/it] 67%|██████▋   | 192/285 [18:28<08:40,  5.60s/it] 68%|██████▊   | 193/285 [18:34<08:47,  5.74s/it] 68%|██████▊   | 194/285 [18:39<08:40,  5.72s/it] 68%|██████▊   | 195/285 [18:45<08:45,  5.84s/it] 69%|██████▉   | 196/285 [18:51<08:28,  5.71s/it] 69%|██████▉   | 197/285 [18:57<08:22,  5.71s/it] 69%|██████▉   | 198/285 [19:03<08:34,  5.91s/it] 70%|██████▉   | 199/285 [19:09<08:24,  5.86s/it] 70%|███████   | 200/285 [19:15<08:25,  5.95s/it]                                                  70%|███████   | 200/285 [19:15<08:25,  5.95s/it] 71%|███████   | 201/285 [19:21<08:21,  5.97s/it] 71%|███████   | 202/285 [19:27<08:08,  5.89s/it] 71%|███████   | 203/285 [19:33<08:06,  5.93s/it] 72%|███████▏  | 204/285 [19:39<08:10,  6.06s/it] 72%|███████▏  | 205/285 [19:45<08:09,  6.12s/it] 72%|███████▏  | 206/285 [19:51<08:02,  6.11s/it] 73%|███████▎  | 207/285 [19:57<07:52,  6.06s/it] 73%|███████▎  | 208/285 [20:03<07:37,  5.95s/it] 73%|███████▎  | 209/285 [20:09<07:34,  5.99s/it] 74%|███████▎  | 210/285 [20:15<07:25,  5.94s/it] 74%|███████▍  | 211/285 [20:21<07:23,  5.99s/it] 74%|███████▍  | 212/285 [20:27<07:11,  5.91s/it] 75%|███████▍  | 213/285 [20:33<07:12,  6.01s/it] 75%|███████▌  | 214/285 [20:39<07:07,  6.02s/it] 75%|███████▌  | 215/285 [20:45<06:59,  5.99s/it] 76%|███████▌  | 216/285 [20:51<06:54,  6.01s/it] 76%|███████▌  | 217/285 [20:57<06:47,  6.00s/it] 76%|███████▋  | 218/285 [21:02<06:32,  5.86s/it] 77%|███████▋  | 219/285 [21:08<06:19,  5.75s/it] 77%|███████▋  | 220/285 [21:14<06:17,  5.80s/it] 78%|███████▊  | 221/285 [21:20<06:13,  5.83s/it] 78%|███████▊  | 222/285 [21:25<05:58,  5.70s/it] 78%|███████▊  | 223/285 [21:31<05:50,  5.66s/it] 79%|███████▊  | 224/285 [21:37<05:49,  5.73s/it] 79%|███████▉  | 225/285 [21:42<05:39,  5.66s/it] 79%|███████▉  | 226/285 [21:48<05:36,  5.70s/it] 80%|███████▉  | 227/285 [21:53<05:29,  5.68s/it] 80%|████████  | 228/285 [21:59<05:21,  5.65s/it] 80%|████████  | 229/285 [22:09<06:22,  6.83s/it] 81%|████████  | 230/285 [22:14<05:58,  6.52s/it] 81%|████████  | 231/285 [22:20<05:39,  6.30s/it] 81%|████████▏ | 232/285 [22:26<05:19,  6.03s/it] 82%|████████▏ | 233/285 [22:31<05:05,  5.88s/it] 82%|████████▏ | 234/285 [22:37<04:54,  5.78s/it] 82%|████████▏ | 235/285 [22:42<04:45,  5.72s/it] 83%|████████▎ | 236/285 [22:48<04:37,  5.66s/it] 83%|████████▎ | 237/285 [22:53<04:27,  5.57s/it] 84%|████████▎ | 238/285 [22:59<04:24,  5.62s/it] 84%|████████▍ | 239/285 [23:04<04:17,  5.59s/it] 84%|████████▍ | 240/285 [23:10<04:13,  5.63s/it] 85%|████████▍ | 241/285 [23:16<04:07,  5.63s/it] 85%|████████▍ | 242/285 [23:22<04:03,  5.66s/it] 85%|████████▌ | 243/285 [23:27<03:53,  5.57s/it] 86%|████████▌ | 244/285 [23:32<03:46,  5.52s/it] 86%|████████▌ | 245/285 [23:38<03:43,  5.60s/it] 86%|████████▋ | 246/285 [23:43<03:35,  5.53s/it] 87%|████████▋ | 247/285 [23:49<03:33,  5.63s/it] 87%|████████▋ | 248/285 [23:55<03:30,  5.69s/it] 87%|████████▋ | 249/285 [24:01<03:27,  5.77s/it] 88%|████████▊ | 250/285 [24:07<03:21,  5.76s/it] 88%|████████▊ | 251/285 [24:12<03:12,  5.65s/it] 88%|████████▊ | 252/285 [24:18<03:09,  5.73s/it] 89%|████████▉ | 253/285 [24:24<03:02,  5.69s/it] 89%|████████▉ | 254/285 [24:30<02:58,  5.76s/it] 89%|████████▉ | 255/285 [24:35<02:53,  5.79s/it] 90%|████████▉ | 256/285 [24:41<02:45,  5.70s/it] 90%|█████████ | 257/285 [24:47<02:38,  5.66s/it] 91%|█████████ | 258/285 [24:52<02:32,  5.67s/it] 91%|█████████ | 259/285 [24:58<02:27,  5.68s/it] 91%|█████████ | 260/285 [25:04<02:21,  5.65s/it] 92%|█████████▏| 261/285 [25:09<02:17,  5.71s/it] 92%|█████████▏| 262/285 [25:15<02:10,  5.68s/it] 92%|█████████▏| 263/285 [25:21<02:08,  5.82s/it] 93%|█████████▎| 264/285 [25:27<02:03,  5.87s/it] 93%|█████████▎| 265/285 [25:33<01:55,  5.76s/it] 93%|█████████▎| 266/285 [25:38<01:49,  5.76s/it] 94%|█████████▎| 267/285 [25:44<01:43,  5.75s/it] 94%|█████████▍| 268/285 [25:50<01:39,  5.83s/it] 94%|█████████▍| 269/285 [25:56<01:34,  5.89s/it] 95%|█████████▍| 270/285 [26:02<01:28,  5.88s/it] 95%|█████████▌| 271/285 [26:08<01:23,  5.95s/it] 95%|█████████▌| 272/285 [26:14<01:17,  5.98s/it] 96%|█████████▌| 273/285 [26:20<01:12,  6.02s/it] 96%|█████████▌| 274/285 [26:26<01:05,  5.91s/it] 96%|█████████▋| 275/285 [26:32<00:58,  5.88s/it] 97%|█████████▋| 276/285 [26:38<00:53,  5.89s/it] 97%|█████████▋| 277/285 [26:44<00:47,  5.96s/it] 98%|█████████▊| 278/285 [26:50<00:42,  6.04s/it] 98%|█████████▊| 279/285 [26:56<00:36,  6.09s/it] 98%|█████████▊| 280/285 [27:03<00:30,  6.15s/it] 99%|█████████▊| 281/285 [27:08<00:24,  6.09s/it] 99%|█████████▉| 282/285 [27:14<00:18,  6.04s/it] 99%|█████████▉| 283/285 [27:21<00:12,  6.16s/it]100%|█████████▉| 284/285 [27:27<00:06,  6.13s/it]100%|██████████| 285/285 [27:33<00:00,  6.09s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [27:33<00:00,  6.09s/it]100%|██████████| 285/285 [27:33<00:00,  5.80s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652359063
{'loss': 0.4603, 'learning_rate': 9.950000000000001e-06, 'epoch': 3.5}
{'train_runtime': 1653.4043, 'train_samples_per_second': 11.171, 'train_steps_per_second': 0.172, 'train_loss': 0.35049673046982077, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:08<38:22,  8.11s/it]  1%|          | 2/285 [00:15<37:33,  7.96s/it]  1%|          | 3/285 [00:24<37:57,  8.08s/it]  1%|▏         | 4/285 [00:32<38:42,  8.26s/it]  2%|▏         | 5/285 [00:41<39:05,  8.38s/it]  2%|▏         | 6/285 [00:49<39:03,  8.40s/it]  2%|▏         | 7/285 [00:57<38:22,  8.28s/it]  3%|▎         | 8/285 [01:05<37:49,  8.19s/it]  3%|▎         | 9/285 [01:14<37:51,  8.23s/it]  4%|▎         | 10/285 [01:22<38:11,  8.33s/it]  4%|▍         | 11/285 [01:31<38:03,  8.33s/it]  4%|▍         | 12/285 [01:40<38:55,  8.55s/it]  5%|▍         | 13/285 [01:48<38:41,  8.54s/it]  5%|▍         | 14/285 [01:56<38:20,  8.49s/it]  5%|▌         | 15/285 [02:04<37:24,  8.31s/it]  6%|▌         | 16/285 [02:12<36:32,  8.15s/it]  6%|▌         | 17/285 [02:21<36:51,  8.25s/it]  6%|▋         | 18/285 [02:29<36:48,  8.27s/it]  7%|▋         | 19/285 [02:37<36:44,  8.29s/it]  7%|▋         | 20/285 [02:46<36:35,  8.29s/it]  7%|▋         | 21/285 [02:54<36:26,  8.28s/it]  8%|▊         | 22/285 [03:02<36:21,  8.29s/it]  8%|▊         | 23/285 [03:10<35:44,  8.19s/it]  8%|▊         | 24/285 [03:18<35:28,  8.16s/it]  9%|▉         | 25/285 [03:26<35:20,  8.16s/it]  9%|▉         | 26/285 [03:35<35:25,  8.21s/it]  9%|▉         | 27/285 [03:43<35:28,  8.25s/it] 10%|▉         | 28/285 [03:51<35:26,  8.28s/it] 10%|█         | 29/285 [04:00<35:33,  8.33s/it] 11%|█         | 30/285 [04:08<35:25,  8.33s/it] 11%|█         | 31/285 [04:17<35:41,  8.43s/it] 11%|█         | 32/285 [04:26<36:28,  8.65s/it] 12%|█▏        | 33/285 [04:34<35:47,  8.52s/it] 12%|█▏        | 34/285 [04:42<35:08,  8.40s/it] 12%|█▏        | 35/285 [04:51<35:04,  8.42s/it] 13%|█▎        | 36/285 [04:59<34:46,  8.38s/it] 13%|█▎        | 37/285 [05:08<34:56,  8.46s/it] 13%|█▎        | 38/285 [05:15<33:47,  8.21s/it] 14%|█▎        | 39/285 [05:24<33:50,  8.25s/it] 14%|█▍        | 40/285 [05:32<33:50,  8.29s/it] 14%|█▍        | 41/285 [05:40<33:27,  8.23s/it] 15%|█▍        | 42/285 [05:48<33:10,  8.19s/it] 15%|█▌        | 43/285 [05:57<33:12,  8.23s/it] 15%|█▌        | 44/285 [06:05<33:07,  8.25s/it] 16%|█▌        | 45/285 [06:13<32:24,  8.10s/it] 16%|█▌        | 46/285 [06:22<33:29,  8.41s/it] 16%|█▋        | 47/285 [06:30<32:47,  8.27s/it] 17%|█▋        | 48/285 [06:38<33:12,  8.41s/it] 17%|█▋        | 49/285 [06:47<32:50,  8.35s/it] 18%|█▊        | 50/285 [06:55<32:39,  8.34s/it] 18%|█▊        | 51/285 [07:04<32:53,  8.43s/it] 18%|█▊        | 52/285 [07:12<32:19,  8.33s/it] 19%|█▊        | 53/285 [07:20<31:56,  8.26s/it] 19%|█▉        | 54/285 [07:28<31:31,  8.19s/it] 19%|█▉        | 55/285 [07:36<31:22,  8.19s/it] 20%|█▉        | 56/285 [07:44<31:17,  8.20s/it] 20%|██        | 57/285 [07:52<31:01,  8.17s/it] 20%|██        | 58/285 [08:07<38:09, 10.09s/it] 21%|██        | 59/285 [08:15<36:04,  9.58s/it] 21%|██        | 60/285 [08:23<34:23,  9.17s/it] 21%|██▏       | 61/285 [08:32<33:02,  8.85s/it] 22%|██▏       | 62/285 [08:39<31:25,  8.45s/it] 22%|██▏       | 63/285 [08:47<31:06,  8.41s/it] 22%|██▏       | 64/285 [08:56<30:57,  8.41s/it] 23%|██▎       | 65/285 [09:04<30:38,  8.36s/it] 23%|██▎       | 66/285 [09:12<29:57,  8.21s/it] 24%|██▎       | 67/285 [09:20<29:52,  8.22s/it] 24%|██▍       | 68/285 [09:29<30:07,  8.33s/it] 24%|██▍       | 69/285 [09:36<29:00,  8.06s/it] 25%|██▍       | 70/285 [09:44<28:37,  7.99s/it] 25%|██▍       | 71/285 [09:52<28:46,  8.07s/it] 25%|██▌       | 72/285 [10:00<28:41,  8.08s/it] 26%|██▌       | 73/285 [10:08<28:02,  7.93s/it] 26%|██▌       | 74/285 [10:16<27:54,  7.94s/it] 26%|██▋       | 75/285 [10:24<28:13,  8.06s/it] 27%|██▋       | 76/285 [10:33<28:44,  8.25s/it] 27%|██▋       | 77/285 [10:41<28:20,  8.18s/it] 27%|██▋       | 78/285 [10:49<28:15,  8.19s/it] 28%|██▊       | 79/285 [10:58<28:32,  8.31s/it] 28%|██▊       | 80/285 [11:06<28:26,  8.33s/it] 28%|██▊       | 81/285 [11:14<27:51,  8.20s/it] 29%|██▉       | 82/285 [11:22<27:39,  8.17s/it] 29%|██▉       | 83/285 [11:30<27:27,  8.15s/it] 29%|██▉       | 84/285 [11:38<27:21,  8.17s/it] 30%|██▉       | 85/285 [11:46<26:38,  7.99s/it] 30%|███       | 86/285 [11:54<26:52,  8.10s/it] 31%|███       | 87/285 [12:02<26:25,  8.01s/it] 31%|███       | 88/285 [12:10<26:28,  8.06s/it] 31%|███       | 89/285 [12:18<26:19,  8.06s/it] 32%|███▏      | 90/285 [12:27<26:24,  8.12s/it] 32%|███▏      | 91/285 [12:35<26:18,  8.14s/it] 32%|███▏      | 92/285 [12:43<25:43,  8.00s/it] 33%|███▎      | 93/285 [12:51<25:39,  8.02s/it] 33%|███▎      | 94/285 [12:59<25:50,  8.12s/it] 33%|███▎      | 95/285 [13:07<25:30,  8.06s/it] 34%|███▎      | 96/285 [13:15<25:25,  8.07s/it] 34%|███▍      | 97/285 [13:23<25:24,  8.11s/it] 34%|███▍      | 98/285 [13:31<25:03,  8.04s/it] 35%|███▍      | 99/285 [13:39<24:25,  7.88s/it] 35%|███▌      | 100/285 [13:46<24:20,  7.90s/it] 35%|███▌      | 101/285 [13:55<24:52,  8.11s/it] 36%|███▌      | 102/285 [14:03<24:26,  8.02s/it] 36%|███▌      | 103/285 [14:11<24:08,  7.96s/it] 36%|███▋      | 104/285 [14:19<24:16,  8.05s/it] 37%|███▋      | 105/285 [14:27<24:12,  8.07s/it] 37%|███▋      | 106/285 [14:36<24:48,  8.31s/it] 38%|███▊      | 107/285 [14:44<24:49,  8.37s/it] 38%|███▊      | 108/285 [14:53<24:40,  8.36s/it] 38%|███▊      | 109/285 [15:01<24:23,  8.31s/it] 39%|███▊      | 110/285 [15:09<24:13,  8.30s/it] 39%|███▉      | 111/285 [15:18<24:06,  8.31s/it] 39%|███▉      | 112/285 [15:26<24:01,  8.33s/it] 40%|███▉      | 113/285 [15:34<23:27,  8.19s/it] 40%|████      | 114/285 [15:42<23:33,  8.26s/it] 40%|████      | 115/285 [15:57<28:40, 10.12s/it] 41%|████      | 116/285 [16:05<26:56,  9.57s/it] 41%|████      | 117/285 [16:13<25:39,  9.16s/it] 41%|████▏     | 118/285 [16:21<24:26,  8.78s/it] 42%|████▏     | 119/285 [16:29<23:39,  8.55s/it] 42%|████▏     | 120/285 [16:37<23:21,  8.50s/it] 42%|████▏     | 121/285 [16:45<22:41,  8.30s/it] 43%|████▎     | 122/285 [16:53<22:18,  8.21s/it] 43%|████▎     | 123/285 [17:02<22:20,  8.28s/it] 44%|████▎     | 124/285 [17:10<21:57,  8.18s/it] 44%|████▍     | 125/285 [17:18<21:59,  8.25s/it] 44%|████▍     | 126/285 [17:27<22:07,  8.35s/it] 45%|████▍     | 127/285 [17:35<21:55,  8.32s/it] 45%|████▍     | 128/285 [17:43<21:40,  8.28s/it] 45%|████▌     | 129/285 [17:51<21:17,  8.19s/it] 46%|████▌     | 130/285 [17:59<20:58,  8.12s/it] 46%|████▌     | 131/285 [18:08<21:13,  8.27s/it] 46%|████▋     | 132/285 [18:16<21:10,  8.30s/it] 47%|████▋     | 133/285 [18:24<20:53,  8.25s/it] 47%|████▋     | 134/285 [18:33<21:02,  8.36s/it] 47%|████▋     | 135/285 [18:41<20:23,  8.16s/it] 48%|████▊     | 136/285 [18:48<19:59,  8.05s/it] 48%|████▊     | 137/285 [18:57<19:59,  8.11s/it] 48%|████▊     | 138/285 [19:05<20:01,  8.17s/it] 49%|████▉     | 139/285 [19:13<19:42,  8.10s/it] 49%|████▉     | 140/285 [19:21<19:30,  8.07s/it] 49%|████▉     | 141/285 [19:29<19:33,  8.15s/it] 50%|████▉     | 142/285 [19:37<19:28,  8.17s/it] 50%|█████     | 143/285 [19:45<18:59,  8.03s/it] 51%|█████     | 144/285 [19:54<19:26,  8.27s/it] 51%|█████     | 145/285 [20:03<19:38,  8.42s/it] 51%|█████     | 146/285 [20:11<19:13,  8.30s/it] 52%|█████▏    | 147/285 [20:19<18:47,  8.17s/it] 52%|█████▏    | 148/285 [20:27<18:34,  8.14s/it] 52%|█████▏    | 149/285 [20:35<18:42,  8.26s/it] 53%|█████▎    | 150/285 [20:44<18:44,  8.33s/it] 53%|█████▎    | 151/285 [20:51<18:07,  8.11s/it] 53%|█████▎    | 152/285 [20:59<17:48,  8.03s/it] 54%|█████▎    | 153/285 [21:07<17:29,  7.95s/it] 54%|█████▍    | 154/285 [21:15<17:39,  8.09s/it] 54%|█████▍    | 155/285 [21:24<17:39,  8.15s/it] 55%|█████▍    | 156/285 [21:32<17:38,  8.20s/it] 55%|█████▌    | 157/285 [21:40<17:34,  8.24s/it] 55%|█████▌    | 158/285 [21:48<17:21,  8.20s/it] 56%|█████▌    | 159/285 [21:57<17:27,  8.31s/it] 56%|█████▌    | 160/285 [22:05<17:09,  8.24s/it] 56%|█████▋    | 161/285 [22:14<17:16,  8.36s/it] 57%|█████▋    | 162/285 [22:22<17:03,  8.32s/it] 57%|█████▋    | 163/285 [22:30<16:46,  8.25s/it] 58%|█████▊    | 164/285 [22:38<16:21,  8.11s/it] 58%|█████▊    | 165/285 [22:47<16:50,  8.42s/it] 58%|█████▊    | 166/285 [22:55<16:30,  8.33s/it] 59%|█████▊    | 167/285 [23:03<16:24,  8.35s/it] 59%|█████▉    | 168/285 [23:11<16:04,  8.24s/it] 59%|█████▉    | 169/285 [23:19<15:50,  8.19s/it] 60%|█████▉    | 170/285 [23:28<15:46,  8.23s/it] 60%|██████    | 171/285 [23:36<15:34,  8.20s/it] 60%|██████    | 172/285 [23:49<18:18,  9.72s/it] 61%|██████    | 173/285 [23:57<17:15,  9.24s/it] 61%|██████    | 174/285 [24:06<16:40,  9.01s/it] 61%|██████▏   | 175/285 [24:14<16:08,  8.80s/it] 62%|██████▏   | 176/285 [24:22<15:36,  8.59s/it] 62%|██████▏   | 177/285 [24:31<15:26,  8.58s/it] 62%|██████▏   | 178/285 [24:39<14:59,  8.41s/it] 63%|██████▎   | 179/285 [24:47<14:36,  8.27s/it] 63%|██████▎   | 180/285 [24:55<14:16,  8.16s/it] 64%|██████▎   | 181/285 [25:04<14:38,  8.45s/it] 64%|██████▍   | 182/285 [25:12<14:23,  8.39s/it] 64%|██████▍   | 183/285 [25:20<14:12,  8.35s/it] 65%|██████▍   | 184/285 [25:28<13:52,  8.24s/it] 65%|██████▍   | 185/285 [25:37<14:00,  8.41s/it] 65%|██████▌   | 186/285 [25:46<13:56,  8.45s/it] 66%|██████▌   | 187/285 [25:54<13:42,  8.39s/it] 66%|██████▌   | 188/285 [26:02<13:28,  8.34s/it] 66%|██████▋   | 189/285 [26:10<13:24,  8.38s/it] 67%|██████▋   | 190/285 [26:18<13:01,  8.23s/it] 67%|██████▋   | 191/285 [26:27<12:59,  8.29s/it] 67%|██████▋   | 192/285 [26:35<12:47,  8.26s/it] 68%|██████▊   | 193/285 [26:43<12:33,  8.19s/it] 68%|██████▊   | 194/285 [26:51<12:21,  8.15s/it] 68%|██████▊   | 195/285 [27:00<12:31,  8.35s/it] 69%|██████▉   | 196/285 [27:08<12:13,  8.24s/it] 69%|██████▉   | 197/285 [27:16<12:08,  8.28s/it] 69%|██████▉   | 198/285 [27:25<12:01,  8.29s/it] 70%|██████▉   | 199/285 [27:32<11:41,  8.16s/it] 70%|███████   | 200/285 [27:41<11:49,  8.34s/it]                                                  70%|███████   | 200/285 [27:41<11:49,  8.34s/it] 71%|███████   | 201/285 [27:50<11:49,  8.45s/it] 71%|███████   | 202/285 [27:58<11:28,  8.29s/it] 71%|███████   | 203/285 [28:06<11:25,  8.36s/it] 72%|███████▏  | 204/285 [28:15<11:25,  8.47s/it] 72%|███████▏  | 205/285 [28:24<11:17,  8.47s/it] 72%|███████▏  | 206/285 [28:31<10:54,  8.28s/it] 73%|███████▎  | 207/285 [28:40<10:45,  8.27s/it] 73%|███████▎  | 208/285 [28:47<10:20,  8.06s/it] 73%|███████▎  | 209/285 [28:55<10:07,  7.99s/it] 74%|███████▎  | 210/285 [29:03<09:55,  7.94s/it] 74%|███████▍  | 211/285 [29:11<09:53,  8.02s/it] 74%|███████▍  | 212/285 [29:19<09:43,  7.99s/it] 75%|███████▍  | 213/285 [29:27<09:42,  8.09s/it] 75%|███████▌  | 214/285 [29:35<09:31,  8.05s/it] 75%|███████▌  | 215/285 [29:43<09:25,  8.07s/it] 76%|███████▌  | 216/285 [29:51<09:16,  8.06s/it] 76%|███████▌  | 217/285 [30:00<09:09,  8.08s/it] 76%|███████▋  | 218/285 [30:08<08:59,  8.05s/it] 77%|███████▋  | 219/285 [30:16<08:53,  8.09s/it] 77%|███████▋  | 220/285 [30:24<08:56,  8.25s/it] 78%|███████▊  | 221/285 [30:33<08:55,  8.37s/it] 78%|███████▊  | 222/285 [30:41<08:36,  8.19s/it] 78%|███████▊  | 223/285 [30:49<08:27,  8.19s/it] 79%|███████▊  | 224/285 [30:58<08:32,  8.39s/it] 79%|███████▉  | 225/285 [31:06<08:16,  8.27s/it] 79%|███████▉  | 226/285 [31:14<08:11,  8.32s/it] 80%|███████▉  | 227/285 [31:23<08:02,  8.32s/it] 80%|████████  | 228/285 [31:31<07:51,  8.27s/it] 80%|████████  | 229/285 [31:45<09:20, 10.02s/it] 81%|████████  | 230/285 [31:53<08:46,  9.57s/it] 81%|████████  | 231/285 [32:02<08:18,  9.23s/it] 81%|████████▏ | 232/285 [32:10<07:49,  8.86s/it] 82%|████████▏ | 233/285 [32:18<07:32,  8.69s/it] 82%|████████▏ | 234/285 [32:26<07:14,  8.51s/it] 82%|████████▏ | 235/285 [32:34<06:58,  8.36s/it] 83%|████████▎ | 236/285 [32:42<06:44,  8.27s/it] 83%|████████▎ | 237/285 [32:50<06:31,  8.15s/it] 84%|████████▎ | 238/285 [32:58<06:25,  8.21s/it] 84%|████████▍ | 239/285 [33:06<06:15,  8.15s/it] 84%|████████▍ | 240/285 [33:15<06:09,  8.21s/it] 85%|████████▍ | 241/285 [33:23<06:03,  8.26s/it] 85%|████████▍ | 242/285 [33:31<05:52,  8.20s/it] 85%|████████▌ | 243/285 [33:39<05:36,  8.01s/it] 86%|████████▌ | 244/285 [33:47<05:25,  7.95s/it] 86%|████████▌ | 245/285 [33:55<05:22,  8.07s/it] 86%|████████▋ | 246/285 [34:03<05:11,  7.99s/it] 87%|████████▋ | 247/285 [34:11<05:10,  8.18s/it] 87%|████████▋ | 248/285 [34:20<05:02,  8.17s/it] 87%|████████▋ | 249/285 [34:28<04:59,  8.31s/it] 88%|████████▊ | 250/285 [34:36<04:50,  8.30s/it] 88%|████████▊ | 251/285 [34:44<04:39,  8.22s/it] 88%|████████▊ | 252/285 [34:53<04:35,  8.34s/it] 89%|████████▉ | 253/285 [35:01<04:23,  8.23s/it] 89%|████████▉ | 254/285 [35:10<04:18,  8.33s/it] 89%|████████▉ | 255/285 [35:18<04:10,  8.36s/it] 90%|████████▉ | 256/285 [35:26<03:59,  8.25s/it] 90%|█████████ | 257/285 [35:34<03:47,  8.14s/it] 91%|█████████ | 258/285 [35:42<03:39,  8.12s/it] 91%|█████████ | 259/285 [35:50<03:31,  8.12s/it] 91%|█████████ | 260/285 [35:58<03:22,  8.09s/it] 92%|█████████▏| 261/285 [36:07<03:16,  8.18s/it] 92%|█████████▏| 262/285 [36:15<03:06,  8.12s/it] 92%|█████████▏| 263/285 [36:23<02:59,  8.17s/it] 93%|█████████▎| 264/285 [36:31<02:52,  8.21s/it] 93%|█████████▎| 265/285 [36:39<02:41,  8.08s/it] 93%|█████████▎| 266/285 [36:47<02:33,  8.10s/it] 94%|█████████▎| 267/285 [36:55<02:26,  8.14s/it] 94%|█████████▍| 268/285 [37:04<02:18,  8.16s/it] 94%|█████████▍| 269/285 [37:12<02:13,  8.33s/it] 95%|█████████▍| 270/285 [37:20<02:03,  8.25s/it] 95%|█████████▌| 271/285 [37:29<01:56,  8.29s/it] 95%|█████████▌| 272/285 [37:37<01:47,  8.30s/it] 96%|█████████▌| 273/285 [37:45<01:38,  8.22s/it] 96%|█████████▌| 274/285 [37:53<01:30,  8.19s/it] 96%|█████████▋| 275/285 [38:01<01:21,  8.11s/it] 97%|█████████▋| 276/285 [38:09<01:12,  8.01s/it] 97%|█████████▋| 277/285 [38:17<01:04,  8.07s/it] 98%|█████████▊| 278/285 [38:25<00:56,  8.06s/it] 98%|█████████▊| 279/285 [38:33<00:48,  8.08s/it] 98%|█████████▊| 280/285 [38:42<00:41,  8.21s/it] 99%|█████████▊| 281/285 [38:50<00:32,  8.15s/it] 99%|█████████▉| 282/285 [38:58<00:24,  8.16s/it] 99%|█████████▉| 283/285 [39:06<00:16,  8.20s/it]100%|█████████▉| 284/285 [39:15<00:08,  8.30s/it]100%|██████████| 285/285 [39:23<00:00,  8.13s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [39:23<00:00,  8.13s/it]100%|██████████| 285/285 [39:23<00:00,  8.29s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652356503
{'loss': 0.2753, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 2363.0046, 'train_samples_per_second': 7.816, 'train_steps_per_second': 0.121, 'train_loss': 0.19999463223574454, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:08<41:47,  8.83s/it]  1%|          | 2/285 [00:17<41:34,  8.81s/it]  1%|          | 3/285 [00:26<41:59,  8.94s/it]  1%|▏         | 4/285 [00:35<42:12,  9.01s/it]  2%|▏         | 5/285 [00:45<42:21,  9.08s/it]  2%|▏         | 6/285 [00:54<42:10,  9.07s/it]  2%|▏         | 7/285 [01:02<41:27,  8.95s/it]  3%|▎         | 8/285 [01:11<40:39,  8.81s/it]  3%|▎         | 9/285 [01:20<40:25,  8.79s/it]  4%|▎         | 10/285 [01:29<40:49,  8.91s/it]  4%|▍         | 11/285 [01:38<40:38,  8.90s/it]  4%|▍         | 12/285 [01:47<41:16,  9.07s/it]  5%|▍         | 13/285 [01:56<40:50,  9.01s/it]  5%|▍         | 14/285 [02:05<41:00,  9.08s/it]  5%|▌         | 15/285 [02:14<40:32,  9.01s/it]  6%|▌         | 16/285 [02:23<39:49,  8.88s/it]  6%|▌         | 17/285 [02:32<40:15,  9.01s/it]  6%|▋         | 18/285 [02:41<40:15,  9.05s/it]  7%|▋         | 19/285 [02:50<40:11,  9.07s/it]  7%|▋         | 20/285 [02:59<40:24,  9.15s/it]  7%|▋         | 21/285 [03:09<40:04,  9.11s/it]  8%|▊         | 22/285 [03:18<40:00,  9.13s/it]  8%|▊         | 23/285 [03:26<39:07,  8.96s/it]  8%|▊         | 24/285 [03:35<38:37,  8.88s/it]  9%|▉         | 25/285 [03:44<38:25,  8.87s/it]  9%|▉         | 26/285 [03:53<38:27,  8.91s/it]  9%|▉         | 27/285 [04:02<38:19,  8.91s/it] 10%|▉         | 28/285 [04:11<38:14,  8.93s/it] 10%|█         | 29/285 [04:19<37:52,  8.88s/it] 11%|█         | 30/285 [04:29<38:13,  8.99s/it] 11%|█         | 31/285 [04:38<38:13,  9.03s/it] 11%|█         | 32/285 [04:47<38:41,  9.18s/it] 12%|█▏        | 33/285 [04:56<37:52,  9.02s/it] 12%|█▏        | 34/285 [05:04<36:58,  8.84s/it] 12%|█▏        | 35/285 [05:13<37:04,  8.90s/it] 13%|█▎        | 36/285 [05:22<36:40,  8.84s/it] 13%|█▎        | 37/285 [05:31<37:04,  8.97s/it] 13%|█▎        | 38/285 [05:40<36:25,  8.85s/it] 14%|█▎        | 39/285 [05:49<36:07,  8.81s/it] 14%|█▍        | 40/285 [05:57<35:55,  8.80s/it] 14%|█▍        | 41/285 [06:06<35:25,  8.71s/it] 15%|█▍        | 42/285 [06:15<35:33,  8.78s/it] 15%|█▌        | 43/285 [06:24<35:57,  8.92s/it] 15%|█▌        | 44/285 [06:33<35:29,  8.84s/it] 16%|█▌        | 45/285 [06:41<35:01,  8.76s/it] 16%|█▌        | 46/285 [06:51<36:03,  9.05s/it] 16%|█▋        | 47/285 [07:00<36:04,  9.09s/it] 17%|█▋        | 48/285 [07:10<37:11,  9.41s/it] 17%|█▋        | 49/285 [07:20<36:54,  9.39s/it] 18%|█▊        | 50/285 [07:29<36:52,  9.41s/it] 18%|█▊        | 51/285 [07:39<37:05,  9.51s/it] 18%|█▊        | 52/285 [07:48<36:18,  9.35s/it] 19%|█▊        | 53/285 [07:57<35:50,  9.27s/it] 19%|█▉        | 54/285 [08:06<35:11,  9.14s/it] 19%|█▉        | 55/285 [08:15<35:18,  9.21s/it] 20%|█▉        | 56/285 [08:25<35:26,  9.29s/it] 20%|██        | 57/285 [08:34<35:41,  9.39s/it] 20%|██        | 58/285 [08:50<42:56, 11.35s/it] 21%|██        | 59/285 [09:00<40:23, 10.72s/it] 21%|██        | 60/285 [09:09<38:23, 10.24s/it] 21%|██▏       | 61/285 [09:18<36:47,  9.85s/it] 22%|██▏       | 62/285 [09:26<34:59,  9.41s/it] 22%|██▏       | 63/285 [09:35<34:45,  9.39s/it] 22%|██▏       | 64/285 [09:45<34:36,  9.40s/it] 23%|██▎       | 65/285 [09:54<34:17,  9.35s/it] 23%|██▎       | 66/285 [10:03<33:30,  9.18s/it] 24%|██▎       | 67/285 [10:12<33:21,  9.18s/it] 24%|██▍       | 68/285 [10:21<33:32,  9.28s/it] 24%|██▍       | 69/285 [10:30<32:24,  9.00s/it] 25%|██▍       | 70/285 [10:39<32:00,  8.93s/it] 25%|██▍       | 71/285 [10:48<32:01,  8.98s/it] 25%|██▌       | 72/285 [10:57<31:53,  8.98s/it] 26%|██▌       | 73/285 [11:05<31:08,  8.81s/it] 26%|██▌       | 74/285 [11:14<30:50,  8.77s/it] 26%|██▋       | 75/285 [11:23<31:07,  8.89s/it] 27%|██▋       | 76/285 [11:32<31:27,  9.03s/it] 27%|██▋       | 77/285 [11:41<30:56,  8.92s/it] 27%|██▋       | 78/285 [11:50<30:42,  8.90s/it] 28%|██▊       | 79/285 [11:59<31:12,  9.09s/it] 28%|██▊       | 80/285 [12:09<31:23,  9.19s/it] 28%|██▊       | 81/285 [12:18<30:47,  9.06s/it] 29%|██▉       | 82/285 [12:26<30:33,  9.03s/it] 29%|██▉       | 83/285 [12:35<30:17,  9.00s/it] 29%|██▉       | 84/285 [12:44<30:07,  8.99s/it] 30%|██▉       | 85/285 [12:53<29:22,  8.81s/it] 30%|███       | 86/285 [13:02<29:35,  8.92s/it] 31%|███       | 87/285 [13:11<29:09,  8.84s/it] 31%|███       | 88/285 [13:20<29:16,  8.92s/it] 31%|███       | 89/285 [13:29<29:08,  8.92s/it] 32%|███▏      | 90/285 [13:38<29:13,  8.99s/it] 32%|███▏      | 91/285 [13:47<29:05,  9.00s/it] 32%|███▏      | 92/285 [13:55<28:24,  8.83s/it] 33%|███▎      | 93/285 [14:04<28:11,  8.81s/it] 33%|███▎      | 94/285 [14:13<28:15,  8.88s/it] 33%|███▎      | 95/285 [14:22<28:05,  8.87s/it] 34%|███▎      | 96/285 [14:31<27:58,  8.88s/it] 34%|███▍      | 97/285 [14:40<27:55,  8.91s/it] 34%|███▍      | 98/285 [14:49<27:41,  8.89s/it] 35%|███▍      | 99/285 [14:57<27:24,  8.84s/it] 35%|███▌      | 100/285 [15:06<27:29,  8.92s/it] 35%|███▌      | 101/285 [15:16<28:02,  9.14s/it] 36%|███▌      | 102/285 [15:25<27:44,  9.10s/it] 36%|███▌      | 103/285 [15:34<27:31,  9.08s/it] 36%|███▋      | 104/285 [15:43<27:33,  9.14s/it] 37%|███▋      | 105/285 [15:52<27:10,  9.06s/it] 37%|███▋      | 106/285 [16:02<27:20,  9.17s/it] 38%|███▊      | 107/285 [16:11<27:20,  9.22s/it] 38%|███▊      | 108/285 [16:20<27:21,  9.27s/it] 38%|███▊      | 109/285 [16:29<26:40,  9.10s/it] 39%|███▊      | 110/285 [16:38<26:27,  9.07s/it] 39%|███▉      | 111/285 [16:47<26:28,  9.13s/it] 39%|███▉      | 112/285 [16:57<26:31,  9.20s/it] 40%|███▉      | 113/285 [17:06<26:49,  9.36s/it] 40%|████      | 114/285 [17:17<27:26,  9.63s/it] 40%|████      | 115/285 [17:34<33:46, 11.92s/it] 41%|████      | 116/285 [17:44<31:48, 11.29s/it] 41%|████      | 117/285 [17:53<29:42, 10.61s/it] 41%|████▏     | 118/285 [18:02<27:54, 10.03s/it] 42%|████▏     | 119/285 [18:10<26:36,  9.62s/it] 42%|████▏     | 120/285 [18:19<26:05,  9.49s/it] 42%|████▏     | 121/285 [18:28<25:16,  9.25s/it] 43%|████▎     | 122/285 [18:37<24:44,  9.10s/it] 43%|████▎     | 123/285 [18:46<24:54,  9.22s/it] 44%|████▎     | 124/285 [18:56<24:53,  9.28s/it] 44%|████▍     | 125/285 [19:05<25:02,  9.39s/it] 44%|████▍     | 126/285 [19:15<25:08,  9.49s/it] 45%|████▍     | 127/285 [19:24<24:45,  9.40s/it] 45%|████▍     | 128/285 [19:33<24:19,  9.30s/it] 45%|████▌     | 129/285 [19:42<23:46,  9.15s/it] 46%|████▌     | 130/285 [19:51<23:20,  9.04s/it] 46%|████▌     | 131/285 [20:00<23:26,  9.14s/it] 46%|████▋     | 132/285 [20:09<23:12,  9.10s/it] 47%|████▋     | 133/285 [20:18<22:55,  9.05s/it] 47%|████▋     | 134/285 [20:28<23:00,  9.14s/it] 47%|████▋     | 135/285 [20:36<22:25,  8.97s/it] 48%|████▊     | 136/285 [20:45<22:02,  8.88s/it] 48%|████▊     | 137/285 [20:54<22:00,  8.92s/it] 48%|████▊     | 138/285 [21:03<22:00,  8.98s/it] 49%|████▉     | 139/285 [21:12<21:39,  8.90s/it] 49%|████▉     | 140/285 [21:21<21:30,  8.90s/it] 49%|████▉     | 141/285 [21:30<21:30,  8.96s/it] 50%|████▉     | 142/285 [21:39<21:25,  8.99s/it] 50%|█████     | 143/285 [21:48<21:11,  8.95s/it] 51%|█████     | 144/285 [21:58<21:41,  9.23s/it] 51%|█████     | 145/285 [22:07<21:45,  9.32s/it] 51%|█████     | 146/285 [22:16<21:13,  9.17s/it] 52%|█████▏    | 147/285 [22:24<20:39,  8.99s/it] 52%|█████▏    | 148/285 [22:33<20:20,  8.91s/it] 52%|█████▏    | 149/285 [22:42<20:27,  9.03s/it] 53%|█████▎    | 150/285 [22:52<20:25,  9.08s/it] 53%|█████▎    | 151/285 [23:00<19:46,  8.85s/it] 53%|█████▎    | 152/285 [23:09<19:29,  8.79s/it] 54%|█████▎    | 153/285 [23:17<19:11,  8.72s/it] 54%|█████▍    | 154/285 [23:26<19:21,  8.87s/it] 54%|█████▍    | 155/285 [23:35<19:18,  8.91s/it] 55%|█████▍    | 156/285 [23:44<19:13,  8.94s/it] 55%|█████▌    | 157/285 [23:53<19:07,  8.96s/it] 55%|█████▌    | 158/285 [24:02<18:54,  8.93s/it] 56%|█████▌    | 159/285 [24:12<19:13,  9.16s/it] 56%|█████▌    | 160/285 [24:21<18:50,  9.04s/it] 56%|█████▋    | 161/285 [24:30<18:55,  9.16s/it] 57%|█████▋    | 162/285 [24:39<18:31,  9.04s/it] 57%|█████▋    | 163/285 [24:48<18:14,  8.97s/it] 58%|█████▊    | 164/285 [24:56<17:47,  8.82s/it] 58%|█████▊    | 165/285 [25:06<18:18,  9.15s/it] 58%|█████▊    | 166/285 [25:15<17:53,  9.02s/it] 59%|█████▊    | 167/285 [25:24<17:43,  9.01s/it] 59%|█████▉    | 168/285 [25:32<17:20,  8.90s/it] 59%|█████▉    | 169/285 [25:41<17:06,  8.85s/it] 60%|█████▉    | 170/285 [25:50<17:00,  8.88s/it] 60%|██████    | 171/285 [25:59<16:49,  8.85s/it] 60%|██████    | 172/285 [26:13<19:49, 10.53s/it] 61%|██████    | 173/285 [26:22<18:39, 10.00s/it] 61%|██████    | 174/285 [26:31<18:06,  9.78s/it] 61%|██████▏   | 175/285 [26:41<17:36,  9.61s/it] 62%|██████▏   | 176/285 [26:50<17:06,  9.41s/it] 62%|██████▏   | 177/285 [26:59<16:48,  9.34s/it] 62%|██████▏   | 178/285 [27:07<16:20,  9.17s/it] 63%|██████▎   | 179/285 [27:16<16:04,  9.10s/it] 63%|██████▎   | 180/285 [27:25<15:43,  8.99s/it] 64%|██████▎   | 181/285 [27:35<15:55,  9.19s/it] 64%|██████▍   | 182/285 [27:44<15:36,  9.09s/it] 64%|██████▍   | 183/285 [27:53<15:20,  9.02s/it] 65%|██████▍   | 184/285 [28:01<14:56,  8.87s/it] 65%|██████▍   | 185/285 [28:11<15:05,  9.05s/it] 65%|██████▌   | 186/285 [28:20<15:01,  9.11s/it] 66%|██████▌   | 187/285 [28:29<14:50,  9.09s/it] 66%|██████▌   | 188/285 [28:38<14:36,  9.04s/it] 66%|██████▋   | 189/285 [28:47<14:31,  9.08s/it] 67%|██████▋   | 190/285 [28:56<14:08,  8.93s/it] 67%|██████▋   | 191/285 [29:05<14:11,  9.06s/it] 67%|██████▋   | 192/285 [29:14<14:06,  9.11s/it] 68%|██████▊   | 193/285 [29:23<13:57,  9.10s/it] 68%|██████▊   | 194/285 [29:32<13:43,  9.05s/it] 68%|██████▊   | 195/285 [29:42<13:53,  9.27s/it] 69%|██████▉   | 196/285 [29:51<13:31,  9.12s/it] 69%|██████▉   | 197/285 [30:00<13:22,  9.12s/it] 69%|██████▉   | 198/285 [30:09<13:11,  9.09s/it] 70%|██████▉   | 199/285 [30:17<12:49,  8.94s/it] 70%|███████   | 200/285 [30:27<12:58,  9.16s/it]                                                  70%|███████   | 200/285 [30:27<12:58,  9.16s/it] 71%|███████   | 201/285 [30:37<13:03,  9.33s/it] 71%|███████   | 202/285 [30:45<12:34,  9.09s/it] 71%|███████   | 203/285 [30:54<12:26,  9.10s/it] 72%|███████▏  | 204/285 [31:04<12:24,  9.19s/it] 72%|███████▏  | 205/285 [31:13<12:13,  9.17s/it] 72%|███████▏  | 206/285 [31:21<11:48,  8.97s/it] 73%|███████▎  | 207/285 [31:31<11:52,  9.14s/it] 73%|███████▎  | 208/285 [31:40<11:34,  9.03s/it] 73%|███████▎  | 209/285 [31:49<11:23,  9.00s/it] 74%|███████▎  | 210/285 [31:58<11:14,  9.00s/it] 74%|███████▍  | 211/285 [32:07<11:11,  9.07s/it] 74%|███████▍  | 212/285 [32:16<10:53,  8.95s/it] 75%|███████▍  | 213/285 [32:25<10:49,  9.02s/it] 75%|███████▌  | 214/285 [32:34<10:35,  8.95s/it] 75%|███████▌  | 215/285 [32:43<10:27,  8.97s/it] 76%|███████▌  | 216/285 [32:51<10:17,  8.94s/it] 76%|███████▌  | 217/285 [33:00<10:09,  8.97s/it] 76%|███████▋  | 218/285 [33:09<09:55,  8.89s/it] 77%|███████▋  | 219/285 [33:18<09:41,  8.81s/it] 77%|███████▋  | 220/285 [33:27<09:42,  8.97s/it] 78%|███████▊  | 221/285 [33:37<09:42,  9.10s/it] 78%|███████▊  | 222/285 [33:45<09:21,  8.91s/it] 78%|███████▊  | 223/285 [33:54<09:12,  8.90s/it] 79%|███████▊  | 224/285 [34:03<09:15,  9.11s/it] 79%|███████▉  | 225/285 [34:13<09:05,  9.10s/it] 79%|███████▉  | 226/285 [34:22<09:06,  9.26s/it] 80%|███████▉  | 227/285 [34:32<08:59,  9.30s/it] 80%|████████  | 228/285 [34:41<08:48,  9.28s/it] 80%|████████  | 229/285 [34:56<10:26, 11.19s/it] 81%|████████  | 230/285 [35:06<09:46, 10.66s/it] 81%|████████  | 231/285 [35:15<09:12, 10.24s/it] 81%|████████▏ | 232/285 [35:24<08:34,  9.71s/it] 82%|████████▏ | 233/285 [35:32<08:08,  9.40s/it] 82%|████████▏ | 234/285 [35:41<07:50,  9.23s/it] 82%|████████▏ | 235/285 [35:50<07:35,  9.11s/it] 83%|████████▎ | 236/285 [35:59<07:20,  9.00s/it] 83%|████████▎ | 237/285 [36:07<07:04,  8.85s/it] 84%|████████▎ | 238/285 [36:16<07:00,  8.95s/it] 84%|████████▍ | 239/285 [36:25<06:49,  8.89s/it] 84%|████████▍ | 240/285 [36:35<06:48,  9.07s/it] 85%|████████▍ | 241/285 [36:44<06:41,  9.12s/it] 85%|████████▍ | 242/285 [36:53<06:26,  9.00s/it] 85%|████████▌ | 243/285 [37:01<06:09,  8.80s/it] 86%|████████▌ | 244/285 [37:09<05:56,  8.70s/it] 86%|████████▌ | 245/285 [37:19<05:53,  8.83s/it] 86%|████████▋ | 246/285 [37:27<05:42,  8.78s/it] 87%|████████▋ | 247/285 [37:37<05:40,  8.95s/it] 87%|████████▋ | 248/285 [37:45<05:30,  8.93s/it] 87%|████████▋ | 249/285 [37:55<05:26,  9.07s/it] 88%|████████▊ | 250/285 [38:04<05:17,  9.08s/it] 88%|████████▊ | 251/285 [38:13<05:03,  8.92s/it] 88%|████████▊ | 252/285 [38:22<04:58,  9.05s/it] 89%|████████▉ | 253/285 [38:31<04:46,  8.96s/it] 89%|████████▉ | 254/285 [38:40<04:41,  9.07s/it] 89%|████████▉ | 255/285 [38:49<04:33,  9.11s/it] 90%|████████▉ | 256/285 [38:58<04:23,  9.08s/it] 90%|█████████ | 257/285 [39:07<04:13,  9.05s/it] 91%|█████████ | 258/285 [39:17<04:13,  9.38s/it] 91%|█████████ | 259/285 [39:26<04:01,  9.29s/it] 91%|█████████ | 260/285 [39:35<03:49,  9.18s/it] 92%|█████████▏| 261/285 [39:45<03:42,  9.28s/it] 92%|█████████▏| 262/285 [39:54<03:29,  9.12s/it] 92%|█████████▏| 263/285 [40:03<03:21,  9.15s/it] 93%|█████████▎| 264/285 [40:12<03:11,  9.12s/it] 93%|█████████▎| 265/285 [40:20<02:59,  8.97s/it] 93%|█████████▎| 266/285 [40:29<02:50,  8.98s/it] 94%|█████████▎| 267/285 [40:39<02:42,  9.03s/it] 94%|█████████▍| 268/285 [40:48<02:33,  9.01s/it] 94%|█████████▍| 269/285 [40:57<02:25,  9.08s/it] 95%|█████████▍| 270/285 [41:06<02:14,  8.98s/it] 95%|█████████▌| 271/285 [41:15<02:06,  9.02s/it] 95%|█████████▌| 272/285 [41:24<01:59,  9.19s/it] 96%|█████████▌| 273/285 [41:34<01:50,  9.25s/it] 96%|█████████▌| 274/285 [41:43<01:41,  9.19s/it] 96%|█████████▋| 275/285 [41:51<01:30,  9.05s/it] 97%|█████████▋| 276/285 [42:00<01:19,  8.88s/it] 97%|█████████▋| 277/285 [42:09<01:11,  8.89s/it] 98%|█████████▊| 278/285 [42:18<01:01,  8.86s/it] 98%|█████████▊| 279/285 [42:27<00:53,  8.90s/it] 98%|█████████▊| 280/285 [42:36<00:45,  9.05s/it] 99%|█████████▊| 281/285 [42:45<00:35,  8.97s/it] 99%|█████████▉| 282/285 [42:54<00:26,  9.00s/it] 99%|█████████▉| 283/285 [43:03<00:18,  9.04s/it]100%|█████████▉| 284/285 [43:12<00:09,  9.14s/it]100%|██████████| 285/285 [43:21<00:00,  8.99s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [43:21<00:00,  8.99s/it]100%|██████████| 285/285 [43:21<00:00,  9.13s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'loss': 0.4041, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 2601.4684, 'train_samples_per_second': 7.1, 'train_steps_per_second': 0.11, 'train_loss': 0.2956205016688297, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:58,  6.11s/it]  5%|▌         | 2/40 [00:12<03:54,  6.16s/it]  8%|▊         | 3/40 [00:18<03:41,  5.98s/it] 10%|█         | 4/40 [00:23<03:32,  5.91s/it] 12%|█▎        | 5/40 [00:30<03:30,  6.01s/it] 15%|█▌        | 6/40 [00:35<03:20,  5.90s/it] 18%|█▊        | 7/40 [00:41<03:10,  5.77s/it] 20%|██        | 8/40 [00:47<03:07,  5.84s/it] 22%|██▎       | 9/40 [00:57<03:42,  7.16s/it] 25%|██▌       | 10/40 [01:03<03:25,  6.84s/it] 28%|██▊       | 11/40 [01:09<03:12,  6.62s/it] 30%|███       | 12/40 [01:15<02:59,  6.39s/it] 32%|███▎      | 13/40 [01:21<02:46,  6.18s/it] 35%|███▌      | 14/40 [01:27<02:39,  6.13s/it] 38%|███▊      | 15/40 [01:32<02:31,  6.04s/it] 40%|████      | 16/40 [01:38<02:23,  5.98s/it] 42%|████▎     | 17/40 [01:48<02:45,  7.21s/it] 45%|████▌     | 18/40 [01:54<02:28,  6.77s/it] 48%|████▊     | 19/40 [02:00<02:15,  6.47s/it] 50%|█████     | 20/40 [02:06<02:04,  6.24s/it] 52%|█████▎    | 21/40 [02:11<01:55,  6.09s/it] 55%|█████▌    | 22/40 [02:18<01:51,  6.17s/it] 57%|█████▊    | 23/40 [02:24<01:44,  6.16s/it] 60%|██████    | 24/40 [02:30<01:36,  6.04s/it] 62%|██████▎   | 25/40 [02:40<01:48,  7.25s/it] 65%|██████▌   | 26/40 [02:46<01:36,  6.86s/it] 68%|██████▊   | 27/40 [02:52<01:25,  6.60s/it] 70%|███████   | 28/40 [02:58<01:16,  6.41s/it] 72%|███████▎  | 29/40 [03:03<01:08,  6.25s/it] 75%|███████▌  | 30/40 [03:09<01:01,  6.17s/it] 78%|███████▊  | 31/40 [03:15<00:54,  6.09s/it] 80%|████████  | 32/40 [03:22<00:48,  6.11s/it] 82%|████████▎ | 33/40 [03:32<00:51,  7.30s/it] 85%|████████▌ | 34/40 [03:38<00:41,  6.95s/it] 88%|████████▊ | 35/40 [03:44<00:33,  6.66s/it] 90%|█████████ | 36/40 [03:49<00:25,  6.34s/it] 92%|█████████▎| 37/40 [03:55<00:18,  6.14s/it] 95%|█████████▌| 38/40 [04:01<00:12,  6.12s/it] 98%|█████████▊| 39/40 [04:07<00:06,  6.03s/it]100%|██████████| 40/40 [04:13<00:00,  5.94s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:13<00:00,  5.94s/it]100%|██████████| 40/40 [04:13<00:00,  6.33s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 253.1154, 'train_samples_per_second': 11.082, 'train_steps_per_second': 0.158, 'train_loss': 0.7463733673095703, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:46,  5.82s/it]  5%|▌         | 2/40 [00:11<03:39,  5.79s/it]  8%|▊         | 3/40 [00:17<03:34,  5.81s/it] 10%|█         | 4/40 [00:23<03:30,  5.85s/it] 12%|█▎        | 5/40 [00:29<03:28,  5.97s/it] 15%|█▌        | 6/40 [00:35<03:20,  5.89s/it] 18%|█▊        | 7/40 [00:40<03:10,  5.78s/it] 20%|██        | 8/40 [00:46<03:06,  5.83s/it] 22%|██▎       | 9/40 [00:56<03:41,  7.13s/it] 25%|██▌       | 10/40 [01:02<03:20,  6.69s/it] 28%|██▊       | 11/40 [01:08<03:07,  6.46s/it] 30%|███       | 12/40 [01:14<02:55,  6.27s/it] 32%|███▎      | 13/40 [01:19<02:43,  6.07s/it] 35%|███▌      | 14/40 [01:25<02:37,  6.06s/it] 38%|███▊      | 15/40 [01:31<02:30,  6.00s/it] 40%|████      | 16/40 [01:37<02:22,  5.95s/it] 42%|████▎     | 17/40 [01:47<02:45,  7.20s/it] 45%|████▌     | 18/40 [01:53<02:28,  6.76s/it] 48%|████▊     | 19/40 [01:59<02:16,  6.48s/it] 50%|█████     | 20/40 [02:04<02:04,  6.25s/it] 52%|█████▎    | 21/40 [02:10<01:55,  6.09s/it] 55%|█████▌    | 22/40 [02:16<01:49,  6.10s/it] 57%|█████▊    | 23/40 [02:22<01:42,  6.05s/it] 60%|██████    | 24/40 [02:28<01:34,  5.91s/it] 62%|██████▎   | 25/40 [02:38<01:47,  7.19s/it] 65%|██████▌   | 26/40 [02:44<01:34,  6.76s/it] 68%|██████▊   | 27/40 [02:50<01:24,  6.50s/it] 70%|███████   | 28/40 [02:56<01:16,  6.37s/it] 72%|███████▎  | 29/40 [03:02<01:08,  6.25s/it] 75%|███████▌  | 30/40 [03:08<01:01,  6.16s/it] 78%|███████▊  | 31/40 [03:13<00:54,  6.05s/it] 80%|████████  | 32/40 [03:20<00:48,  6.06s/it] 82%|████████▎ | 33/40 [03:29<00:50,  7.20s/it] 85%|████████▌ | 34/40 [03:35<00:41,  6.86s/it] 88%|████████▊ | 35/40 [03:41<00:32,  6.58s/it] 90%|█████████ | 36/40 [03:47<00:25,  6.27s/it] 92%|█████████▎| 37/40 [03:53<00:18,  6.08s/it] 95%|█████████▌| 38/40 [03:58<00:11,  5.93s/it] 98%|█████████▊| 39/40 [04:04<00:05,  5.89s/it]100%|██████████| 40/40 [04:10<00:00,  5.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:10<00:00,  5.82s/it]100%|██████████| 40/40 [04:10<00:00,  6.25s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 250.0652, 'train_samples_per_second': 11.217, 'train_steps_per_second': 0.16, 'train_loss': 0.7505083084106445, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:24,  8.33s/it]  5%|▌         | 2/40 [00:16<05:14,  8.27s/it]  8%|▊         | 3/40 [00:24<05:06,  8.28s/it] 10%|█         | 4/40 [00:33<05:01,  8.37s/it] 12%|█▎        | 5/40 [00:42<05:02,  8.63s/it] 15%|█▌        | 6/40 [00:50<04:51,  8.57s/it] 18%|█▊        | 7/40 [00:58<04:36,  8.37s/it] 20%|██        | 8/40 [01:07<04:30,  8.46s/it] 22%|██▎       | 9/40 [01:22<05:22, 10.41s/it] 25%|██▌       | 10/40 [01:30<04:53,  9.77s/it] 28%|██▊       | 11/40 [01:39<04:34,  9.48s/it] 30%|███       | 12/40 [01:48<04:18,  9.23s/it] 32%|███▎      | 13/40 [01:56<04:01,  8.95s/it] 35%|███▌      | 14/40 [02:05<03:54,  9.02s/it] 38%|███▊      | 15/40 [02:14<03:43,  8.93s/it] 40%|████      | 16/40 [02:22<03:30,  8.77s/it] 42%|████▎     | 17/40 [02:37<04:01, 10.50s/it] 45%|████▌     | 18/40 [02:45<03:35,  9.82s/it] 48%|████▊     | 19/40 [02:53<03:17,  9.42s/it] 50%|█████     | 20/40 [03:02<03:02,  9.13s/it] 52%|█████▎    | 21/40 [03:10<02:49,  8.90s/it] 55%|█████▌    | 22/40 [03:19<02:40,  8.89s/it] 57%|█████▊    | 23/40 [03:28<02:29,  8.82s/it] 60%|██████    | 24/40 [03:36<02:16,  8.56s/it] 62%|██████▎   | 25/40 [03:50<02:34, 10.28s/it] 65%|██████▌   | 26/40 [03:58<02:14,  9.64s/it] 68%|██████▊   | 27/40 [04:06<02:00,  9.24s/it] 70%|███████   | 28/40 [04:15<01:47,  8.95s/it] 72%|███████▎  | 29/40 [04:23<01:36,  8.77s/it] 75%|███████▌  | 30/40 [04:31<01:26,  8.65s/it] 78%|███████▊  | 31/40 [04:40<01:16,  8.49s/it] 80%|████████  | 32/40 [04:48<01:08,  8.58s/it] 82%|████████▎ | 33/40 [05:03<01:11, 10.27s/it] 85%|████████▌ | 34/40 [05:11<00:58,  9.83s/it] 88%|████████▊ | 35/40 [05:20<00:47,  9.48s/it] 90%|█████████ | 36/40 [05:28<00:36,  9.04s/it] 92%|█████████▎| 37/40 [05:36<00:26,  8.86s/it] 95%|█████████▌| 38/40 [05:44<00:17,  8.60s/it] 98%|█████████▊| 39/40 [05:53<00:08,  8.56s/it]100%|██████████| 40/40 [06:01<00:00,  8.45s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:01<00:00,  8.45s/it]100%|██████████| 40/40 [06:01<00:00,  9.04s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 361.6108, 'train_samples_per_second': 7.757, 'train_steps_per_second': 0.111, 'train_loss': 0.7569923400878906, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:58,  9.18s/it]  5%|▌         | 2/40 [00:18<05:45,  9.10s/it]  8%|▊         | 3/40 [00:27<05:37,  9.13s/it] 10%|█         | 4/40 [00:36<05:29,  9.15s/it] 12%|█▎        | 5/40 [00:46<05:27,  9.36s/it] 15%|█▌        | 6/40 [00:55<05:13,  9.23s/it] 18%|█▊        | 7/40 [01:03<04:57,  9.02s/it] 20%|██        | 8/40 [01:13<04:51,  9.11s/it] 22%|██▎       | 9/40 [01:29<05:49, 11.26s/it] 25%|██▌       | 10/40 [01:38<05:18, 10.63s/it] 28%|██▊       | 11/40 [01:48<05:00, 10.37s/it] 30%|███       | 12/40 [01:57<04:44, 10.17s/it] 32%|███▎      | 13/40 [02:06<04:25,  9.85s/it] 35%|███▌      | 14/40 [02:16<04:13,  9.77s/it] 38%|███▊      | 15/40 [02:25<04:00,  9.60s/it] 40%|████      | 16/40 [02:34<03:47,  9.48s/it] 42%|████▎     | 17/40 [02:50<04:22, 11.43s/it] 45%|████▌     | 18/40 [02:59<03:55, 10.71s/it] 48%|████▊     | 19/40 [03:09<03:35, 10.28s/it] 50%|█████     | 20/40 [03:18<03:17,  9.90s/it] 52%|█████▎    | 21/40 [03:27<03:03,  9.64s/it] 55%|█████▌    | 22/40 [03:37<02:54,  9.67s/it] 57%|█████▊    | 23/40 [03:46<02:43,  9.60s/it] 60%|██████    | 24/40 [03:55<02:31,  9.47s/it] 62%|██████▎   | 25/40 [04:11<02:50, 11.37s/it] 65%|██████▌   | 26/40 [04:20<02:29, 10.67s/it] 68%|██████▊   | 27/40 [04:29<02:12, 10.21s/it] 70%|███████   | 28/40 [04:39<02:00, 10.06s/it] 72%|███████▎  | 29/40 [04:48<01:47,  9.78s/it] 75%|███████▌  | 30/40 [04:57<01:35,  9.60s/it] 78%|███████▊  | 31/40 [05:06<01:24,  9.41s/it] 80%|████████  | 32/40 [05:16<01:15,  9.46s/it] 82%|████████▎ | 33/40 [05:31<01:19, 11.29s/it] 85%|████████▌ | 34/40 [05:41<01:04, 10.78s/it] 88%|████████▊ | 35/40 [05:50<00:51, 10.38s/it] 90%|█████████ | 36/40 [05:59<00:39,  9.93s/it] 92%|█████████▎| 37/40 [06:08<00:28,  9.65s/it] 95%|█████████▌| 38/40 [06:17<00:18,  9.45s/it] 98%|█████████▊| 39/40 [06:27<00:09,  9.47s/it]100%|██████████| 40/40 [06:36<00:00,  9.40s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:36<00:00,  9.40s/it]100%|██████████| 40/40 [06:36<00:00,  9.91s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 396.3857, 'train_samples_per_second': 7.076, 'train_steps_per_second': 0.101, 'train_loss': 0.7835698127746582, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:54,  6.00s/it]  5%|▌         | 2/40 [00:12<03:54,  6.17s/it]  8%|▊         | 3/40 [00:18<03:43,  6.05s/it] 10%|█         | 4/40 [00:24<03:36,  6.02s/it] 12%|█▎        | 5/40 [00:30<03:35,  6.17s/it] 15%|█▌        | 6/40 [00:36<03:28,  6.12s/it] 18%|█▊        | 7/40 [00:42<03:20,  6.08s/it] 20%|██        | 8/40 [00:48<03:16,  6.13s/it] 22%|██▎       | 9/40 [00:55<03:18,  6.41s/it] 25%|██▌       | 10/40 [01:01<03:08,  6.28s/it] 28%|██▊       | 11/40 [01:07<03:00,  6.22s/it] 30%|███       | 12/40 [01:13<02:51,  6.13s/it] 32%|███▎      | 13/40 [01:19<02:45,  6.11s/it] 35%|███▌      | 14/40 [01:25<02:38,  6.09s/it] 38%|███▊      | 15/40 [01:32<02:33,  6.14s/it] 40%|████      | 16/40 [01:38<02:29,  6.22s/it] 42%|████▎     | 17/40 [01:45<02:29,  6.50s/it] 45%|████▌     | 18/40 [01:52<02:23,  6.52s/it] 48%|████▊     | 19/40 [01:58<02:17,  6.54s/it] 50%|█████     | 20/40 [02:05<02:08,  6.44s/it] 52%|█████▎    | 21/40 [02:11<02:00,  6.35s/it] 55%|█████▌    | 22/40 [02:17<01:54,  6.37s/it] 57%|█████▊    | 23/40 [02:24<01:48,  6.36s/it] 60%|██████    | 24/40 [02:30<01:42,  6.39s/it] 62%|██████▎   | 25/40 [02:37<01:39,  6.62s/it] 65%|██████▌   | 26/40 [02:44<01:32,  6.59s/it] 68%|██████▊   | 27/40 [02:50<01:24,  6.50s/it] 70%|███████   | 28/40 [02:56<01:16,  6.37s/it] 72%|███████▎  | 29/40 [03:02<01:09,  6.36s/it] 75%|███████▌  | 30/40 [03:09<01:03,  6.34s/it] 78%|███████▊  | 31/40 [03:15<00:57,  6.41s/it] 80%|████████  | 32/40 [03:22<00:51,  6.44s/it] 82%|████████▎ | 33/40 [03:29<00:47,  6.76s/it] 85%|████████▌ | 34/40 [03:36<00:40,  6.73s/it] 88%|████████▊ | 35/40 [03:43<00:33,  6.71s/it] 90%|█████████ | 36/40 [03:49<00:26,  6.60s/it] 92%|█████████▎| 37/40 [03:55<00:19,  6.54s/it] 95%|█████████▌| 38/40 [04:02<00:13,  6.62s/it] 98%|█████████▊| 39/40 [04:08<00:06,  6.53s/it]100%|██████████| 40/40 [04:14<00:00,  6.34s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:14<00:00,  6.34s/it]100%|██████████| 40/40 [04:14<00:00,  6.37s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 254.8487, 'train_samples_per_second': 10.222, 'train_steps_per_second': 0.157, 'train_loss': 0.6989841461181641, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:56,  6.06s/it]  5%|▌         | 2/40 [00:12<03:58,  6.28s/it]  8%|▊         | 3/40 [00:18<03:47,  6.16s/it] 10%|█         | 4/40 [00:24<03:42,  6.19s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.32s/it] 15%|█▌        | 6/40 [00:37<03:32,  6.24s/it] 18%|█▊        | 7/40 [00:43<03:24,  6.18s/it] 20%|██        | 8/40 [00:49<03:18,  6.20s/it] 22%|██▎       | 9/40 [00:56<03:20,  6.45s/it] 25%|██▌       | 10/40 [01:02<03:08,  6.30s/it] 28%|██▊       | 11/40 [01:08<03:00,  6.22s/it] 30%|███       | 12/40 [01:14<02:51,  6.14s/it] 32%|███▎      | 13/40 [01:20<02:45,  6.12s/it] 35%|███▌      | 14/40 [01:26<02:38,  6.09s/it] 38%|███▊      | 15/40 [01:32<02:33,  6.13s/it] 40%|████      | 16/40 [01:39<02:28,  6.18s/it] 42%|████▎     | 17/40 [01:46<02:27,  6.40s/it] 45%|████▌     | 18/40 [01:52<02:21,  6.42s/it] 48%|████▊     | 19/40 [01:59<02:14,  6.41s/it] 50%|█████     | 20/40 [02:05<02:07,  6.36s/it] 52%|█████▎    | 21/40 [02:11<02:00,  6.35s/it] 55%|█████▌    | 22/40 [02:17<01:52,  6.25s/it] 57%|█████▊    | 23/40 [02:23<01:45,  6.21s/it] 60%|██████    | 24/40 [02:30<01:42,  6.42s/it] 62%|██████▎   | 25/40 [02:38<01:41,  6.77s/it] 65%|██████▌   | 26/40 [02:44<01:32,  6.62s/it] 68%|██████▊   | 27/40 [02:50<01:24,  6.48s/it] 70%|███████   | 28/40 [02:56<01:15,  6.31s/it] 72%|███████▎  | 29/40 [03:02<01:08,  6.27s/it] 75%|███████▌  | 30/40 [03:08<01:01,  6.19s/it] 78%|███████▊  | 31/40 [03:14<00:55,  6.20s/it] 80%|████████  | 32/40 [03:21<00:49,  6.17s/it] 82%|████████▎ | 33/40 [03:28<00:45,  6.45s/it] 85%|████████▌ | 34/40 [03:34<00:38,  6.39s/it] 88%|████████▊ | 35/40 [03:40<00:31,  6.36s/it] 90%|█████████ | 36/40 [03:46<00:25,  6.25s/it] 92%|█████████▎| 37/40 [03:52<00:18,  6.19s/it] 95%|█████████▌| 38/40 [03:58<00:12,  6.19s/it] 98%|█████████▊| 39/40 [04:04<00:06,  6.14s/it]100%|██████████| 40/40 [04:10<00:00,  6.01s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:10<00:00,  6.01s/it]100%|██████████| 40/40 [04:10<00:00,  6.27s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 250.6811, 'train_samples_per_second': 10.392, 'train_steps_per_second': 0.16, 'train_loss': 0.7157560348510742, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:31,  8.51s/it]  5%|▌         | 2/40 [00:17<05:36,  8.85s/it]  8%|▊         | 3/40 [00:26<05:22,  8.70s/it] 10%|█         | 4/40 [00:34<05:12,  8.69s/it] 12%|█▎        | 5/40 [00:44<05:13,  8.95s/it] 15%|█▌        | 6/40 [00:52<05:02,  8.89s/it] 18%|█▊        | 7/40 [01:01<04:52,  8.86s/it] 20%|██        | 8/40 [01:10<04:46,  8.95s/it] 22%|██▎       | 9/40 [01:21<04:51,  9.39s/it] 25%|██▌       | 10/40 [01:29<04:35,  9.18s/it] 28%|██▊       | 11/40 [01:38<04:22,  9.06s/it] 30%|███       | 12/40 [01:47<04:09,  8.91s/it] 32%|███▎      | 13/40 [01:56<03:59,  8.87s/it] 35%|███▌      | 14/40 [02:04<03:48,  8.79s/it] 38%|███▊      | 15/40 [02:13<03:42,  8.91s/it] 40%|████      | 16/40 [02:23<03:36,  9.01s/it] 42%|████▎     | 17/40 [02:33<03:33,  9.29s/it] 45%|████▌     | 18/40 [02:41<03:21,  9.17s/it] 48%|████▊     | 19/40 [02:50<03:10,  9.07s/it] 50%|█████     | 20/40 [02:59<02:59,  8.99s/it] 52%|█████▎    | 21/40 [03:08<02:50,  8.97s/it] 55%|█████▌    | 22/40 [03:17<02:39,  8.87s/it] 57%|█████▊    | 23/40 [03:25<02:29,  8.80s/it] 60%|██████    | 24/40 [03:35<02:23,  8.96s/it] 62%|██████▎   | 25/40 [03:45<02:19,  9.29s/it] 65%|██████▌   | 26/40 [03:53<02:07,  9.12s/it] 68%|██████▊   | 27/40 [04:02<01:57,  9.05s/it] 70%|███████   | 28/40 [04:11<01:46,  8.91s/it] 72%|███████▎  | 29/40 [04:20<01:38,  8.94s/it] 75%|███████▌  | 30/40 [04:29<01:28,  8.86s/it] 78%|███████▊  | 31/40 [04:38<01:20,  8.89s/it] 80%|████████  | 32/40 [04:46<01:11,  8.89s/it] 82%|████████▎ | 33/40 [04:57<01:05,  9.30s/it] 85%|████████▌ | 34/40 [05:06<00:55,  9.27s/it] 88%|████████▊ | 35/40 [05:15<00:45,  9.19s/it] 90%|█████████ | 36/40 [05:24<00:36,  9.03s/it] 92%|█████████▎| 37/40 [05:32<00:26,  8.93s/it] 95%|█████████▌| 38/40 [05:41<00:17,  8.95s/it] 98%|█████████▊| 39/40 [05:50<00:08,  8.89s/it]100%|██████████| 40/40 [05:58<00:00,  8.68s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [05:58<00:00,  8.68s/it]100%|██████████| 40/40 [05:58<00:00,  8.97s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 358.689, 'train_samples_per_second': 7.263, 'train_steps_per_second': 0.112, 'train_loss': 0.6979462623596191, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:57,  9.17s/it]  5%|▌         | 2/40 [00:18<06:02,  9.54s/it]  8%|▊         | 3/40 [00:28<05:48,  9.42s/it] 10%|█         | 4/40 [00:37<05:38,  9.40s/it] 12%|█▎        | 5/40 [00:47<05:38,  9.69s/it] 15%|█▌        | 6/40 [00:57<05:26,  9.62s/it] 18%|█▊        | 7/40 [01:06<05:15,  9.57s/it] 20%|██        | 8/40 [01:16<05:09,  9.66s/it] 22%|██▎       | 9/40 [01:27<05:13, 10.12s/it] 25%|██▌       | 10/40 [01:37<04:57,  9.90s/it] 28%|██▊       | 11/40 [01:46<04:43,  9.79s/it] 30%|███       | 12/40 [01:56<04:30,  9.66s/it] 32%|███▎      | 13/40 [02:05<04:20,  9.65s/it] 35%|███▌      | 14/40 [02:15<04:09,  9.60s/it] 38%|███▊      | 15/40 [02:25<04:02,  9.68s/it] 40%|████      | 16/40 [02:35<03:54,  9.78s/it] 42%|████▎     | 17/40 [02:45<03:52, 10.10s/it] 45%|████▌     | 18/40 [02:55<03:39,  9.99s/it] 48%|████▊     | 19/40 [03:05<03:27,  9.88s/it] 50%|█████     | 20/40 [03:14<03:15,  9.80s/it] 52%|█████▎    | 21/40 [03:24<03:05,  9.77s/it] 55%|█████▌    | 22/40 [03:33<02:54,  9.67s/it] 57%|█████▊    | 23/40 [03:43<02:42,  9.58s/it] 60%|██████    | 24/40 [03:53<02:35,  9.75s/it] 62%|██████▎   | 25/40 [04:04<02:31, 10.12s/it] 65%|██████▌   | 26/40 [04:14<02:19,  9.95s/it] 68%|██████▊   | 27/40 [04:23<02:08,  9.86s/it] 70%|███████   | 28/40 [04:32<01:56,  9.69s/it] 72%|███████▎  | 29/40 [04:42<01:46,  9.72s/it] 75%|███████▌  | 30/40 [04:52<01:36,  9.64s/it] 78%|███████▊  | 31/40 [05:01<01:27,  9.68s/it] 80%|████████  | 32/40 [05:11<01:17,  9.68s/it] 82%|████████▎ | 33/40 [05:22<01:10, 10.12s/it] 85%|████████▌ | 34/40 [05:32<01:00, 10.07s/it] 88%|████████▊ | 35/40 [05:42<00:50, 10.01s/it] 90%|█████████ | 36/40 [05:52<00:39,  9.86s/it] 92%|█████████▎| 37/40 [06:01<00:29,  9.76s/it] 95%|█████████▌| 38/40 [06:11<00:19,  9.79s/it] 98%|█████████▊| 39/40 [06:21<00:09,  9.72s/it]100%|██████████| 40/40 [06:30<00:00,  9.49s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:30<00:00,  9.49s/it]100%|██████████| 40/40 [06:30<00:00,  9.75s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 390.0718, 'train_samples_per_second': 6.678, 'train_steps_per_second': 0.103, 'train_loss': 0.7552658081054687, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:54,  6.01s/it]  5%|▌         | 2/40 [00:12<03:51,  6.10s/it]  8%|▊         | 3/40 [00:18<03:46,  6.12s/it] 10%|█         | 4/40 [00:24<03:40,  6.13s/it] 12%|█▎        | 5/40 [00:30<03:30,  6.02s/it] 15%|█▌        | 6/40 [00:36<03:22,  5.97s/it] 18%|█▊        | 7/40 [00:42<03:18,  6.01s/it] 20%|██        | 8/40 [00:48<03:15,  6.10s/it] 22%|██▎       | 9/40 [00:55<03:16,  6.33s/it] 25%|██▌       | 10/40 [01:01<03:06,  6.23s/it] 28%|██▊       | 11/40 [01:07<02:56,  6.08s/it] 30%|███       | 12/40 [01:13<02:50,  6.08s/it] 32%|███▎      | 13/40 [01:19<02:45,  6.15s/it] 35%|███▌      | 14/40 [01:25<02:38,  6.09s/it] 38%|███▊      | 15/40 [01:31<02:33,  6.14s/it] 40%|████      | 16/40 [01:37<02:28,  6.17s/it] 42%|████▎     | 17/40 [01:44<02:25,  6.33s/it] 45%|████▌     | 18/40 [01:50<02:18,  6.28s/it] 48%|████▊     | 19/40 [01:56<02:11,  6.24s/it] 50%|█████     | 20/40 [02:02<02:01,  6.09s/it] 52%|█████▎    | 21/40 [02:09<01:56,  6.15s/it] 55%|█████▌    | 22/40 [02:14<01:49,  6.08s/it] 57%|█████▊    | 23/40 [02:20<01:43,  6.06s/it] 60%|██████    | 24/40 [02:27<01:39,  6.20s/it] 62%|██████▎   | 25/40 [02:34<01:36,  6.43s/it] 65%|██████▌   | 26/40 [02:40<01:28,  6.29s/it] 68%|██████▊   | 27/40 [02:46<01:21,  6.27s/it] 70%|███████   | 28/40 [02:52<01:13,  6.12s/it] 72%|███████▎  | 29/40 [02:58<01:07,  6.11s/it] 75%|███████▌  | 30/40 [03:04<01:00,  6.08s/it] 78%|███████▊  | 31/40 [03:10<00:55,  6.15s/it] 80%|████████  | 32/40 [03:16<00:48,  6.11s/it] 82%|████████▎ | 33/40 [03:23<00:44,  6.31s/it] 85%|████████▌ | 34/40 [03:29<00:37,  6.27s/it] 88%|████████▊ | 35/40 [03:36<00:31,  6.29s/it] 90%|█████████ | 36/40 [03:42<00:24,  6.18s/it] 92%|█████████▎| 37/40 [03:48<00:18,  6.21s/it] 95%|█████████▌| 38/40 [03:54<00:12,  6.10s/it] 98%|█████████▊| 39/40 [04:00<00:06,  6.14s/it]100%|██████████| 40/40 [04:06<00:00,  6.02s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:06<00:00,  6.02s/it]100%|██████████| 40/40 [04:06<00:00,  6.15s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 246.1361, 'train_samples_per_second': 10.584, 'train_steps_per_second': 0.163, 'train_loss': 0.6989320755004883, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:52,  5.96s/it]  5%|▌         | 2/40 [00:12<03:50,  6.07s/it]  8%|▊         | 3/40 [00:18<03:45,  6.11s/it] 10%|█         | 4/40 [00:24<03:40,  6.12s/it] 12%|█▎        | 5/40 [00:30<03:30,  6.01s/it] 15%|█▌        | 6/40 [00:36<03:22,  5.97s/it] 18%|█▊        | 7/40 [00:42<03:18,  6.01s/it] 20%|██        | 8/40 [00:48<03:15,  6.10s/it] 22%|██▎       | 9/40 [00:55<03:16,  6.33s/it] 25%|██▌       | 10/40 [01:01<03:07,  6.23s/it] 28%|██▊       | 11/40 [01:07<02:56,  6.08s/it] 30%|███       | 12/40 [01:13<02:50,  6.08s/it] 32%|███▎      | 13/40 [01:19<02:46,  6.15s/it] 35%|███▌      | 14/40 [01:25<02:38,  6.10s/it] 38%|███▊      | 15/40 [01:31<02:33,  6.15s/it] 40%|████      | 16/40 [01:37<02:28,  6.18s/it] 42%|████▎     | 17/40 [01:44<02:25,  6.34s/it] 45%|████▌     | 18/40 [01:50<02:18,  6.28s/it] 48%|████▊     | 19/40 [01:56<02:11,  6.24s/it] 50%|█████     | 20/40 [02:02<02:01,  6.09s/it] 52%|█████▎    | 21/40 [02:08<01:56,  6.15s/it] 55%|█████▌    | 22/40 [02:14<01:49,  6.08s/it] 57%|█████▊    | 23/40 [02:20<01:43,  6.06s/it] 60%|██████    | 24/40 [02:27<01:39,  6.20s/it] 62%|██████▎   | 25/40 [02:34<01:36,  6.43s/it] 65%|██████▌   | 26/40 [02:40<01:28,  6.29s/it] 68%|██████▊   | 27/40 [02:46<01:21,  6.28s/it] 70%|███████   | 28/40 [02:52<01:13,  6.12s/it] 72%|███████▎  | 29/40 [02:58<01:07,  6.11s/it] 75%|███████▌  | 30/40 [03:04<01:00,  6.08s/it] 78%|███████▊  | 31/40 [03:10<00:55,  6.15s/it] 80%|████████  | 32/40 [03:16<00:48,  6.11s/it] 82%|████████▎ | 33/40 [03:23<00:44,  6.31s/it] 85%|████████▌ | 34/40 [03:29<00:37,  6.28s/it] 88%|████████▊ | 35/40 [03:36<00:31,  6.30s/it] 90%|█████████ | 36/40 [03:42<00:24,  6.19s/it] 92%|█████████▎| 37/40 [03:48<00:18,  6.21s/it] 95%|█████████▌| 38/40 [03:54<00:12,  6.09s/it] 98%|█████████▊| 39/40 [04:00<00:06,  6.13s/it]100%|██████████| 40/40 [04:06<00:00,  6.02s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:06<00:00,  6.02s/it]100%|██████████| 40/40 [04:06<00:00,  6.15s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 246.1377, 'train_samples_per_second': 10.584, 'train_steps_per_second': 0.163, 'train_loss': 0.7157247543334961, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:35,  8.61s/it]  5%|▌         | 2/40 [00:17<05:36,  8.84s/it]  8%|▊         | 3/40 [00:26<05:28,  8.89s/it] 10%|█         | 4/40 [00:35<05:20,  8.89s/it] 12%|█▎        | 5/40 [00:43<05:05,  8.72s/it] 15%|█▌        | 6/40 [00:52<04:54,  8.66s/it] 18%|█▊        | 7/40 [01:01<04:48,  8.74s/it] 20%|██        | 8/40 [01:10<04:44,  8.89s/it] 22%|██▎       | 9/40 [01:20<04:46,  9.24s/it] 25%|██▌       | 10/40 [01:29<04:33,  9.11s/it] 28%|██▊       | 11/40 [01:37<04:16,  8.83s/it] 30%|███       | 12/40 [01:46<04:07,  8.83s/it] 32%|███▎      | 13/40 [01:55<04:01,  8.95s/it] 35%|███▌      | 14/40 [02:04<03:50,  8.85s/it] 38%|███▊      | 15/40 [02:13<03:43,  8.92s/it] 40%|████      | 16/40 [02:22<03:35,  9.00s/it] 42%|████▎     | 17/40 [02:32<03:32,  9.22s/it] 45%|████▌     | 18/40 [02:41<03:20,  9.13s/it] 48%|████▊     | 19/40 [02:50<03:10,  9.09s/it] 50%|█████     | 20/40 [02:58<02:57,  8.85s/it] 52%|█████▎    | 21/40 [03:07<02:50,  8.98s/it] 55%|█████▌    | 22/40 [03:16<02:39,  8.84s/it] 57%|█████▊    | 23/40 [03:25<02:29,  8.82s/it] 60%|██████    | 24/40 [03:34<02:24,  9.04s/it] 62%|██████▎   | 25/40 [03:44<02:20,  9.37s/it] 65%|██████▌   | 26/40 [03:53<02:07,  9.14s/it] 68%|██████▊   | 27/40 [04:02<01:59,  9.15s/it] 70%|███████   | 28/40 [04:10<01:46,  8.91s/it] 72%|███████▎  | 29/40 [04:19<01:37,  8.90s/it] 75%|███████▌  | 30/40 [04:28<01:28,  8.83s/it] 78%|███████▊  | 31/40 [04:37<01:20,  8.96s/it] 80%|████████  | 32/40 [04:46<01:11,  8.93s/it] 82%|████████▎ | 33/40 [04:56<01:04,  9.20s/it] 85%|████████▌ | 34/40 [05:05<00:54,  9.16s/it] 88%|████████▊ | 35/40 [05:14<00:45,  9.17s/it] 90%|█████████ | 36/40 [05:23<00:35,  9.00s/it] 92%|█████████▎| 37/40 [05:32<00:27,  9.05s/it] 95%|█████████▌| 38/40 [05:40<00:17,  8.87s/it] 98%|█████████▊| 39/40 [05:49<00:08,  8.96s/it]100%|██████████| 40/40 [05:58<00:00,  8.76s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [05:58<00:00,  8.76s/it]100%|██████████| 40/40 [05:58<00:00,  8.96s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 358.2842, 'train_samples_per_second': 7.271, 'train_steps_per_second': 0.112, 'train_loss': 0.6985294342041015, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:09,  9.46s/it]  5%|▌         | 2/40 [00:19<06:07,  9.66s/it]  8%|▊         | 3/40 [00:29<05:59,  9.73s/it] 10%|█         | 4/40 [00:38<05:50,  9.75s/it] 12%|█▎        | 5/40 [00:48<05:35,  9.58s/it] 15%|█▌        | 6/40 [00:57<05:23,  9.51s/it] 18%|█▊        | 7/40 [01:07<05:16,  9.58s/it] 20%|██        | 8/40 [01:17<05:11,  9.72s/it] 22%|██▎       | 9/40 [01:28<05:13, 10.10s/it] 25%|██▌       | 10/40 [01:37<04:58,  9.95s/it] 28%|██▊       | 11/40 [01:46<04:40,  9.68s/it] 30%|███       | 12/40 [01:56<04:31,  9.68s/it] 32%|███▎      | 13/40 [02:06<04:24,  9.81s/it] 35%|███▌      | 14/40 [02:16<04:12,  9.71s/it] 38%|███▊      | 15/40 [02:26<04:04,  9.79s/it] 40%|████      | 16/40 [02:36<03:56,  9.86s/it] 42%|████▎     | 17/40 [02:46<03:52, 10.10s/it] 45%|████▌     | 18/40 [02:56<03:39,  9.99s/it] 48%|████▊     | 19/40 [03:06<03:28,  9.95s/it] 50%|█████     | 20/40 [03:15<03:14,  9.70s/it] 52%|█████▎    | 21/40 [03:25<03:06,  9.82s/it] 55%|█████▌    | 22/40 [03:34<02:54,  9.69s/it] 57%|█████▊    | 23/40 [03:44<02:44,  9.66s/it] 60%|██████    | 24/40 [03:54<02:38,  9.89s/it] 62%|██████▎   | 25/40 [04:06<02:33, 10.26s/it] 65%|██████▌   | 26/40 [04:15<02:20, 10.03s/it] 68%|██████▊   | 27/40 [04:25<02:10, 10.03s/it] 70%|███████   | 28/40 [04:34<01:57,  9.77s/it] 72%|███████▎  | 29/40 [04:44<01:47,  9.74s/it] 75%|███████▌  | 30/40 [04:54<01:36,  9.68s/it] 78%|███████▊  | 31/40 [05:04<01:28,  9.81s/it] 80%|████████  | 32/40 [05:13<01:18,  9.77s/it] 82%|████████▎ | 33/40 [05:24<01:10, 10.08s/it] 85%|████████▌ | 34/40 [05:34<01:00, 10.01s/it] 88%|████████▊ | 35/40 [05:44<00:50, 10.03s/it] 90%|█████████ | 36/40 [05:53<00:39,  9.86s/it] 92%|█████████▎| 37/40 [06:04<00:29,  9.91s/it] 95%|█████████▌| 38/40 [06:13<00:19,  9.72s/it] 98%|█████████▊| 39/40 [06:23<00:09,  9.80s/it]100%|██████████| 40/40 [06:32<00:00,  9.60s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:32<00:00,  9.60s/it]100%|██████████| 40/40 [06:32<00:00,  9.81s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 392.4248, 'train_samples_per_second': 6.638, 'train_steps_per_second': 0.102, 'train_loss': 0.755159568786621, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:55,  6.04s/it]  5%|▌         | 2/40 [00:12<03:57,  6.25s/it]  8%|▊         | 3/40 [00:18<03:52,  6.28s/it] 10%|█         | 4/40 [00:24<03:44,  6.23s/it] 12%|█▎        | 5/40 [00:31<03:37,  6.20s/it] 15%|█▌        | 6/40 [00:37<03:29,  6.15s/it] 18%|█▊        | 7/40 [00:43<03:20,  6.07s/it] 20%|██        | 8/40 [00:49<03:17,  6.18s/it] 22%|██▎       | 9/40 [00:56<03:22,  6.52s/it] 25%|██▌       | 10/40 [01:02<03:09,  6.30s/it] 28%|██▊       | 11/40 [01:08<03:01,  6.25s/it] 30%|███       | 12/40 [01:14<02:53,  6.19s/it] 32%|███▎      | 13/40 [01:20<02:47,  6.21s/it] 35%|███▌      | 14/40 [01:27<02:40,  6.17s/it] 38%|███▊      | 15/40 [01:33<02:35,  6.23s/it] 40%|████      | 16/40 [01:39<02:30,  6.29s/it] 42%|████▎     | 17/40 [01:46<02:28,  6.45s/it] 45%|████▌     | 18/40 [01:52<02:20,  6.38s/it] 48%|████▊     | 19/40 [01:58<02:11,  6.28s/it] 50%|█████     | 20/40 [02:04<02:04,  6.22s/it] 52%|█████▎    | 21/40 [02:11<01:57,  6.21s/it] 55%|█████▌    | 22/40 [02:17<01:50,  6.15s/it] 57%|█████▊    | 23/40 [02:23<01:44,  6.14s/it] 60%|██████    | 24/40 [02:29<01:38,  6.17s/it] 62%|██████▎   | 25/40 [02:36<01:36,  6.45s/it] 65%|██████▌   | 26/40 [02:42<01:28,  6.30s/it] 68%|██████▊   | 27/40 [02:48<01:21,  6.26s/it] 70%|███████   | 28/40 [02:54<01:13,  6.12s/it] 72%|███████▎  | 29/40 [03:00<01:06,  6.04s/it] 75%|███████▌  | 30/40 [03:06<01:00,  6.05s/it] 78%|███████▊  | 31/40 [03:12<00:54,  6.05s/it] 80%|████████  | 32/40 [03:18<00:48,  6.11s/it] 82%|████████▎ | 33/40 [03:25<00:44,  6.36s/it] 85%|████████▌ | 34/40 [03:31<00:37,  6.32s/it] 88%|████████▊ | 35/40 [03:38<00:31,  6.27s/it] 90%|█████████ | 36/40 [03:44<00:24,  6.20s/it] 92%|█████████▎| 37/40 [03:50<00:18,  6.15s/it] 95%|█████████▌| 38/40 [03:56<00:12,  6.13s/it] 98%|█████████▊| 39/40 [04:02<00:06,  6.09s/it]100%|██████████| 40/40 [04:08<00:00,  6.02s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:08<00:00,  6.02s/it]100%|██████████| 40/40 [04:08<00:00,  6.20s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 248.1389, 'train_samples_per_second': 10.498, 'train_steps_per_second': 0.161, 'train_loss': 0.6988674163818359, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:49,  5.88s/it]  5%|▌         | 2/40 [00:12<03:51,  6.09s/it]  8%|▊         | 3/40 [00:18<03:44,  6.07s/it] 10%|█         | 4/40 [00:24<03:37,  6.05s/it] 12%|█▎        | 5/40 [00:30<03:31,  6.05s/it] 15%|█▌        | 6/40 [00:36<03:24,  6.01s/it] 18%|█▊        | 7/40 [00:41<03:15,  5.94s/it] 20%|██        | 8/40 [00:48<03:13,  6.06s/it] 22%|██▎       | 9/40 [00:55<03:18,  6.40s/it] 25%|██▌       | 10/40 [01:01<03:05,  6.17s/it] 28%|██▊       | 11/40 [01:07<02:57,  6.12s/it] 30%|███       | 12/40 [01:13<02:49,  6.07s/it] 32%|███▎      | 13/40 [01:19<02:44,  6.09s/it] 35%|███▌      | 14/40 [01:25<02:37,  6.05s/it] 38%|███▊      | 15/40 [01:31<02:33,  6.12s/it] 40%|████      | 16/40 [01:37<02:28,  6.18s/it] 42%|████▎     | 17/40 [01:44<02:25,  6.34s/it] 45%|████▌     | 18/40 [01:50<02:17,  6.27s/it] 48%|████▊     | 19/40 [01:56<02:09,  6.17s/it] 50%|█████     | 20/40 [02:02<02:02,  6.11s/it] 52%|█████▎    | 21/40 [02:08<01:56,  6.12s/it] 55%|█████▌    | 22/40 [02:14<01:49,  6.09s/it] 57%|█████▊    | 23/40 [02:20<01:43,  6.10s/it] 60%|██████    | 24/40 [02:26<01:38,  6.14s/it] 62%|██████▎   | 25/40 [02:34<01:36,  6.43s/it] 65%|██████▌   | 26/40 [02:40<01:27,  6.28s/it] 68%|██████▊   | 27/40 [02:46<01:21,  6.25s/it] 70%|███████   | 28/40 [02:51<01:13,  6.11s/it] 72%|███████▎  | 29/40 [02:57<01:06,  6.04s/it] 75%|███████▌  | 30/40 [03:03<01:00,  6.04s/it] 78%|███████▊  | 31/40 [03:09<00:54,  6.05s/it] 80%|████████  | 32/40 [03:16<00:48,  6.10s/it] 82%|████████▎ | 33/40 [03:23<00:44,  6.36s/it] 85%|████████▌ | 34/40 [03:29<00:37,  6.31s/it] 88%|████████▊ | 35/40 [03:35<00:31,  6.27s/it] 90%|█████████ | 36/40 [03:41<00:24,  6.20s/it] 92%|█████████▎| 37/40 [03:47<00:18,  6.15s/it] 95%|█████████▌| 38/40 [03:53<00:12,  6.13s/it] 98%|█████████▊| 39/40 [03:59<00:06,  6.09s/it]100%|██████████| 40/40 [04:05<00:00,  6.02s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:05<00:00,  6.02s/it]100%|██████████| 40/40 [04:05<00:00,  6.14s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 245.5409, 'train_samples_per_second': 10.609, 'train_steps_per_second': 0.163, 'train_loss': 0.7159580230712891, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:27,  8.40s/it]  5%|▌         | 2/40 [00:17<05:33,  8.76s/it]  8%|▊         | 3/40 [00:26<05:23,  8.76s/it] 10%|█         | 4/40 [00:34<05:14,  8.72s/it] 12%|█▎        | 5/40 [00:43<05:05,  8.74s/it] 15%|█▌        | 6/40 [00:52<04:55,  8.68s/it] 18%|█▊        | 7/40 [01:00<04:43,  8.58s/it] 20%|██        | 8/40 [01:09<04:40,  8.77s/it] 22%|██▎       | 9/40 [01:20<04:48,  9.30s/it] 25%|██▌       | 10/40 [01:28<04:27,  8.92s/it] 28%|██▊       | 11/40 [01:36<04:16,  8.85s/it] 30%|███       | 12/40 [01:45<04:05,  8.78s/it] 32%|███▎      | 13/40 [01:54<03:57,  8.81s/it] 35%|███▌      | 14/40 [02:02<03:46,  8.73s/it] 38%|███▊      | 15/40 [02:12<03:41,  8.84s/it] 40%|████      | 16/40 [02:21<03:34,  8.93s/it] 42%|████▎     | 17/40 [02:30<03:30,  9.14s/it] 45%|████▌     | 18/40 [02:39<03:18,  9.03s/it] 48%|████▊     | 19/40 [02:48<03:06,  8.87s/it] 50%|█████     | 20/40 [02:56<02:55,  8.78s/it] 52%|█████▎    | 21/40 [03:05<02:47,  8.84s/it] 55%|█████▌    | 22/40 [03:14<02:37,  8.77s/it] 57%|█████▊    | 23/40 [03:23<02:29,  8.81s/it] 60%|██████    | 24/40 [03:32<02:22,  8.88s/it] 62%|██████▎   | 25/40 [03:42<02:19,  9.30s/it] 65%|██████▌   | 26/40 [03:50<02:06,  9.04s/it] 68%|██████▊   | 27/40 [03:59<01:57,  9.03s/it] 70%|███████   | 28/40 [04:08<01:45,  8.81s/it] 72%|███████▎  | 29/40 [04:16<01:35,  8.70s/it] 75%|███████▌  | 30/40 [04:25<01:27,  8.70s/it] 78%|███████▊  | 31/40 [04:34<01:18,  8.72s/it] 80%|████████  | 32/40 [04:43<01:10,  8.84s/it] 82%|████████▎ | 33/40 [04:53<01:04,  9.20s/it] 85%|████████▌ | 34/40 [05:02<00:54,  9.14s/it] 88%|████████▊ | 35/40 [05:11<00:45,  9.04s/it] 90%|█████████ | 36/40 [05:19<00:35,  8.95s/it] 92%|█████████▎| 37/40 [05:28<00:26,  8.87s/it] 95%|█████████▌| 38/40 [05:37<00:17,  8.84s/it] 98%|█████████▊| 39/40 [05:45<00:08,  8.79s/it]100%|██████████| 40/40 [05:54<00:00,  8.70s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [05:54<00:00,  8.70s/it]100%|██████████| 40/40 [05:54<00:00,  8.86s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 354.4529, 'train_samples_per_second': 7.349, 'train_steps_per_second': 0.113, 'train_loss': 0.6981109619140625, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:02,  9.29s/it]  5%|▌         | 2/40 [00:19<06:05,  9.61s/it]  8%|▊         | 3/40 [00:28<05:55,  9.62s/it] 10%|█         | 4/40 [00:38<05:45,  9.59s/it] 12%|█▎        | 5/40 [00:47<05:35,  9.60s/it] 15%|█▌        | 6/40 [00:57<05:24,  9.55s/it] 18%|█▊        | 7/40 [01:06<05:11,  9.44s/it] 20%|██        | 8/40 [01:16<05:08,  9.63s/it] 22%|██▎       | 9/40 [01:28<05:15, 10.19s/it] 25%|██▌       | 10/40 [01:36<04:54,  9.81s/it] 28%|██▊       | 11/40 [01:46<04:42,  9.73s/it] 30%|███       | 12/40 [01:55<04:29,  9.64s/it] 32%|███▎      | 13/40 [02:05<04:20,  9.66s/it] 35%|███▌      | 14/40 [02:15<04:09,  9.59s/it] 38%|███▊      | 15/40 [02:25<04:02,  9.71s/it] 40%|████      | 16/40 [02:35<03:55,  9.81s/it] 42%|████▎     | 17/40 [02:45<03:51, 10.05s/it] 45%|████▌     | 18/40 [02:55<03:38,  9.93s/it] 48%|████▊     | 19/40 [03:04<03:25,  9.77s/it] 50%|█████     | 20/40 [03:14<03:13,  9.67s/it] 52%|█████▎    | 21/40 [03:24<03:04,  9.71s/it] 55%|█████▌    | 22/40 [03:33<02:53,  9.66s/it] 57%|█████▊    | 23/40 [03:43<02:44,  9.67s/it] 60%|██████    | 24/40 [03:53<02:36,  9.75s/it] 62%|██████▎   | 25/40 [04:04<02:33, 10.21s/it] 65%|██████▌   | 26/40 [04:13<02:19,  9.95s/it] 68%|██████▊   | 27/40 [04:23<02:08,  9.91s/it] 70%|███████   | 28/40 [04:32<01:56,  9.68s/it] 72%|███████▎  | 29/40 [04:42<01:45,  9.57s/it] 75%|███████▌  | 30/40 [04:51<01:35,  9.57s/it] 78%|███████▊  | 31/40 [05:01<01:26,  9.59s/it] 80%|████████  | 32/40 [05:11<01:17,  9.70s/it] 82%|████████▎ | 33/40 [05:22<01:10, 10.10s/it] 85%|████████▌ | 34/40 [05:32<01:00, 10.03s/it] 88%|████████▊ | 35/40 [05:41<00:49,  9.94s/it] 90%|█████████ | 36/40 [05:51<00:39,  9.83s/it] 92%|█████████▎| 37/40 [06:01<00:29,  9.75s/it] 95%|█████████▌| 38/40 [06:10<00:19,  9.72s/it] 98%|█████████▊| 39/40 [06:20<00:09,  9.68s/it]100%|██████████| 40/40 [06:29<00:00,  9.61s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:29<00:00,  9.61s/it]100%|██████████| 40/40 [06:29<00:00,  9.74s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 389.7411, 'train_samples_per_second': 6.684, 'train_steps_per_second': 0.103, 'train_loss': 0.7550222396850585, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:47,  5.84s/it]  5%|▌         | 2/40 [00:11<03:44,  5.92s/it]  8%|▊         | 3/40 [00:17<03:38,  5.91s/it] 10%|█         | 4/40 [00:23<03:31,  5.87s/it] 12%|█▎        | 5/40 [00:29<03:27,  5.93s/it] 15%|█▌        | 6/40 [00:35<03:17,  5.80s/it] 18%|█▊        | 7/40 [00:40<03:12,  5.82s/it] 20%|██        | 8/40 [00:46<03:04,  5.77s/it] 22%|██▎       | 9/40 [00:56<03:39,  7.07s/it] 25%|██▌       | 10/40 [01:02<03:19,  6.66s/it] 28%|██▊       | 11/40 [01:08<03:05,  6.39s/it] 30%|███       | 12/40 [01:13<02:52,  6.16s/it] 32%|███▎      | 13/40 [01:19<02:42,  6.00s/it] 35%|███▌      | 14/40 [01:25<02:35,  5.98s/it] 38%|███▊      | 15/40 [01:30<02:26,  5.87s/it] 40%|████      | 16/40 [01:36<02:22,  5.92s/it] 42%|████▎     | 17/40 [01:47<02:44,  7.17s/it] 45%|████▌     | 18/40 [01:52<02:28,  6.75s/it] 48%|████▊     | 19/40 [01:58<02:13,  6.38s/it] 50%|█████     | 20/40 [02:04<02:05,  6.27s/it] 52%|█████▎    | 21/40 [02:09<01:54,  6.05s/it] 55%|█████▌    | 22/40 [02:15<01:48,  6.05s/it] 57%|█████▊    | 23/40 [02:21<01:41,  5.95s/it] 60%|██████    | 24/40 [02:27<01:33,  5.86s/it] 62%|██████▎   | 25/40 [02:37<01:46,  7.12s/it] 65%|██████▌   | 26/40 [02:43<01:34,  6.72s/it] 68%|██████▊   | 27/40 [02:48<01:23,  6.44s/it] 70%|███████   | 28/40 [02:54<01:14,  6.18s/it] 72%|███████▎  | 29/40 [03:00<01:07,  6.17s/it] 75%|███████▌  | 30/40 [03:06<01:00,  6.09s/it] 78%|███████▊  | 31/40 [03:12<00:54,  6.02s/it] 80%|████████  | 32/40 [03:18<00:48,  6.01s/it] 82%|████████▎ | 33/40 [03:28<00:49,  7.12s/it] 85%|████████▌ | 34/40 [03:34<00:40,  6.76s/it] 88%|████████▊ | 35/40 [03:39<00:32,  6.42s/it] 90%|█████████ | 36/40 [03:45<00:24,  6.24s/it] 92%|█████████▎| 37/40 [03:51<00:18,  6.10s/it] 95%|█████████▌| 38/40 [03:56<00:11,  5.96s/it] 98%|█████████▊| 39/40 [04:02<00:05,  5.96s/it]100%|██████████| 40/40 [04:08<00:00,  5.76s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:08<00:00,  5.76s/it]100%|██████████| 40/40 [04:08<00:00,  6.20s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 248.1539, 'train_samples_per_second': 11.303, 'train_steps_per_second': 0.161, 'train_loss': 0.7464151382446289, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:45,  5.79s/it]  5%|▌         | 2/40 [00:11<03:44,  5.91s/it]  8%|▊         | 3/40 [00:17<03:40,  5.97s/it] 10%|█         | 4/40 [00:23<03:31,  5.86s/it] 12%|█▎        | 5/40 [00:29<03:27,  5.93s/it] 15%|█▌        | 6/40 [00:35<03:16,  5.78s/it] 18%|█▊        | 7/40 [00:40<03:11,  5.81s/it] 20%|██        | 8/40 [00:46<03:04,  5.77s/it] 22%|██▎       | 9/40 [00:56<03:39,  7.07s/it] 25%|██▌       | 10/40 [01:02<03:19,  6.66s/it] 28%|██▊       | 11/40 [01:08<03:05,  6.39s/it] 30%|███       | 12/40 [01:13<02:52,  6.16s/it] 32%|███▎      | 13/40 [01:19<02:42,  6.00s/it] 35%|███▌      | 14/40 [01:25<02:35,  5.98s/it] 38%|███▊      | 15/40 [01:30<02:26,  5.87s/it] 40%|████      | 16/40 [01:36<02:21,  5.91s/it] 42%|████▎     | 17/40 [01:46<02:44,  7.17s/it] 45%|████▌     | 18/40 [01:52<02:28,  6.74s/it] 48%|████▊     | 19/40 [01:58<02:13,  6.37s/it] 50%|█████     | 20/40 [02:04<02:05,  6.27s/it] 52%|█████▎    | 21/40 [02:09<01:54,  6.05s/it] 55%|█████▌    | 22/40 [02:15<01:48,  6.05s/it] 57%|█████▊    | 23/40 [02:21<01:41,  5.94s/it] 60%|██████    | 24/40 [02:27<01:33,  5.86s/it] 62%|██████▎   | 25/40 [02:36<01:45,  7.02s/it] 65%|██████▌   | 26/40 [02:42<01:32,  6.59s/it] 68%|██████▊   | 27/40 [02:48<01:22,  6.32s/it] 70%|███████   | 28/40 [02:53<01:13,  6.10s/it] 72%|███████▎  | 29/40 [02:59<01:07,  6.10s/it] 75%|███████▌  | 30/40 [03:05<01:00,  6.05s/it] 78%|███████▊  | 31/40 [03:11<00:53,  5.94s/it] 80%|████████  | 32/40 [03:17<00:47,  5.94s/it] 82%|████████▎ | 33/40 [03:27<00:49,  7.06s/it] 85%|████████▌ | 34/40 [03:32<00:40,  6.72s/it] 88%|████████▊ | 35/40 [03:38<00:31,  6.38s/it] 90%|█████████ | 36/40 [03:44<00:24,  6.21s/it] 92%|█████████▎| 37/40 [03:50<00:18,  6.09s/it] 95%|█████████▌| 38/40 [03:55<00:11,  5.95s/it] 98%|█████████▊| 39/40 [04:01<00:05,  5.96s/it]100%|██████████| 40/40 [04:07<00:00,  5.75s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:07<00:00,  5.75s/it]100%|██████████| 40/40 [04:07<00:00,  6.18s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 247.0955, 'train_samples_per_second': 11.352, 'train_steps_per_second': 0.162, 'train_loss': 0.750673484802246, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:22,  8.27s/it]  5%|▌         | 2/40 [00:16<05:16,  8.33s/it]  8%|▊         | 3/40 [00:24<05:07,  8.32s/it] 10%|█         | 4/40 [00:33<04:58,  8.29s/it] 12%|█▎        | 5/40 [00:41<04:53,  8.39s/it] 15%|█▌        | 6/40 [00:49<04:38,  8.20s/it] 18%|█▊        | 7/40 [00:58<04:33,  8.28s/it] 20%|██        | 8/40 [01:06<04:23,  8.22s/it] 22%|██▎       | 9/40 [01:20<05:12, 10.09s/it] 25%|██▌       | 10/40 [01:28<04:45,  9.52s/it] 28%|██▊       | 11/40 [01:36<04:25,  9.15s/it] 30%|███       | 12/40 [01:44<04:07,  8.83s/it] 32%|███▎      | 13/40 [01:53<03:53,  8.63s/it] 35%|███▌      | 14/40 [02:01<03:43,  8.61s/it] 38%|███▊      | 15/40 [02:09<03:30,  8.43s/it] 40%|████      | 16/40 [02:18<03:24,  8.52s/it] 42%|████▎     | 17/40 [02:32<03:56, 10.30s/it] 45%|████▌     | 18/40 [02:41<03:33,  9.69s/it] 48%|████▊     | 19/40 [02:49<03:12,  9.17s/it] 50%|█████     | 20/40 [02:57<03:00,  9.02s/it] 52%|█████▎    | 21/40 [03:05<02:44,  8.68s/it] 55%|█████▌    | 22/40 [03:14<02:36,  8.71s/it] 57%|█████▊    | 23/40 [03:22<02:25,  8.55s/it] 60%|██████    | 24/40 [03:30<02:15,  8.44s/it] 62%|██████▎   | 25/40 [03:44<02:31, 10.12s/it] 65%|██████▌   | 26/40 [03:52<02:12,  9.48s/it] 68%|██████▊   | 27/40 [04:00<01:57,  9.07s/it] 70%|███████   | 28/40 [04:08<01:44,  8.73s/it] 72%|███████▎  | 29/40 [04:17<01:36,  8.79s/it] 75%|███████▌  | 30/40 [04:26<01:27,  8.72s/it] 78%|███████▊  | 31/40 [04:34<01:16,  8.53s/it] 80%|████████  | 32/40 [04:43<01:08,  8.57s/it] 82%|████████▎ | 33/40 [04:56<01:11, 10.16s/it] 85%|████████▌ | 34/40 [05:05<00:57,  9.65s/it] 88%|████████▊ | 35/40 [05:13<00:45,  9.16s/it] 90%|█████████ | 36/40 [05:21<00:35,  8.94s/it] 92%|█████████▎| 37/40 [05:30<00:26,  8.79s/it] 95%|█████████▌| 38/40 [05:38<00:17,  8.58s/it] 98%|█████████▊| 39/40 [05:47<00:08,  8.59s/it]100%|██████████| 40/40 [05:54<00:00,  8.24s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [05:54<00:00,  8.24s/it]100%|██████████| 40/40 [05:54<00:00,  8.86s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 354.4676, 'train_samples_per_second': 7.913, 'train_steps_per_second': 0.113, 'train_loss': 0.7577497482299804, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:56,  9.13s/it]  5%|▌         | 2/40 [00:18<05:50,  9.23s/it]  8%|▊         | 3/40 [00:27<05:40,  9.20s/it] 10%|█         | 4/40 [00:36<05:29,  9.14s/it] 12%|█▎        | 5/40 [00:46<05:23,  9.25s/it] 15%|█▌        | 6/40 [00:54<05:08,  9.06s/it] 18%|█▊        | 7/40 [01:04<05:01,  9.14s/it] 20%|██        | 8/40 [01:13<04:51,  9.10s/it] 22%|██▎       | 9/40 [01:28<05:46, 11.18s/it] 25%|██▌       | 10/40 [01:37<05:16, 10.54s/it] 28%|██▊       | 11/40 [01:47<04:53, 10.13s/it] 30%|███       | 12/40 [01:56<04:33,  9.77s/it] 32%|███▎      | 13/40 [02:05<04:17,  9.55s/it] 35%|███▌      | 14/40 [02:14<04:07,  9.51s/it] 38%|███▊      | 15/40 [02:23<03:53,  9.33s/it] 40%|████      | 16/40 [02:33<03:45,  9.40s/it] 42%|████▎     | 17/40 [02:49<04:21, 11.39s/it] 45%|████▌     | 18/40 [02:58<03:55, 10.70s/it] 48%|████▊     | 19/40 [03:06<03:32, 10.13s/it] 50%|█████     | 20/40 [03:16<03:19,  9.96s/it] 52%|█████▎    | 21/40 [03:25<03:03,  9.64s/it] 55%|█████▌    | 22/40 [03:35<02:53,  9.64s/it] 57%|█████▊    | 23/40 [03:44<02:40,  9.46s/it] 60%|██████    | 24/40 [03:53<02:29,  9.34s/it] 62%|██████▎   | 25/40 [04:08<02:48, 11.20s/it] 65%|██████▌   | 26/40 [04:17<02:26, 10.50s/it] 68%|██████▊   | 27/40 [04:26<02:10, 10.05s/it] 70%|███████   | 28/40 [04:35<01:56,  9.68s/it] 72%|███████▎  | 29/40 [04:45<01:46,  9.71s/it] 75%|███████▌  | 30/40 [04:54<01:36,  9.62s/it] 78%|███████▊  | 31/40 [05:03<01:25,  9.45s/it] 80%|████████  | 32/40 [05:13<01:15,  9.46s/it] 82%|████████▎ | 33/40 [05:28<01:18, 11.25s/it] 85%|████████▌ | 34/40 [05:37<01:04, 10.68s/it] 88%|████████▊ | 35/40 [05:46<00:50, 10.15s/it] 90%|█████████ | 36/40 [05:56<00:39,  9.89s/it] 92%|█████████▎| 37/40 [06:05<00:29,  9.71s/it] 95%|█████████▌| 38/40 [06:14<00:18,  9.48s/it] 98%|█████████▊| 39/40 [06:23<00:09,  9.48s/it]100%|██████████| 40/40 [06:32<00:00,  9.14s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:32<00:00,  9.14s/it]100%|██████████| 40/40 [06:32<00:00,  9.80s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 392.1418, 'train_samples_per_second': 7.153, 'train_steps_per_second': 0.102, 'train_loss': 0.7824322700500488, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:42,  5.71s/it]  5%|▌         | 2/40 [00:11<03:38,  5.75s/it]  8%|▊         | 3/40 [00:17<03:34,  5.80s/it] 10%|█         | 4/40 [00:23<03:28,  5.78s/it] 12%|█▎        | 5/40 [00:29<03:24,  5.83s/it] 15%|█▌        | 6/40 [00:34<03:16,  5.77s/it] 18%|█▊        | 7/40 [00:40<03:10,  5.77s/it] 20%|██        | 8/40 [00:46<03:03,  5.73s/it] 22%|██▎       | 9/40 [00:55<03:38,  7.03s/it] 25%|██▌       | 10/40 [01:01<03:18,  6.61s/it] 28%|██▊       | 11/40 [01:07<03:04,  6.36s/it] 30%|███       | 12/40 [01:13<02:53,  6.19s/it] 32%|███▎      | 13/40 [01:18<02:43,  6.06s/it] 35%|███▌      | 14/40 [01:25<02:37,  6.06s/it] 38%|███▊      | 15/40 [01:30<02:28,  5.96s/it] 40%|████      | 16/40 [01:36<02:23,  5.98s/it] 42%|████▎     | 17/40 [01:46<02:46,  7.22s/it] 45%|████▌     | 18/40 [01:52<02:30,  6.82s/it] 48%|████▊     | 19/40 [01:58<02:15,  6.46s/it] 50%|█████     | 20/40 [02:04<02:05,  6.29s/it] 52%|█████▎    | 21/40 [02:09<01:55,  6.09s/it] 55%|█████▌    | 22/40 [02:16<01:49,  6.10s/it] 57%|█████▊    | 23/40 [02:21<01:41,  5.94s/it] 60%|██████    | 24/40 [02:27<01:33,  5.86s/it] 62%|██████▎   | 25/40 [02:37<01:45,  7.06s/it] 65%|██████▌   | 26/40 [02:42<01:32,  6.63s/it] 68%|██████▊   | 27/40 [02:48<01:22,  6.38s/it] 70%|███████   | 28/40 [02:54<01:14,  6.25s/it] 72%|███████▎  | 29/40 [03:00<01:07,  6.18s/it] 75%|███████▌  | 30/40 [03:06<01:00,  6.01s/it] 78%|███████▊  | 31/40 [03:11<00:52,  5.88s/it] 80%|████████  | 32/40 [03:17<00:46,  5.87s/it] 82%|████████▎ | 33/40 [03:27<00:49,  7.04s/it] 85%|████████▌ | 34/40 [03:33<00:40,  6.67s/it] 88%|████████▊ | 35/40 [03:38<00:32,  6.41s/it] 90%|█████████ | 36/40 [03:44<00:24,  6.19s/it] 92%|█████████▎| 37/40 [03:50<00:17,  5.94s/it] 95%|█████████▌| 38/40 [03:55<00:11,  5.87s/it] 98%|█████████▊| 39/40 [04:01<00:05,  5.89s/it]100%|██████████| 40/40 [04:07<00:00,  5.85s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:07<00:00,  5.85s/it]100%|██████████| 40/40 [04:07<00:00,  6.19s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 247.4195, 'train_samples_per_second': 11.337, 'train_steps_per_second': 0.162, 'train_loss': 0.7462547302246094, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:42,  5.70s/it]  5%|▌         | 2/40 [00:11<03:37,  5.73s/it]  8%|▊         | 3/40 [00:17<03:34,  5.79s/it] 10%|█         | 4/40 [00:23<03:27,  5.77s/it] 12%|█▎        | 5/40 [00:28<03:23,  5.82s/it] 15%|█▌        | 6/40 [00:34<03:15,  5.76s/it] 18%|█▊        | 7/40 [00:40<03:09,  5.76s/it] 20%|██        | 8/40 [00:45<03:02,  5.71s/it] 22%|██▎       | 9/40 [00:55<03:37,  7.02s/it] 25%|██▌       | 10/40 [01:01<03:17,  6.60s/it] 28%|██▊       | 11/40 [01:07<03:04,  6.35s/it] 30%|███       | 12/40 [01:12<02:52,  6.15s/it] 32%|███▎      | 13/40 [01:18<02:41,  5.98s/it] 35%|███▌      | 14/40 [01:24<02:35,  5.97s/it] 38%|███▊      | 15/40 [01:30<02:27,  5.89s/it] 40%|████      | 16/40 [01:36<02:22,  5.93s/it] 42%|████▎     | 17/40 [01:46<02:45,  7.18s/it] 45%|████▌     | 18/40 [01:52<02:29,  6.79s/it] 48%|████▊     | 19/40 [01:57<02:15,  6.43s/it] 50%|█████     | 20/40 [02:03<02:05,  6.27s/it] 52%|█████▎    | 21/40 [02:09<01:55,  6.07s/it] 55%|█████▌    | 22/40 [02:15<01:49,  6.08s/it] 57%|█████▊    | 23/40 [02:20<01:40,  5.93s/it] 60%|██████    | 24/40 [02:26<01:33,  5.85s/it] 62%|██████▎   | 25/40 [02:36<01:45,  7.05s/it] 65%|██████▌   | 26/40 [02:42<01:32,  6.62s/it] 68%|██████▊   | 27/40 [02:47<01:22,  6.37s/it] 70%|███████   | 28/40 [02:53<01:14,  6.23s/it] 72%|███████▎  | 29/40 [02:59<01:07,  6.17s/it] 75%|███████▌  | 30/40 [03:05<01:00,  6.00s/it] 78%|███████▊  | 31/40 [03:10<00:52,  5.87s/it] 80%|████████  | 32/40 [03:16<00:46,  5.86s/it] 82%|████████▎ | 33/40 [03:26<00:49,  7.03s/it] 85%|████████▌ | 34/40 [03:32<00:39,  6.66s/it] 88%|████████▊ | 35/40 [03:38<00:31,  6.40s/it] 90%|█████████ | 36/40 [03:43<00:24,  6.18s/it] 92%|█████████▎| 37/40 [03:49<00:17,  5.94s/it] 95%|█████████▌| 38/40 [03:54<00:11,  5.86s/it] 98%|█████████▊| 39/40 [04:00<00:05,  5.88s/it]100%|██████████| 40/40 [04:06<00:00,  5.84s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:06<00:00,  5.84s/it]100%|██████████| 40/40 [04:06<00:00,  6.16s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 246.583, 'train_samples_per_second': 11.375, 'train_steps_per_second': 0.162, 'train_loss': 0.750364875793457, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:20,  8.22s/it]  5%|▌         | 2/40 [00:16<05:13,  8.24s/it]  8%|▊         | 3/40 [00:24<05:08,  8.33s/it] 10%|█         | 4/40 [00:33<04:59,  8.33s/it] 12%|█▎        | 5/40 [00:41<04:53,  8.38s/it] 15%|█▌        | 6/40 [00:49<04:42,  8.31s/it] 18%|█▊        | 7/40 [00:58<04:34,  8.30s/it] 20%|██        | 8/40 [01:06<04:22,  8.22s/it] 22%|██▎       | 9/40 [01:20<05:11, 10.06s/it] 25%|██▌       | 10/40 [01:28<04:44,  9.47s/it] 28%|██▊       | 11/40 [01:36<04:24,  9.13s/it] 30%|███       | 12/40 [01:44<04:07,  8.83s/it] 32%|███▎      | 13/40 [01:53<03:51,  8.59s/it] 35%|███▌      | 14/40 [02:01<03:43,  8.59s/it] 38%|███▊      | 15/40 [02:09<03:32,  8.48s/it] 40%|████      | 16/40 [02:18<03:25,  8.55s/it] 42%|████▎     | 17/40 [02:33<03:57, 10.34s/it] 45%|████▌     | 18/40 [02:41<03:34,  9.76s/it] 48%|████▊     | 19/40 [02:49<03:14,  9.25s/it] 50%|█████     | 20/40 [02:58<03:00,  9.03s/it] 52%|█████▎    | 21/40 [03:06<02:46,  8.75s/it] 55%|█████▌    | 22/40 [03:15<02:38,  8.80s/it] 57%|█████▊    | 23/40 [03:23<02:25,  8.57s/it] 60%|██████    | 24/40 [03:31<02:15,  8.44s/it] 62%|██████▎   | 25/40 [03:45<02:32, 10.15s/it] 65%|██████▌   | 26/40 [03:53<02:13,  9.52s/it] 68%|██████▊   | 27/40 [04:01<01:58,  9.15s/it] 70%|███████   | 28/40 [04:10<01:47,  8.97s/it] 72%|███████▎  | 29/40 [04:19<01:38,  8.91s/it] 75%|███████▌  | 30/40 [04:27<01:26,  8.65s/it] 78%|███████▊  | 31/40 [04:35<01:15,  8.44s/it] 80%|████████  | 32/40 [04:43<01:07,  8.43s/it] 82%|████████▎ | 33/40 [04:57<01:10, 10.11s/it] 85%|████████▌ | 34/40 [05:05<00:57,  9.58s/it] 88%|████████▊ | 35/40 [05:14<00:46,  9.21s/it] 90%|█████████ | 36/40 [05:22<00:35,  8.88s/it] 92%|█████████▎| 37/40 [05:29<00:25,  8.53s/it] 95%|█████████▌| 38/40 [05:38<00:16,  8.42s/it] 98%|█████████▊| 39/40 [05:46<00:08,  8.47s/it]100%|██████████| 40/40 [05:54<00:00,  8.41s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [05:54<00:00,  8.41s/it]100%|██████████| 40/40 [05:54<00:00,  8.87s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 354.9939, 'train_samples_per_second': 7.902, 'train_steps_per_second': 0.113, 'train_loss': 0.7576642990112304, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:53,  9.07s/it]  5%|▌         | 2/40 [00:18<05:45,  9.09s/it]  8%|▊         | 3/40 [00:27<05:39,  9.17s/it] 10%|█         | 4/40 [00:36<05:30,  9.17s/it] 12%|█▎        | 5/40 [00:45<05:23,  9.24s/it] 15%|█▌        | 6/40 [00:54<05:11,  9.16s/it] 18%|█▊        | 7/40 [01:04<05:02,  9.16s/it] 20%|██        | 8/40 [01:13<04:50,  9.08s/it] 22%|██▎       | 9/40 [01:28<05:45, 11.13s/it] 25%|██▌       | 10/40 [01:37<05:14, 10.47s/it] 28%|██▊       | 11/40 [01:46<04:52, 10.10s/it] 30%|███       | 12/40 [01:55<04:33,  9.75s/it] 32%|███▎      | 13/40 [02:04<04:16,  9.50s/it] 35%|███▌      | 14/40 [02:14<04:06,  9.48s/it] 38%|███▊      | 15/40 [02:23<03:54,  9.36s/it] 40%|████      | 16/40 [02:32<03:45,  9.41s/it] 42%|████▎     | 17/40 [02:48<04:22, 11.40s/it] 45%|████▌     | 18/40 [02:58<03:57, 10.77s/it] 48%|████▊     | 19/40 [03:07<03:34, 10.22s/it] 50%|█████     | 20/40 [03:16<03:19,  9.96s/it] 52%|█████▎    | 21/40 [03:25<03:03,  9.65s/it] 55%|█████▌    | 22/40 [03:35<02:54,  9.68s/it] 57%|█████▊    | 23/40 [03:44<02:40,  9.43s/it] 60%|██████    | 24/40 [03:53<02:28,  9.31s/it] 62%|██████▎   | 25/40 [04:08<02:47, 11.20s/it] 65%|██████▌   | 26/40 [04:17<02:27, 10.51s/it] 68%|██████▊   | 27/40 [04:26<02:11, 10.11s/it] 70%|███████   | 28/40 [04:36<01:58,  9.89s/it] 72%|███████▎  | 29/40 [04:45<01:47,  9.81s/it] 75%|███████▌  | 30/40 [04:54<01:35,  9.54s/it] 78%|███████▊  | 31/40 [05:03<01:23,  9.33s/it] 80%|████████  | 32/40 [05:12<01:14,  9.32s/it] 82%|████████▎ | 33/40 [05:28<01:18, 11.18s/it] 85%|████████▌ | 34/40 [05:37<01:03, 10.59s/it] 88%|████████▊ | 35/40 [05:46<00:50, 10.17s/it] 90%|█████████ | 36/40 [05:55<00:39,  9.82s/it] 92%|█████████▎| 37/40 [06:04<00:28,  9.44s/it] 95%|█████████▌| 38/40 [06:13<00:18,  9.32s/it] 98%|█████████▊| 39/40 [06:22<00:09,  9.35s/it]100%|██████████| 40/40 [06:31<00:00,  9.29s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:31<00:00,  9.29s/it]100%|██████████| 40/40 [06:31<00:00,  9.80s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 391.871, 'train_samples_per_second': 7.158, 'train_steps_per_second': 0.102, 'train_loss': 0.7828170776367187, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:02,  6.23s/it]  5%|▌         | 2/40 [00:12<03:46,  5.97s/it]  8%|▊         | 3/40 [00:18<03:41,  5.99s/it] 10%|█         | 4/40 [00:23<03:35,  5.98s/it] 12%|█▎        | 5/40 [00:29<03:28,  5.97s/it] 15%|█▌        | 6/40 [00:35<03:20,  5.90s/it] 18%|█▊        | 7/40 [00:41<03:10,  5.76s/it] 20%|██        | 8/40 [00:46<03:03,  5.74s/it] 22%|██▎       | 9/40 [00:57<03:40,  7.13s/it] 25%|██▌       | 10/40 [01:03<03:23,  6.78s/it] 28%|██▊       | 11/40 [01:08<03:07,  6.47s/it] 30%|███       | 12/40 [01:14<02:55,  6.26s/it] 32%|███▎      | 13/40 [01:20<02:44,  6.11s/it] 35%|███▌      | 14/40 [01:26<02:37,  6.05s/it] 38%|███▊      | 15/40 [01:31<02:27,  5.91s/it] 40%|████      | 16/40 [01:37<02:21,  5.90s/it] 42%|████▎     | 17/40 [01:48<02:47,  7.27s/it] 45%|████▌     | 18/40 [01:53<02:29,  6.78s/it] 48%|████▊     | 19/40 [01:59<02:15,  6.47s/it] 50%|█████     | 20/40 [02:05<02:06,  6.33s/it] 52%|█████▎    | 21/40 [02:11<01:57,  6.18s/it] 55%|█████▌    | 22/40 [02:17<01:50,  6.14s/it] 57%|█████▊    | 23/40 [02:23<01:42,  6.00s/it] 60%|██████    | 24/40 [02:28<01:34,  5.89s/it] 62%|██████▎   | 25/40 [02:38<01:46,  7.13s/it] 65%|██████▌   | 26/40 [02:44<01:33,  6.69s/it] 68%|██████▊   | 27/40 [02:50<01:24,  6.49s/it] 70%|███████   | 28/40 [02:56<01:15,  6.29s/it] 72%|███████▎  | 29/40 [03:02<01:07,  6.16s/it] 75%|███████▌  | 30/40 [03:08<01:00,  6.09s/it] 78%|███████▊  | 31/40 [03:13<00:53,  5.95s/it] 80%|████████  | 32/40 [03:19<00:47,  6.00s/it] 82%|████████▎ | 33/40 [03:29<00:50,  7.14s/it] 85%|████████▌ | 34/40 [03:35<00:40,  6.76s/it] 88%|████████▊ | 35/40 [03:41<00:32,  6.50s/it] 90%|█████████ | 36/40 [03:47<00:25,  6.28s/it] 92%|█████████▎| 37/40 [03:52<00:18,  6.06s/it] 95%|█████████▌| 38/40 [03:58<00:11,  5.94s/it] 98%|█████████▊| 39/40 [04:04<00:05,  5.95s/it]100%|██████████| 40/40 [04:10<00:00,  5.99s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:10<00:00,  5.99s/it]100%|██████████| 40/40 [04:10<00:00,  6.26s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 250.4351, 'train_samples_per_second': 11.201, 'train_steps_per_second': 0.16, 'train_loss': 0.7466821670532227, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:55,  6.03s/it]  5%|▌         | 2/40 [00:11<03:41,  5.82s/it]  8%|▊         | 3/40 [00:17<03:36,  5.85s/it] 10%|█         | 4/40 [00:23<03:32,  5.89s/it] 12%|█▎        | 5/40 [00:29<03:26,  5.91s/it] 15%|█▌        | 6/40 [00:35<03:19,  5.86s/it] 18%|█▊        | 7/40 [00:40<03:09,  5.73s/it] 20%|██        | 8/40 [00:46<03:03,  5.72s/it] 22%|██▎       | 9/40 [00:56<03:40,  7.11s/it] 25%|██▌       | 10/40 [01:02<03:22,  6.76s/it] 28%|██▊       | 11/40 [01:08<03:07,  6.46s/it] 30%|███       | 12/40 [01:14<02:55,  6.26s/it] 32%|███▎      | 13/40 [01:19<02:44,  6.10s/it] 35%|███▌      | 14/40 [01:25<02:37,  6.05s/it] 38%|███▊      | 15/40 [01:31<02:27,  5.91s/it] 40%|████      | 16/40 [01:37<02:21,  5.90s/it] 42%|████▎     | 17/40 [01:47<02:47,  7.27s/it] 45%|████▌     | 18/40 [01:53<02:29,  6.78s/it] 48%|████▊     | 19/40 [01:59<02:15,  6.46s/it] 50%|█████     | 20/40 [02:05<02:06,  6.32s/it] 52%|█████▎    | 21/40 [02:10<01:57,  6.18s/it] 55%|█████▌    | 22/40 [02:16<01:50,  6.13s/it] 57%|█████▊    | 23/40 [02:22<01:41,  6.00s/it] 60%|██████    | 24/40 [02:28<01:34,  5.88s/it] 62%|██████▎   | 25/40 [02:38<01:46,  7.12s/it] 65%|██████▌   | 26/40 [02:43<01:33,  6.68s/it] 68%|██████▊   | 27/40 [02:49<01:24,  6.49s/it] 70%|███████   | 28/40 [02:55<01:15,  6.29s/it] 72%|███████▎  | 29/40 [03:01<01:07,  6.15s/it] 75%|███████▌  | 30/40 [03:07<01:00,  6.08s/it] 78%|███████▊  | 31/40 [03:13<00:53,  5.94s/it] 80%|████████  | 32/40 [03:19<00:47,  5.99s/it] 82%|████████▎ | 33/40 [03:29<00:49,  7.14s/it] 85%|████████▌ | 34/40 [03:34<00:40,  6.76s/it] 88%|████████▊ | 35/40 [03:40<00:32,  6.49s/it] 90%|█████████ | 36/40 [03:46<00:25,  6.28s/it] 92%|█████████▎| 37/40 [03:52<00:18,  6.06s/it] 95%|█████████▌| 38/40 [03:57<00:11,  5.94s/it] 98%|█████████▊| 39/40 [04:03<00:05,  5.95s/it]100%|██████████| 40/40 [04:09<00:00,  5.98s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:09<00:00,  5.98s/it]100%|██████████| 40/40 [04:09<00:00,  6.24s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 249.7787, 'train_samples_per_second': 11.23, 'train_steps_per_second': 0.16, 'train_loss': 0.7505592346191406, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:43,  8.82s/it]  5%|▌         | 2/40 [00:16<05:18,  8.38s/it]  8%|▊         | 3/40 [00:25<05:12,  8.44s/it] 10%|█         | 4/40 [00:34<05:06,  8.51s/it] 12%|█▎        | 5/40 [00:42<04:58,  8.52s/it] 15%|█▌        | 6/40 [00:50<04:47,  8.45s/it] 18%|█▊        | 7/40 [00:58<04:31,  8.24s/it] 20%|██        | 8/40 [01:06<04:23,  8.22s/it] 22%|██▎       | 9/40 [01:21<05:16, 10.21s/it] 25%|██▌       | 10/40 [01:30<04:52,  9.74s/it] 28%|██▊       | 11/40 [01:38<04:30,  9.32s/it] 30%|███       | 12/40 [01:46<04:11,  8.99s/it] 32%|███▎      | 13/40 [01:55<03:57,  8.79s/it] 35%|███▌      | 14/40 [02:03<03:46,  8.72s/it] 38%|███▊      | 15/40 [02:11<03:32,  8.51s/it] 40%|████      | 16/40 [02:20<03:23,  8.49s/it] 42%|████▎     | 17/40 [02:35<04:00, 10.47s/it] 45%|████▌     | 18/40 [02:43<03:34,  9.74s/it] 48%|████▊     | 19/40 [02:51<03:15,  9.32s/it] 50%|█████     | 20/40 [03:00<03:02,  9.12s/it] 52%|█████▎    | 21/40 [03:08<02:49,  8.92s/it] 55%|█████▌    | 22/40 [03:17<02:39,  8.85s/it] 57%|█████▊    | 23/40 [03:25<02:27,  8.65s/it] 60%|██████    | 24/40 [03:33<02:15,  8.48s/it] 62%|██████▎   | 25/40 [03:48<02:33, 10.26s/it] 65%|██████▌   | 26/40 [03:56<02:14,  9.59s/it] 68%|██████▊   | 27/40 [04:04<02:01,  9.33s/it] 70%|███████   | 28/40 [04:13<01:48,  9.04s/it] 72%|███████▎  | 29/40 [04:21<01:37,  8.87s/it] 75%|███████▌  | 30/40 [04:30<01:27,  8.79s/it] 78%|███████▊  | 31/40 [04:38<01:17,  8.56s/it] 80%|████████  | 32/40 [04:47<01:09,  8.64s/it] 82%|████████▎ | 33/40 [05:01<01:12, 10.32s/it] 85%|████████▌ | 34/40 [05:10<00:59,  9.86s/it] 88%|████████▊ | 35/40 [05:18<00:47,  9.45s/it] 90%|█████████ | 36/40 [05:26<00:36,  9.10s/it] 92%|█████████▎| 37/40 [05:34<00:26,  8.76s/it] 95%|█████████▌| 38/40 [05:42<00:17,  8.57s/it] 98%|█████████▊| 39/40 [05:51<00:08,  8.59s/it]100%|██████████| 40/40 [06:00<00:00,  8.65s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:00<00:00,  8.65s/it]100%|██████████| 40/40 [06:00<00:00,  9.01s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 360.3773, 'train_samples_per_second': 7.784, 'train_steps_per_second': 0.111, 'train_loss': 0.7576536178588867, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:17,  9.68s/it]  5%|▌         | 2/40 [00:18<05:51,  9.26s/it]  8%|▊         | 3/40 [00:28<05:44,  9.30s/it] 10%|█         | 4/40 [00:37<05:37,  9.37s/it] 12%|█▎        | 5/40 [00:46<05:28,  9.38s/it] 15%|█▌        | 6/40 [00:56<05:16,  9.31s/it] 18%|█▊        | 7/40 [01:04<05:00,  9.10s/it] 20%|██        | 8/40 [01:13<04:50,  9.09s/it] 22%|██▎       | 9/40 [01:29<05:49, 11.29s/it] 25%|██▌       | 10/40 [01:39<05:22, 10.75s/it] 28%|██▊       | 11/40 [01:48<04:58, 10.28s/it] 30%|███       | 12/40 [01:57<04:38,  9.93s/it] 32%|███▎      | 13/40 [02:06<04:21,  9.70s/it] 35%|███▌      | 14/40 [02:16<04:09,  9.61s/it] 38%|███▊      | 15/40 [02:25<03:54,  9.39s/it] 40%|████      | 16/40 [02:34<03:45,  9.38s/it] 42%|████▎     | 17/40 [02:51<04:25, 11.56s/it] 45%|████▌     | 18/40 [03:00<03:57, 10.78s/it] 48%|████▊     | 19/40 [03:09<03:36, 10.30s/it] 50%|█████     | 20/40 [03:18<03:21, 10.07s/it] 52%|█████▎    | 21/40 [03:28<03:06,  9.84s/it] 55%|█████▌    | 22/40 [03:37<02:55,  9.76s/it] 57%|█████▊    | 23/40 [03:46<02:42,  9.55s/it] 60%|██████    | 24/40 [03:55<02:29,  9.36s/it] 62%|██████▎   | 25/40 [04:11<02:50, 11.33s/it] 65%|██████▌   | 26/40 [04:20<02:28, 10.62s/it] 68%|██████▊   | 27/40 [04:30<02:14, 10.31s/it] 70%|███████   | 28/40 [04:39<01:59,  9.99s/it] 72%|███████▎  | 29/40 [04:48<01:47,  9.79s/it] 75%|███████▌  | 30/40 [04:58<01:36,  9.69s/it] 78%|███████▊  | 31/40 [05:07<01:25,  9.46s/it] 80%|████████  | 32/40 [05:16<01:16,  9.53s/it] 82%|████████▎ | 33/40 [05:32<01:19, 11.35s/it] 85%|████████▌ | 34/40 [05:41<01:04, 10.74s/it] 88%|████████▊ | 35/40 [05:51<00:51, 10.31s/it] 90%|█████████ | 36/40 [06:00<00:39,  9.98s/it] 92%|█████████▎| 37/40 [06:09<00:28,  9.64s/it] 95%|█████████▌| 38/40 [06:18<00:18,  9.45s/it] 98%|█████████▊| 39/40 [06:27<00:09,  9.46s/it]100%|██████████| 40/40 [06:37<00:00,  9.52s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:37<00:00,  9.52s/it]100%|██████████| 40/40 [06:37<00:00,  9.93s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 397.3366, 'train_samples_per_second': 7.06, 'train_steps_per_second': 0.101, 'train_loss': 0.7828004837036133, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:54,  6.02s/it]  5%|▌         | 2/40 [00:12<03:56,  6.23s/it]  8%|▊         | 3/40 [00:18<03:43,  6.05s/it] 10%|█         | 4/40 [00:24<03:40,  6.12s/it] 12%|█▎        | 5/40 [00:30<03:35,  6.15s/it] 15%|█▌        | 6/40 [00:36<03:26,  6.06s/it] 18%|█▊        | 7/40 [00:42<03:18,  6.01s/it] 20%|██        | 8/40 [00:48<03:15,  6.10s/it] 22%|██▎       | 9/40 [00:55<03:16,  6.35s/it] 25%|██▌       | 10/40 [01:01<03:08,  6.27s/it] 28%|██▊       | 11/40 [01:07<03:00,  6.22s/it] 30%|███       | 12/40 [01:13<02:50,  6.09s/it] 32%|███▎      | 13/40 [01:19<02:44,  6.09s/it] 35%|███▌      | 14/40 [01:25<02:38,  6.11s/it] 38%|███▊      | 15/40 [01:32<02:34,  6.16s/it] 40%|████      | 16/40 [01:38<02:29,  6.21s/it] 42%|████▎     | 17/40 [01:45<02:27,  6.40s/it] 45%|████▌     | 18/40 [01:51<02:19,  6.36s/it] 48%|████▊     | 19/40 [01:57<02:11,  6.28s/it] 50%|█████     | 20/40 [02:03<02:04,  6.25s/it] 52%|█████▎    | 21/40 [02:10<01:58,  6.26s/it] 55%|█████▌    | 22/40 [02:15<01:49,  6.11s/it] 57%|█████▊    | 23/40 [02:21<01:43,  6.09s/it] 60%|██████    | 24/40 [02:28<01:38,  6.18s/it] 62%|██████▎   | 25/40 [02:35<01:36,  6.44s/it] 65%|██████▌   | 26/40 [02:41<01:28,  6.29s/it] 68%|██████▊   | 27/40 [02:47<01:21,  6.24s/it] 70%|███████   | 28/40 [02:53<01:14,  6.17s/it] 72%|███████▎  | 29/40 [02:59<01:07,  6.14s/it] 75%|███████▌  | 30/40 [03:05<01:01,  6.15s/it] 78%|███████▊  | 31/40 [03:11<00:55,  6.17s/it] 80%|████████  | 32/40 [03:17<00:48,  6.10s/it] 82%|████████▎ | 33/40 [03:24<00:44,  6.32s/it] 85%|████████▌ | 34/40 [03:31<00:37,  6.33s/it] 88%|████████▊ | 35/40 [03:37<00:31,  6.32s/it] 90%|█████████ | 36/40 [03:43<00:24,  6.23s/it] 92%|█████████▎| 37/40 [03:49<00:18,  6.14s/it] 95%|█████████▌| 38/40 [03:55<00:12,  6.14s/it] 98%|█████████▊| 39/40 [04:01<00:06,  6.17s/it]100%|██████████| 40/40 [04:07<00:00,  6.10s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:07<00:00,  6.10s/it]100%|██████████| 40/40 [04:07<00:00,  6.19s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 247.6, 'train_samples_per_second': 10.521, 'train_steps_per_second': 0.162, 'train_loss': 0.6987804412841797, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:55,  6.03s/it]  5%|▌         | 2/40 [00:12<03:57,  6.25s/it]  8%|▊         | 3/40 [00:18<03:44,  6.06s/it] 10%|█         | 4/40 [00:24<03:40,  6.13s/it] 12%|█▎        | 5/40 [00:30<03:35,  6.16s/it] 15%|█▌        | 6/40 [00:36<03:26,  6.07s/it] 18%|█▊        | 7/40 [00:42<03:18,  6.02s/it] 20%|██        | 8/40 [00:48<03:15,  6.11s/it] 22%|██▎       | 9/40 [00:55<03:17,  6.36s/it] 25%|██▌       | 10/40 [01:01<03:08,  6.28s/it] 28%|██▊       | 11/40 [01:07<03:00,  6.23s/it] 30%|███       | 12/40 [01:13<02:50,  6.10s/it] 32%|███▎      | 13/40 [01:19<02:44,  6.10s/it] 35%|███▌      | 14/40 [01:25<02:38,  6.11s/it] 38%|███▊      | 15/40 [01:32<02:34,  6.17s/it] 40%|████      | 16/40 [01:38<02:29,  6.22s/it] 42%|████▎     | 17/40 [01:45<02:27,  6.41s/it] 45%|████▌     | 18/40 [01:51<02:19,  6.36s/it] 48%|████▊     | 19/40 [01:57<02:12,  6.29s/it] 50%|█████     | 20/40 [02:04<02:05,  6.25s/it] 52%|█████▎    | 21/40 [02:10<01:59,  6.27s/it] 55%|█████▌    | 22/40 [02:16<01:50,  6.12s/it] 57%|█████▊    | 23/40 [02:22<01:43,  6.10s/it] 60%|██████    | 24/40 [02:28<01:39,  6.20s/it] 62%|██████▎   | 25/40 [02:35<01:36,  6.45s/it] 65%|██████▌   | 26/40 [02:41<01:28,  6.31s/it] 68%|██████▊   | 27/40 [02:47<01:21,  6.25s/it] 70%|███████   | 28/40 [02:53<01:14,  6.18s/it] 72%|███████▎  | 29/40 [02:59<01:07,  6.15s/it] 75%|███████▌  | 30/40 [03:06<01:01,  6.16s/it] 78%|███████▊  | 31/40 [03:12<00:55,  6.18s/it] 80%|████████  | 32/40 [03:18<00:48,  6.11s/it] 82%|████████▎ | 33/40 [03:25<00:44,  6.34s/it] 85%|████████▌ | 34/40 [03:31<00:38,  6.34s/it] 88%|████████▊ | 35/40 [03:37<00:31,  6.33s/it] 90%|█████████ | 36/40 [03:43<00:24,  6.24s/it] 92%|█████████▎| 37/40 [03:49<00:18,  6.15s/it] 95%|█████████▌| 38/40 [03:55<00:12,  6.15s/it] 98%|█████████▊| 39/40 [04:02<00:06,  6.18s/it]100%|██████████| 40/40 [04:08<00:00,  6.11s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:08<00:00,  6.11s/it]100%|██████████| 40/40 [04:08<00:00,  6.20s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 248.0204, 'train_samples_per_second': 10.503, 'train_steps_per_second': 0.161, 'train_loss': 0.7158161163330078, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:38,  8.69s/it]  5%|▌         | 2/40 [00:17<05:42,  9.03s/it]  8%|▊         | 3/40 [00:26<05:23,  8.75s/it] 10%|█         | 4/40 [00:35<05:20,  8.90s/it] 12%|█▎        | 5/40 [00:44<05:12,  8.93s/it] 15%|█▌        | 6/40 [00:52<04:57,  8.75s/it] 18%|█▊        | 7/40 [01:01<04:46,  8.68s/it] 20%|██        | 8/40 [01:10<04:42,  8.84s/it] 22%|██▎       | 9/40 [01:20<04:44,  9.19s/it] 25%|██▌       | 10/40 [01:29<04:33,  9.10s/it] 28%|██▊       | 11/40 [01:38<04:21,  9.03s/it] 30%|███       | 12/40 [01:46<04:05,  8.78s/it] 32%|███▎      | 13/40 [01:55<03:57,  8.78s/it] 35%|███▌      | 14/40 [02:04<03:48,  8.80s/it] 38%|███▊      | 15/40 [02:13<03:42,  8.91s/it] 40%|████      | 16/40 [02:22<03:36,  9.00s/it] 42%|████▎     | 17/40 [02:32<03:32,  9.24s/it] 45%|████▌     | 18/40 [02:41<03:22,  9.18s/it] 48%|████▊     | 19/40 [02:50<03:11,  9.10s/it] 50%|█████     | 20/40 [02:59<03:00,  9.05s/it] 52%|█████▎    | 21/40 [03:08<02:52,  9.10s/it] 55%|█████▌    | 22/40 [03:16<02:39,  8.84s/it] 57%|█████▊    | 23/40 [03:25<02:29,  8.81s/it] 60%|██████    | 24/40 [03:34<02:23,  8.97s/it] 62%|██████▎   | 25/40 [03:44<02:20,  9.35s/it] 65%|██████▌   | 26/40 [03:53<02:07,  9.09s/it] 68%|██████▊   | 27/40 [04:02<01:57,  9.03s/it] 70%|███████   | 28/40 [04:11<01:47,  8.94s/it] 72%|███████▎  | 29/40 [04:19<01:37,  8.88s/it] 75%|███████▌  | 30/40 [04:28<01:28,  8.89s/it] 78%|███████▊  | 31/40 [04:37<01:20,  8.94s/it] 80%|████████  | 32/40 [04:46<01:10,  8.86s/it] 82%|████████▎ | 33/40 [04:56<01:04,  9.15s/it] 85%|████████▌ | 34/40 [05:05<00:55,  9.18s/it] 88%|████████▊ | 35/40 [05:14<00:45,  9.17s/it] 90%|█████████ | 36/40 [05:23<00:36,  9.04s/it] 92%|█████████▎| 37/40 [05:32<00:26,  8.91s/it] 95%|█████████▌| 38/40 [05:40<00:17,  8.89s/it] 98%|█████████▊| 39/40 [05:49<00:08,  8.94s/it]100%|██████████| 40/40 [05:58<00:00,  8.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [05:58<00:00,  8.82s/it]100%|██████████| 40/40 [05:58<00:00,  8.96s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 358.4768, 'train_samples_per_second': 7.267, 'train_steps_per_second': 0.112, 'train_loss': 0.6984512329101562, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:13,  9.58s/it]  5%|▌         | 2/40 [00:19<06:17,  9.93s/it]  8%|▊         | 3/40 [00:29<05:57,  9.66s/it] 10%|█         | 4/40 [00:39<05:51,  9.77s/it] 12%|█▎        | 5/40 [00:48<05:43,  9.82s/it] 15%|█▌        | 6/40 [00:58<05:28,  9.65s/it] 18%|█▊        | 7/40 [01:07<05:15,  9.57s/it] 20%|██        | 8/40 [01:17<05:11,  9.72s/it] 22%|██▎       | 9/40 [01:28<05:13, 10.11s/it] 25%|██▌       | 10/40 [01:38<05:00, 10.00s/it] 28%|██▊       | 11/40 [01:48<04:47,  9.91s/it] 30%|███       | 12/40 [01:57<04:31,  9.69s/it] 32%|███▎      | 13/40 [02:06<04:21,  9.68s/it] 35%|███▌      | 14/40 [02:16<04:12,  9.70s/it] 38%|███▊      | 15/40 [02:26<04:04,  9.79s/it] 40%|████      | 16/40 [02:36<03:57,  9.88s/it] 42%|████▎     | 17/40 [02:47<03:53, 10.15s/it] 45%|████▌     | 18/40 [02:57<03:41, 10.07s/it] 48%|████▊     | 19/40 [03:07<03:29,  9.96s/it] 50%|█████     | 20/40 [03:16<03:18,  9.90s/it] 52%|█████▎    | 21/40 [03:26<03:08,  9.94s/it] 55%|█████▌    | 22/40 [03:36<02:54,  9.69s/it] 57%|█████▊    | 23/40 [03:45<02:44,  9.66s/it] 60%|██████    | 24/40 [03:55<02:37,  9.82s/it] 62%|██████▎   | 25/40 [04:07<02:33, 10.23s/it] 65%|██████▌   | 26/40 [04:16<02:19,  9.97s/it] 68%|██████▊   | 27/40 [04:26<02:08,  9.91s/it] 70%|███████   | 28/40 [04:35<01:57,  9.82s/it] 72%|███████▎  | 29/40 [04:45<01:47,  9.77s/it] 75%|███████▌  | 30/40 [04:55<01:37,  9.77s/it] 78%|███████▊  | 31/40 [05:05<01:28,  9.82s/it] 80%|████████  | 32/40 [05:14<01:17,  9.73s/it] 82%|████████▎ | 33/40 [05:25<01:10, 10.07s/it] 85%|████████▌ | 34/40 [05:35<01:00, 10.07s/it] 88%|████████▊ | 35/40 [05:45<00:50, 10.06s/it] 90%|█████████ | 36/40 [05:55<00:39,  9.93s/it] 92%|█████████▎| 37/40 [06:04<00:29,  9.79s/it] 95%|█████████▌| 38/40 [06:14<00:19,  9.78s/it] 98%|█████████▊| 39/40 [06:24<00:09,  9.85s/it]100%|██████████| 40/40 [06:34<00:00,  9.76s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:34<00:00,  9.76s/it]100%|██████████| 40/40 [06:34<00:00,  9.85s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 394.1094, 'train_samples_per_second': 6.61, 'train_steps_per_second': 0.101, 'train_loss': 0.7546205520629883, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:59,  6.14s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:18<03:48,  6.18s/it] 10%|█         | 4/40 [00:24<03:41,  6.16s/it] 12%|█▎        | 5/40 [00:30<03:36,  6.18s/it] 15%|█▌        | 6/40 [00:37<03:29,  6.17s/it] 18%|█▊        | 7/40 [00:43<03:21,  6.10s/it] 20%|██        | 8/40 [00:49<03:16,  6.14s/it] 22%|██▎       | 9/40 [00:56<03:18,  6.40s/it] 25%|██▌       | 10/40 [01:02<03:09,  6.32s/it] 28%|██▊       | 11/40 [01:08<03:01,  6.27s/it] 30%|███       | 12/40 [01:14<02:50,  6.10s/it] 32%|███▎      | 13/40 [01:20<02:46,  6.16s/it] 35%|███▌      | 14/40 [01:26<02:40,  6.19s/it] 38%|███▊      | 15/40 [01:33<02:36,  6.27s/it] 40%|████      | 16/40 [01:39<02:31,  6.31s/it] 42%|████▎     | 17/40 [01:46<02:28,  6.48s/it] 45%|████▌     | 18/40 [01:52<02:20,  6.37s/it] 48%|████▊     | 19/40 [01:58<02:12,  6.31s/it] 50%|█████     | 20/40 [02:04<02:04,  6.23s/it] 52%|█████▎    | 21/40 [02:10<01:57,  6.17s/it] 55%|█████▌    | 22/40 [02:16<01:50,  6.12s/it] 57%|█████▊    | 23/40 [02:22<01:42,  6.05s/it] 60%|██████    | 24/40 [02:29<01:38,  6.17s/it] 62%|██████▎   | 25/40 [02:36<01:36,  6.45s/it] 65%|██████▌   | 26/40 [02:42<01:28,  6.33s/it] 68%|██████▊   | 27/40 [02:48<01:22,  6.32s/it] 70%|███████   | 28/40 [02:54<01:14,  6.21s/it] 72%|███████▎  | 29/40 [03:00<01:08,  6.22s/it] 75%|███████▌  | 30/40 [03:06<01:01,  6.15s/it] 78%|███████▊  | 31/40 [03:12<00:54,  6.11s/it] 80%|████████  | 32/40 [03:18<00:48,  6.06s/it] 82%|████████▎ | 33/40 [03:25<00:44,  6.31s/it] 85%|████████▌ | 34/40 [03:32<00:37,  6.32s/it] 88%|████████▊ | 35/40 [03:38<00:31,  6.36s/it] 90%|█████████ | 36/40 [03:44<00:24,  6.23s/it] 92%|█████████▎| 37/40 [03:50<00:18,  6.12s/it] 95%|█████████▌| 38/40 [03:56<00:12,  6.12s/it] 98%|█████████▊| 39/40 [04:02<00:06,  6.17s/it]100%|██████████| 40/40 [04:08<00:00,  6.04s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:08<00:00,  6.04s/it]100%|██████████| 40/40 [04:08<00:00,  6.21s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 248.5193, 'train_samples_per_second': 10.482, 'train_steps_per_second': 0.161, 'train_loss': 0.6985921859741211, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:59,  6.14s/it]  5%|▌         | 2/40 [00:12<04:00,  6.34s/it]  8%|▊         | 3/40 [00:18<03:47,  6.16s/it] 10%|█         | 4/40 [00:24<03:41,  6.15s/it] 12%|█▎        | 5/40 [00:30<03:36,  6.18s/it] 15%|█▌        | 6/40 [00:37<03:31,  6.21s/it] 18%|█▊        | 7/40 [00:43<03:23,  6.17s/it] 20%|██        | 8/40 [00:49<03:19,  6.22s/it] 22%|██▎       | 9/40 [00:56<03:19,  6.45s/it] 25%|██▌       | 10/40 [01:02<03:08,  6.28s/it] 28%|██▊       | 11/40 [01:08<02:59,  6.20s/it] 30%|███       | 12/40 [01:14<02:49,  6.04s/it] 32%|███▎      | 13/40 [01:20<02:44,  6.11s/it] 35%|███▌      | 14/40 [01:26<02:39,  6.15s/it] 38%|███▊      | 15/40 [01:33<02:36,  6.24s/it] 40%|████      | 16/40 [01:39<02:31,  6.29s/it] 42%|████▎     | 17/40 [01:46<02:28,  6.46s/it] 45%|████▌     | 18/40 [01:52<02:19,  6.36s/it] 48%|████▊     | 19/40 [01:58<02:12,  6.30s/it] 50%|█████     | 20/40 [02:04<02:04,  6.23s/it] 52%|█████▎    | 21/40 [02:10<01:57,  6.17s/it] 55%|█████▌    | 22/40 [02:16<01:50,  6.11s/it] 57%|█████▊    | 23/40 [02:22<01:42,  6.05s/it] 60%|██████    | 24/40 [02:29<01:38,  6.17s/it] 62%|██████▎   | 25/40 [02:36<01:36,  6.45s/it] 65%|██████▌   | 26/40 [02:42<01:28,  6.33s/it] 68%|██████▊   | 27/40 [02:48<01:22,  6.32s/it] 70%|███████   | 28/40 [02:54<01:14,  6.21s/it] 72%|███████▎  | 29/40 [03:00<01:08,  6.22s/it] 75%|███████▌  | 30/40 [03:06<01:01,  6.15s/it] 78%|███████▊  | 31/40 [03:12<00:54,  6.11s/it] 80%|████████  | 32/40 [03:18<00:48,  6.06s/it] 82%|████████▎ | 33/40 [03:25<00:44,  6.33s/it] 85%|████████▌ | 34/40 [03:32<00:38,  6.34s/it] 88%|████████▊ | 35/40 [03:38<00:32,  6.41s/it] 90%|█████████ | 36/40 [03:44<00:25,  6.33s/it] 92%|█████████▎| 37/40 [03:50<00:18,  6.27s/it] 95%|█████████▌| 38/40 [03:57<00:12,  6.28s/it] 98%|█████████▊| 39/40 [04:03<00:06,  6.34s/it]100%|██████████| 40/40 [04:09<00:00,  6.24s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:09<00:00,  6.24s/it]100%|██████████| 40/40 [04:09<00:00,  6.24s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 249.6631, 'train_samples_per_second': 10.434, 'train_steps_per_second': 0.16, 'train_loss': 0.7158300399780273, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:43,  8.82s/it]  5%|▌         | 2/40 [00:18<05:48,  9.17s/it]  8%|▊         | 3/40 [00:26<05:29,  8.92s/it] 10%|█         | 4/40 [00:35<05:21,  8.93s/it] 12%|█▎        | 5/40 [00:44<05:13,  8.96s/it] 15%|█▌        | 6/40 [00:53<05:05,  9.00s/it] 18%|█▊        | 7/40 [01:02<04:55,  8.95s/it] 20%|██        | 8/40 [01:11<04:48,  9.01s/it] 22%|██▎       | 9/40 [01:22<04:50,  9.38s/it] 25%|██▌       | 10/40 [01:30<04:35,  9.18s/it] 28%|██▊       | 11/40 [01:39<04:23,  9.10s/it] 30%|███       | 12/40 [01:48<04:08,  8.87s/it] 32%|███▎      | 13/40 [01:57<04:02,  8.96s/it] 35%|███▌      | 14/40 [02:06<03:54,  9.03s/it] 38%|███▊      | 15/40 [02:16<03:49,  9.20s/it] 40%|████      | 16/40 [02:25<03:42,  9.28s/it] 42%|████▎     | 17/40 [02:35<03:37,  9.46s/it] 45%|████▌     | 18/40 [02:44<03:24,  9.28s/it] 48%|████▊     | 19/40 [02:53<03:12,  9.18s/it] 50%|█████     | 20/40 [03:01<03:00,  9.03s/it] 52%|█████▎    | 21/40 [03:10<02:50,  8.95s/it] 55%|█████▌    | 22/40 [03:19<02:39,  8.86s/it] 57%|█████▊    | 23/40 [03:27<02:29,  8.77s/it] 60%|██████    | 24/40 [03:37<02:23,  8.95s/it] 62%|██████▎   | 25/40 [03:47<02:20,  9.35s/it] 65%|██████▌   | 26/40 [03:56<02:08,  9.14s/it] 68%|██████▊   | 27/40 [04:05<01:58,  9.15s/it] 70%|███████   | 28/40 [04:13<01:47,  8.97s/it] 72%|███████▎  | 29/40 [04:22<01:38,  9.00s/it] 75%|███████▌  | 30/40 [04:31<01:28,  8.90s/it] 78%|███████▊  | 31/40 [04:40<01:19,  8.85s/it] 80%|████████  | 32/40 [04:48<01:10,  8.79s/it] 82%|████████▎ | 33/40 [04:58<01:04,  9.15s/it] 85%|████████▌ | 34/40 [05:08<00:55,  9.17s/it] 88%|████████▊ | 35/40 [05:17<00:46,  9.25s/it] 90%|█████████ | 36/40 [05:26<00:36,  9.03s/it] 92%|█████████▎| 37/40 [05:34<00:26,  8.84s/it] 95%|█████████▌| 38/40 [05:43<00:17,  8.87s/it] 98%|█████████▊| 39/40 [05:52<00:08,  8.95s/it]100%|██████████| 40/40 [06:00<00:00,  8.73s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:00<00:00,  8.73s/it]100%|██████████| 40/40 [06:00<00:00,  9.02s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/08e4846e537177426273712802403f7ba8261b6c/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 360.8226, 'train_samples_per_second': 7.22, 'train_steps_per_second': 0.111, 'train_loss': 0.6978919982910157, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:09,  9.48s/it]  5%|▌         | 2/40 [00:19<06:14,  9.87s/it]  8%|▊         | 3/40 [00:28<05:54,  9.58s/it] 10%|█         | 4/40 [00:38<05:44,  9.56s/it] 12%|█▎        | 5/40 [00:48<05:36,  9.62s/it] 15%|█▌        | 6/40 [00:57<05:29,  9.68s/it] 18%|█▊        | 7/40 [01:07<05:17,  9.63s/it] 20%|██        | 8/40 [01:17<05:10,  9.71s/it] 22%|██▎       | 9/40 [01:28<05:13, 10.11s/it] 25%|██▌       | 10/40 [01:37<04:56,  9.89s/it] 28%|██▊       | 11/40 [01:47<04:44,  9.80s/it] 30%|███       | 12/40 [01:56<04:27,  9.56s/it] 32%|███▎      | 13/40 [02:06<04:21,  9.68s/it] 35%|███▌      | 14/40 [02:16<04:13,  9.75s/it] 38%|███▊      | 15/40 [02:26<04:07,  9.92s/it] 40%|████      | 16/40 [02:36<04:00, 10.00s/it] 42%|████▎     | 17/40 [02:47<03:56, 10.27s/it] 45%|████▌     | 18/40 [02:57<03:42, 10.12s/it] 48%|████▊     | 19/40 [03:07<03:30, 10.03s/it] 50%|█████     | 20/40 [03:16<03:18,  9.90s/it] 52%|█████▎    | 21/40 [03:26<03:06,  9.82s/it] 55%|█████▌    | 22/40 [03:35<02:55,  9.74s/it] 57%|█████▊    | 23/40 [03:45<02:43,  9.63s/it] 60%|██████    | 24/40 [03:55<02:37,  9.82s/it] 62%|██████▎   | 25/40 [04:06<02:34, 10.27s/it] 65%|██████▌   | 26/40 [04:16<02:20, 10.07s/it] 68%|██████▊   | 27/40 [04:26<02:10, 10.06s/it] 70%|███████   | 28/40 [04:35<01:58,  9.87s/it] 72%|███████▎  | 29/40 [04:45<01:48,  9.89s/it] 75%|███████▌  | 30/40 [04:55<01:37,  9.78s/it] 78%|███████▊  | 31/40 [05:05<01:27,  9.73s/it] 80%|████████  | 32/40 [05:14<01:17,  9.67s/it] 82%|████████▎ | 33/40 [05:25<01:10, 10.05s/it] 85%|████████▌ | 34/40 [05:35<01:00, 10.07s/it] 88%|████████▊ | 35/40 [05:45<00:50, 10.15s/it] 90%|█████████ | 36/40 [05:55<00:39,  9.93s/it] 92%|█████████▎| 37/40 [06:04<00:29,  9.74s/it] 95%|█████████▌| 38/40 [06:14<00:19,  9.75s/it] 98%|█████████▊| 39/40 [06:24<00:09,  9.83s/it]100%|██████████| 40/40 [06:33<00:00,  9.61s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:33<00:00,  9.61s/it]100%|██████████| 40/40 [06:33<00:00,  9.84s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 393.5975, 'train_samples_per_second': 6.618, 'train_steps_per_second': 0.102, 'train_loss': 0.7546815872192383, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:38,  5.60s/it]  5%|▌         | 2/40 [00:11<03:37,  5.71s/it]