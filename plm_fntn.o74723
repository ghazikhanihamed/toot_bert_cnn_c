Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
/home/h_ghazik/python_path_gpu/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:21,  6.71s/it]  5%|▌         | 2/40 [00:12<03:54,  6.17s/it]  8%|▊         | 3/40 [00:18<03:45,  6.11s/it] 10%|█         | 4/40 [00:24<03:40,  6.11s/it] 12%|█▎        | 5/40 [00:30<03:34,  6.13s/it] 15%|█▌        | 6/40 [00:36<03:29,  6.15s/it] 18%|█▊        | 7/40 [00:42<03:19,  6.06s/it] 20%|██        | 8/40 [00:48<03:13,  6.05s/it] 22%|██▎       | 9/40 [00:59<03:49,  7.40s/it] 25%|██▌       | 10/40 [01:05<03:32,  7.08s/it] 28%|██▊       | 11/40 [01:11<03:15,  6.74s/it] 30%|███       | 12/40 [01:17<03:00,  6.45s/it] 32%|███▎      | 13/40 [01:23<02:52,  6.39s/it] 35%|███▌      | 14/40 [01:29<02:42,  6.24s/it] 38%|███▊      | 15/40 [01:35<02:32,  6.09s/it] 40%|████      | 16/40 [01:41<02:26,  6.12s/it] 42%|████▎     | 17/40 [01:52<02:52,  7.48s/it] 45%|████▌     | 18/40 [01:58<02:35,  7.06s/it] 48%|████▊     | 19/40 [02:03<02:20,  6.67s/it] 50%|█████     | 20/40 [02:09<02:09,  6.48s/it] 52%|█████▎    | 21/40 [02:15<01:59,  6.28s/it] 55%|█████▌    | 22/40 [02:21<01:51,  6.20s/it] 57%|█████▊    | 23/40 [02:27<01:42,  6.04s/it] 60%|██████    | 24/40 [02:33<01:35,  6.00s/it] 62%|██████▎   | 25/40 [02:43<01:49,  7.33s/it] 65%|██████▌   | 26/40 [02:49<01:36,  6.87s/it] 68%|██████▊   | 27/40 [02:55<01:26,  6.64s/it] 70%|███████   | 28/40 [03:01<01:16,  6.39s/it] 72%|███████▎  | 29/40 [03:07<01:10,  6.39s/it] 75%|███████▌  | 30/40 [03:14<01:05,  6.50s/it] 78%|███████▊  | 31/40 [03:20<00:57,  6.41s/it] 80%|████████  | 32/40 [03:27<00:51,  6.48s/it] 82%|████████▎ | 33/40 [03:38<00:54,  7.83s/it] 85%|████████▌ | 34/40 [03:45<00:44,  7.47s/it] 88%|████████▊ | 35/40 [03:51<00:35,  7.15s/it] 90%|█████████ | 36/40 [03:57<00:27,  6.89s/it] 92%|█████████▎| 37/40 [04:04<00:20,  6.70s/it] 95%|█████████▌| 38/40 [04:09<00:12,  6.47s/it] 98%|█████████▊| 39/40 [04:15<00:06,  6.32s/it]100%|██████████| 40/40 [04:21<00:00,  6.17s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:21<00:00,  6.17s/it]100%|██████████| 40/40 [04:21<00:00,  6.54s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 261.7896, 'train_samples_per_second': 10.715, 'train_steps_per_second': 0.153, 'train_loss': 0.7460233688354492, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:01,  6.19s/it]  5%|▌         | 2/40 [00:12<03:54,  6.17s/it]  8%|▊         | 3/40 [00:18<03:45,  6.09s/it] 10%|█         | 4/40 [00:24<03:39,  6.09s/it] 12%|█▎        | 5/40 [00:30<03:33,  6.10s/it] 15%|█▌        | 6/40 [00:36<03:25,  6.05s/it] 18%|█▊        | 7/40 [00:42<03:17,  5.99s/it] 20%|██        | 8/40 [00:48<03:11,  5.98s/it] 22%|██▎       | 9/40 [00:58<03:46,  7.30s/it] 25%|██▌       | 10/40 [01:04<03:29,  6.98s/it] 28%|██▊       | 11/40 [01:10<03:12,  6.62s/it] 30%|███       | 12/40 [01:16<02:57,  6.33s/it] 32%|███▎      | 13/40 [01:22<02:48,  6.23s/it] 35%|███▌      | 14/40 [01:28<02:40,  6.19s/it] 38%|███▊      | 15/40 [01:34<02:31,  6.05s/it] 40%|████      | 16/40 [01:40<02:26,  6.10s/it] 42%|████▎     | 17/40 [01:51<02:53,  7.54s/it] 45%|████▌     | 18/40 [01:57<02:36,  7.10s/it] 48%|████▊     | 19/40 [02:03<02:20,  6.69s/it] 50%|█████     | 20/40 [02:09<02:09,  6.49s/it] 52%|█████▎    | 21/40 [02:14<01:59,  6.27s/it] 55%|█████▌    | 22/40 [02:20<01:51,  6.18s/it] 57%|█████▊    | 23/40 [02:26<01:42,  6.04s/it] 60%|██████    | 24/40 [02:32<01:36,  6.03s/it] 62%|██████▎   | 25/40 [02:43<01:50,  7.38s/it] 65%|██████▌   | 26/40 [02:48<01:36,  6.92s/it] 68%|██████▊   | 27/40 [02:55<01:27,  6.70s/it] 70%|███████   | 28/40 [03:00<01:17,  6.44s/it] 72%|███████▎  | 29/40 [03:06<01:09,  6.32s/it] 75%|███████▌  | 30/40 [03:13<01:02,  6.26s/it] 78%|███████▊  | 31/40 [03:18<00:54,  6.08s/it] 80%|████████  | 32/40 [03:24<00:49,  6.13s/it] 82%|████████▎ | 33/40 [03:35<00:51,  7.33s/it] 85%|████████▌ | 34/40 [03:41<00:41,  6.97s/it] 88%|████████▊ | 35/40 [03:47<00:33,  6.63s/it] 90%|█████████ | 36/40 [03:52<00:25,  6.38s/it] 92%|█████████▎| 37/40 [03:59<00:18,  6.31s/it] 95%|█████████▌| 38/40 [04:05<00:12,  6.38s/it] 98%|█████████▊| 39/40 [04:11<00:06,  6.22s/it]100%|██████████| 40/40 [04:17<00:00,  6.08s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:17<00:00,  6.08s/it]100%|██████████| 40/40 [04:17<00:00,  6.43s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 257.1718, 'train_samples_per_second': 10.907, 'train_steps_per_second': 0.156, 'train_loss': 0.7508064270019531, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:24,  8.32s/it]  5%|▌         | 2/40 [00:16<05:11,  8.20s/it]  8%|▊         | 3/40 [00:24<05:07,  8.32s/it] 10%|█         | 4/40 [00:33<05:05,  8.49s/it] 12%|█▎        | 5/40 [00:42<04:58,  8.54s/it] 15%|█▌        | 6/40 [00:50<04:51,  8.56s/it] 18%|█▊        | 7/40 [00:59<04:40,  8.49s/it] 20%|██        | 8/40 [01:07<04:32,  8.51s/it] 22%|██▎       | 9/40 [01:22<05:20, 10.33s/it] 25%|██▌       | 10/40 [01:31<04:58,  9.95s/it] 28%|██▊       | 11/40 [01:39<04:34,  9.45s/it] 30%|███       | 12/40 [01:47<04:12,  9.02s/it] 32%|███▎      | 13/40 [01:56<03:59,  8.87s/it] 35%|███▌      | 14/40 [02:04<03:46,  8.70s/it] 38%|███▊      | 15/40 [02:12<03:32,  8.50s/it] 40%|████      | 16/40 [02:21<03:25,  8.55s/it] 42%|████▎     | 17/40 [02:36<04:01, 10.50s/it] 45%|████▌     | 18/40 [02:44<03:39,  9.97s/it] 48%|████▊     | 19/40 [02:53<03:19,  9.48s/it] 50%|█████     | 20/40 [03:01<03:04,  9.22s/it] 52%|█████▎    | 21/40 [03:10<02:50,  8.98s/it] 55%|█████▌    | 22/40 [03:18<02:40,  8.92s/it] 57%|█████▊    | 23/40 [03:27<02:27,  8.69s/it] 60%|██████    | 24/40 [03:35<02:18,  8.63s/it] 62%|██████▎   | 25/40 [03:50<02:39, 10.64s/it] 65%|██████▌   | 26/40 [03:59<02:18,  9.92s/it] 68%|██████▊   | 27/40 [04:07<02:04,  9.55s/it] 70%|███████   | 28/40 [04:16<01:49,  9.15s/it] 72%|███████▎  | 29/40 [04:24<01:38,  8.99s/it] 75%|███████▌  | 30/40 [04:33<01:28,  8.88s/it] 78%|███████▊  | 31/40 [04:41<01:17,  8.56s/it] 80%|████████  | 32/40 [04:49<01:09,  8.64s/it] 82%|████████▎ | 33/40 [05:05<01:14, 10.62s/it] 85%|████████▌ | 34/40 [05:14<01:01, 10.21s/it] 88%|████████▊ | 35/40 [05:23<00:48,  9.77s/it] 90%|█████████ | 36/40 [05:32<00:37,  9.49s/it] 92%|█████████▎| 37/40 [05:41<00:28,  9.35s/it] 95%|█████████▌| 38/40 [05:49<00:18,  9.13s/it] 98%|█████████▊| 39/40 [05:58<00:08,  8.93s/it]100%|██████████| 40/40 [06:06<00:00,  8.73s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:06<00:00,  8.73s/it]100%|██████████| 40/40 [06:06<00:00,  9.16s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 366.4563, 'train_samples_per_second': 7.654, 'train_steps_per_second': 0.109, 'train_loss': 0.7579294204711914, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_4.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:05,  9.38s/it]  5%|▌         | 2/40 [00:18<05:51,  9.24s/it]  8%|▊         | 3/40 [00:28<05:47,  9.40s/it] 10%|█         | 4/40 [00:37<05:43,  9.54s/it] 12%|█▎        | 5/40 [00:47<05:35,  9.60s/it] 15%|█▌        | 6/40 [00:57<05:28,  9.65s/it] 18%|█▊        | 7/40 [01:06<05:14,  9.53s/it] 20%|██        | 8/40 [01:16<05:06,  9.58s/it] 22%|██▎       | 9/40 [01:32<06:03, 11.73s/it] 25%|██▌       | 10/40 [01:42<05:37, 11.24s/it] 28%|██▊       | 11/40 [01:52<05:10, 10.69s/it] 30%|███       | 12/40 [02:01<04:45, 10.20s/it] 32%|███▎      | 13/40 [02:10<04:30, 10.00s/it] 35%|███▌      | 14/40 [02:20<04:13,  9.74s/it] 38%|███▊      | 15/40 [02:29<03:57,  9.50s/it] 40%|████      | 16/40 [02:38<03:48,  9.53s/it] 42%|████▎     | 17/40 [02:56<04:34, 11.95s/it] 45%|████▌     | 18/40 [03:06<04:08, 11.30s/it] 48%|████▊     | 19/40 [03:15<03:45, 10.72s/it] 50%|█████     | 20/40 [03:25<03:30, 10.50s/it] 52%|█████▎    | 21/40 [03:34<03:14, 10.22s/it] 55%|█████▌    | 22/40 [03:44<03:02, 10.13s/it] 57%|█████▊    | 23/40 [03:54<02:47,  9.87s/it] 60%|██████    | 24/40 [04:03<02:36,  9.76s/it] 62%|██████▎   | 25/40 [04:20<02:58, 11.92s/it] 65%|██████▌   | 26/40 [04:30<02:36, 11.21s/it] 68%|██████▊   | 27/40 [04:39<02:19, 10.75s/it] 70%|███████   | 28/40 [04:49<02:03, 10.30s/it] 72%|███████▎  | 29/40 [04:58<01:50, 10.05s/it] 75%|███████▌  | 30/40 [05:08<01:38,  9.89s/it] 78%|███████▊  | 31/40 [05:17<01:26,  9.62s/it] 80%|████████  | 32/40 [05:26<01:17,  9.69s/it] 82%|████████▎ | 33/40 [05:42<01:21, 11.61s/it] 85%|████████▌ | 34/40 [05:52<01:06, 11.05s/it] 88%|████████▊ | 35/40 [06:02<00:52, 10.53s/it] 90%|█████████ | 36/40 [06:11<00:40, 10.16s/it] 92%|█████████▎| 37/40 [06:20<00:29,  9.99s/it] 95%|█████████▌| 38/40 [06:30<00:19,  9.81s/it] 98%|█████████▊| 39/40 [06:39<00:09,  9.66s/it]100%|██████████| 40/40 [06:48<00:00,  9.51s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:48<00:00,  9.51s/it]100%|██████████| 40/40 [06:48<00:00, 10.22s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 408.7909, 'train_samples_per_second': 6.862, 'train_steps_per_second': 0.098, 'train_loss': 0.7833407402038575, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:46,  5.80s/it]  5%|▌         | 2/40 [00:11<03:42,  5.85s/it]  8%|▊         | 3/40 [00:17<03:40,  5.97s/it] 10%|█         | 4/40 [00:23<03:36,  6.02s/it] 12%|█▎        | 5/40 [00:30<03:38,  6.24s/it] 15%|█▌        | 6/40 [00:36<03:32,  6.24s/it] 18%|█▊        | 7/40 [00:42<03:24,  6.21s/it] 20%|██        | 8/40 [00:49<03:23,  6.35s/it] 22%|██▎       | 9/40 [01:00<03:57,  7.66s/it] 25%|██▌       | 10/40 [01:06<03:37,  7.25s/it] 28%|██▊       | 11/40 [01:12<03:17,  6.82s/it] 30%|███       | 12/40 [01:18<03:03,  6.55s/it] 32%|███▎      | 13/40 [01:24<02:52,  6.38s/it] 35%|███▌      | 14/40 [01:30<02:44,  6.33s/it] 38%|███▊      | 15/40 [01:36<02:34,  6.19s/it] 40%|████      | 16/40 [01:42<02:29,  6.23s/it] 42%|████▎     | 17/40 [01:53<02:54,  7.60s/it] 45%|████▌     | 18/40 [01:59<02:35,  7.08s/it] 48%|████▊     | 19/40 [02:05<02:21,  6.74s/it] 50%|█████     | 20/40 [02:11<02:10,  6.52s/it] 52%|█████▎    | 21/40 [02:17<02:01,  6.42s/it] 55%|█████▌    | 22/40 [02:24<01:57,  6.51s/it] 57%|█████▊    | 23/40 [02:30<01:50,  6.48s/it] 60%|██████    | 24/40 [02:36<01:42,  6.42s/it] 62%|██████▎   | 25/40 [02:48<02:00,  8.02s/it] 65%|██████▌   | 26/40 [02:55<01:46,  7.59s/it] 68%|██████▊   | 27/40 [03:01<01:33,  7.23s/it] 70%|███████   | 28/40 [03:08<01:24,  7.08s/it] 72%|███████▎  | 29/40 [03:14<01:14,  6.80s/it] 75%|███████▌  | 30/40 [03:20<01:06,  6.65s/it] 78%|███████▊  | 31/40 [03:26<00:57,  6.43s/it] 80%|████████  | 32/40 [03:32<00:50,  6.34s/it] 82%|████████▎ | 33/40 [03:43<00:52,  7.57s/it] 85%|████████▌ | 34/40 [03:49<00:43,  7.22s/it] 88%|████████▊ | 35/40 [03:55<00:34,  6.86s/it] 90%|█████████ | 36/40 [04:01<00:26,  6.63s/it] 92%|█████████▎| 37/40 [04:07<00:19,  6.46s/it] 95%|█████████▌| 38/40 [04:13<00:12,  6.23s/it] 98%|█████████▊| 39/40 [04:19<00:06,  6.19s/it]100%|██████████| 40/40 [04:25<00:00,  6.12s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:25<00:00,  6.12s/it]100%|██████████| 40/40 [04:25<00:00,  6.64s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 265.4943, 'train_samples_per_second': 10.565, 'train_steps_per_second': 0.151, 'train_loss': 0.745904541015625, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:01,  6.19s/it]  5%|▌         | 2/40 [00:13<04:09,  6.57s/it]  8%|▊         | 3/40 [00:19<04:05,  6.62s/it] 10%|█         | 4/40 [00:30<04:54,  8.18s/it] 12%|█▎        | 5/40 [00:40<05:08,  8.82s/it] 15%|█▌        | 6/40 [00:46<04:31,  8.00s/it] 18%|█▊        | 7/40 [00:57<04:57,  9.02s/it] 20%|██        | 8/40 [01:06<04:42,  8.84s/it] 22%|██▎       | 9/40 [01:19<05:19, 10.29s/it] 25%|██▌       | 10/40 [01:26<04:32,  9.07s/it] 28%|██▊       | 11/40 [01:32<03:56,  8.16s/it] 30%|███       | 12/40 [01:38<03:30,  7.53s/it] 32%|███▎      | 13/40 [01:44<03:11,  7.11s/it] 35%|███▌      | 14/40 [01:50<02:57,  6.84s/it] 38%|███▊      | 15/40 [01:56<02:44,  6.58s/it] 40%|████      | 16/40 [02:02<02:35,  6.49s/it] 42%|████▎     | 17/40 [02:13<03:01,  7.90s/it] 45%|████▌     | 18/40 [02:19<02:40,  7.29s/it] 48%|████▊     | 19/40 [02:25<02:24,  6.89s/it] 50%|█████     | 20/40 [02:31<02:12,  6.64s/it] 52%|█████▎    | 21/40 [02:38<02:03,  6.50s/it] 55%|█████▌    | 22/40 [02:44<01:56,  6.47s/it] 57%|█████▊    | 23/40 [02:50<01:47,  6.34s/it] 60%|██████    | 24/40 [02:56<01:38,  6.18s/it] 62%|██████▎   | 25/40 [03:07<01:54,  7.63s/it] 65%|██████▌   | 26/40 [03:13<01:40,  7.18s/it] 68%|██████▊   | 27/40 [03:19<01:28,  6.82s/it] 70%|███████   | 28/40 [03:25<01:19,  6.61s/it] 72%|███████▎  | 29/40 [03:31<01:10,  6.41s/it] 75%|███████▌  | 30/40 [03:37<01:03,  6.32s/it] 78%|███████▊  | 31/40 [03:43<00:55,  6.14s/it] 80%|████████  | 32/40 [03:49<00:48,  6.12s/it] 82%|████████▎ | 33/40 [03:59<00:51,  7.38s/it] 85%|████████▌ | 34/40 [04:06<00:42,  7.05s/it] 88%|████████▊ | 35/40 [04:12<00:33,  6.74s/it] 90%|█████████ | 36/40 [04:17<00:26,  6.50s/it] 92%|█████████▎| 37/40 [04:23<00:19,  6.34s/it] 95%|█████████▌| 38/40 [04:29<00:12,  6.16s/it] 98%|█████████▊| 39/40 [04:35<00:06,  6.15s/it]100%|██████████| 40/40 [04:41<00:00,  6.06s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:41<00:00,  6.06s/it]100%|██████████| 40/40 [04:41<00:00,  7.04s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 281.6512, 'train_samples_per_second': 9.959, 'train_steps_per_second': 0.142, 'train_loss': 0.750439453125, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:22,  8.27s/it]  5%|▌         | 2/40 [00:16<05:16,  8.32s/it]  8%|▊         | 3/40 [00:25<05:13,  8.47s/it] 10%|█         | 4/40 [00:33<05:04,  8.46s/it] 12%|█▎        | 5/40 [00:42<05:06,  8.75s/it] 15%|█▌        | 6/40 [00:51<04:54,  8.68s/it] 18%|█▊        | 7/40 [00:59<04:43,  8.58s/it] 20%|██        | 8/40 [01:08<04:35,  8.61s/it] 22%|██▎       | 9/40 [01:22<05:22, 10.42s/it] 25%|██▌       | 10/40 [01:31<04:56,  9.88s/it] 28%|██▊       | 11/40 [01:39<04:31,  9.37s/it] 30%|███       | 12/40 [01:48<04:13,  9.06s/it] 32%|███▎      | 13/40 [01:56<04:00,  8.91s/it] 35%|███▌      | 14/40 [02:05<03:51,  8.91s/it] 38%|███▊      | 15/40 [02:13<03:37,  8.71s/it] 40%|████      | 16/40 [02:23<03:32,  8.83s/it] 42%|████▎     | 17/40 [02:38<04:09, 10.84s/it] 45%|████▌     | 18/40 [02:46<03:42, 10.09s/it] 48%|████▊     | 19/40 [02:55<03:21,  9.60s/it] 50%|█████     | 20/40 [03:03<03:05,  9.26s/it] 52%|█████▎    | 21/40 [03:12<02:52,  9.06s/it] 55%|█████▌    | 22/40 [03:21<02:41,  8.99s/it] 57%|█████▊    | 23/40 [03:29<02:30,  8.88s/it] 60%|██████    | 24/40 [03:37<02:18,  8.64s/it] 62%|██████▎   | 25/40 [03:53<02:40, 10.72s/it] 65%|██████▌   | 26/40 [04:02<02:22, 10.18s/it] 68%|██████▊   | 27/40 [04:11<02:06,  9.74s/it] 70%|███████   | 28/40 [04:20<01:54,  9.58s/it] 72%|███████▎  | 29/40 [04:29<01:43,  9.37s/it] 75%|███████▌  | 30/40 [04:38<01:33,  9.32s/it] 78%|███████▊  | 31/40 [04:46<01:21,  9.06s/it] 80%|████████  | 32/40 [04:55<01:11,  8.97s/it] 82%|████████▎ | 33/40 [05:10<01:14, 10.64s/it] 85%|████████▌ | 34/40 [05:19<01:01, 10.20s/it] 88%|████████▊ | 35/40 [05:27<00:48,  9.67s/it] 90%|█████████ | 36/40 [05:36<00:37,  9.28s/it] 92%|█████████▎| 37/40 [05:44<00:27,  9.06s/it] 95%|█████████▌| 38/40 [05:52<00:17,  8.74s/it] 98%|█████████▊| 39/40 [06:01<00:08,  8.72s/it]100%|██████████| 40/40 [06:09<00:00,  8.57s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:09<00:00,  8.57s/it]100%|██████████| 40/40 [06:09<00:00,  9.24s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 369.6468, 'train_samples_per_second': 7.588, 'train_steps_per_second': 0.108, 'train_loss': 0.757661247253418, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_10.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:54,  9.09s/it]  5%|▌         | 2/40 [00:18<05:50,  9.22s/it]  8%|▊         | 3/40 [00:28<05:48,  9.42s/it] 10%|█         | 4/40 [00:37<05:38,  9.40s/it] 12%|█▎        | 5/40 [00:47<05:39,  9.69s/it] 15%|█▌        | 6/40 [00:57<05:25,  9.59s/it] 18%|█▊        | 7/40 [01:06<05:12,  9.48s/it] 20%|██        | 8/40 [01:15<05:03,  9.48s/it] 22%|██▎       | 9/40 [01:32<06:00, 11.62s/it] 25%|██▌       | 10/40 [01:41<05:31, 11.04s/it] 28%|██▊       | 11/40 [01:51<05:05, 10.54s/it] 30%|███       | 12/40 [02:00<04:45, 10.20s/it] 32%|███▎      | 13/40 [02:10<04:30, 10.02s/it] 35%|███▌      | 14/40 [02:20<04:20, 10.00s/it] 38%|███▊      | 15/40 [02:29<04:05,  9.81s/it] 40%|████      | 16/40 [02:39<03:56,  9.85s/it] 42%|████▎     | 17/40 [02:56<04:38, 12.12s/it] 45%|████▌     | 18/40 [03:06<04:09, 11.32s/it] 48%|████▊     | 19/40 [03:15<03:46, 10.77s/it] 50%|█████     | 20/40 [03:25<03:27, 10.36s/it] 52%|█████▎    | 21/40 [03:34<03:12, 10.13s/it] 55%|█████▌    | 22/40 [03:44<03:01, 10.10s/it] 57%|█████▊    | 23/40 [03:54<02:49,  9.96s/it] 60%|██████    | 24/40 [04:03<02:35,  9.75s/it] 62%|██████▎   | 25/40 [04:21<03:00, 12.06s/it] 65%|██████▌   | 26/40 [04:30<02:38, 11.31s/it] 68%|██████▊   | 27/40 [04:40<02:20, 10.80s/it] 70%|███████   | 28/40 [04:50<02:06, 10.57s/it] 72%|███████▎  | 29/40 [05:00<01:53, 10.28s/it] 75%|███████▌  | 30/40 [05:09<01:41, 10.15s/it] 78%|███████▊  | 31/40 [05:19<01:28,  9.88s/it] 80%|████████  | 32/40 [05:28<01:18,  9.84s/it] 82%|████████▎ | 33/40 [05:45<01:22, 11.85s/it] 85%|████████▌ | 34/40 [05:55<01:07, 11.31s/it] 88%|████████▊ | 35/40 [06:04<00:53, 10.74s/it] 90%|█████████ | 36/40 [06:14<00:41, 10.34s/it] 92%|█████████▎| 37/40 [06:23<00:30, 10.09s/it] 95%|█████████▌| 38/40 [06:32<00:19,  9.77s/it] 98%|█████████▊| 39/40 [06:42<00:09,  9.76s/it]100%|██████████| 40/40 [06:51<00:00,  9.64s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:51<00:00,  9.64s/it]100%|██████████| 40/40 [06:51<00:00, 10.30s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 411.9245, 'train_samples_per_second': 6.81, 'train_steps_per_second': 0.097, 'train_loss': 0.7830683708190918, 'epoch': 4.91}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:13,  6.51s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:19<03:58,  6.45s/it] 10%|█         | 4/40 [00:25<03:54,  6.52s/it] 12%|█▎        | 5/40 [00:32<03:50,  6.58s/it] 15%|█▌        | 6/40 [00:39<03:41,  6.52s/it] 18%|█▊        | 7/40 [00:45<03:36,  6.55s/it] 20%|██        | 8/40 [00:51<03:27,  6.48s/it] 22%|██▎       | 9/40 [00:59<03:28,  6.73s/it] 25%|██▌       | 10/40 [01:05<03:18,  6.62s/it] 28%|██▊       | 11/40 [01:12<03:10,  6.58s/it] 30%|███       | 12/40 [01:18<03:03,  6.54s/it] 32%|███▎      | 13/40 [01:25<03:01,  6.73s/it] 35%|███▌      | 14/40 [01:32<02:57,  6.83s/it] 38%|███▊      | 15/40 [01:39<02:51,  6.88s/it] 40%|████      | 16/40 [01:46<02:45,  6.90s/it] 42%|████▎     | 17/40 [01:54<02:47,  7.28s/it] 45%|████▌     | 18/40 [02:01<02:38,  7.20s/it] 48%|████▊     | 19/40 [02:08<02:29,  7.13s/it] 50%|█████     | 20/40 [02:15<02:22,  7.11s/it] 52%|█████▎    | 21/40 [02:22<02:13,  7.05s/it] 55%|█████▌    | 22/40 [02:29<02:03,  6.86s/it] 57%|█████▊    | 23/40 [02:35<01:55,  6.81s/it] 60%|██████    | 24/40 [02:42<01:47,  6.71s/it] 62%|██████▎   | 25/40 [02:49<01:43,  6.92s/it] 65%|██████▌   | 26/40 [02:56<01:35,  6.80s/it] 68%|██████▊   | 27/40 [03:02<01:26,  6.63s/it] 70%|███████   | 28/40 [03:09<01:20,  6.69s/it] 72%|███████▎  | 29/40 [03:16<01:13,  6.68s/it] 75%|███████▌  | 30/40 [03:22<01:06,  6.62s/it] 78%|███████▊  | 31/40 [03:29<01:00,  6.70s/it] 80%|████████  | 32/40 [03:36<00:53,  6.68s/it] 82%|████████▎ | 33/40 [03:44<00:49,  7.09s/it] 85%|████████▌ | 34/40 [03:50<00:41,  6.94s/it] 88%|████████▊ | 35/40 [03:57<00:34,  6.89s/it] 90%|█████████ | 36/40 [04:03<00:27,  6.76s/it] 92%|█████████▎| 37/40 [04:10<00:20,  6.73s/it] 95%|█████████▌| 38/40 [04:17<00:13,  6.68s/it] 98%|█████████▊| 39/40 [04:23<00:06,  6.60s/it]100%|██████████| 40/40 [04:29<00:00,  6.54s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:29<00:00,  6.54s/it]100%|██████████| 40/40 [04:29<00:00,  6.75s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_iontransporters_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 269.982, 'train_samples_per_second': 9.667, 'train_steps_per_second': 0.148, 'train_loss': 0.6997037887573242, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:16,  6.58s/it]  5%|▌         | 2/40 [00:12<04:03,  6.41s/it]  8%|▊         | 3/40 [00:19<03:59,  6.48s/it] 10%|█         | 4/40 [00:26<03:55,  6.55s/it] 12%|█▎        | 5/40 [00:32<03:51,  6.62s/it] 15%|█▌        | 6/40 [00:39<03:42,  6.55s/it] 18%|█▊        | 7/40 [00:45<03:36,  6.57s/it] 20%|██        | 8/40 [00:52<03:27,  6.50s/it] 22%|██▎       | 9/40 [00:59<03:29,  6.76s/it] 25%|██▌       | 10/40 [01:06<03:20,  6.67s/it] 28%|██▊       | 11/40 [01:12<03:12,  6.64s/it] 30%|███       | 12/40 [01:19<03:04,  6.59s/it] 32%|███▎      | 13/40 [01:25<02:59,  6.66s/it] 35%|███▌      | 14/40 [01:32<02:52,  6.64s/it] 38%|███▊      | 15/40 [01:39<02:47,  6.72s/it] 40%|████      | 16/40 [01:46<02:40,  6.70s/it] 42%|████▎     | 17/40 [01:53<02:41,  7.01s/it] 45%|████▌     | 18/40 [02:00<02:32,  6.91s/it] 48%|████▊     | 19/40 [02:06<02:22,  6.78s/it] 50%|█████     | 20/40 [02:13<02:15,  6.76s/it] 52%|█████▎    | 21/40 [02:20<02:07,  6.68s/it] 55%|█████▌    | 22/40 [02:26<01:58,  6.60s/it] 57%|█████▊    | 23/40 [02:33<01:53,  6.68s/it] 60%|██████    | 24/40 [02:40<01:47,  6.69s/it] 62%|██████▎   | 25/40 [02:47<01:43,  6.93s/it] 65%|██████▌   | 26/40 [02:54<01:35,  6.81s/it] 68%|██████▊   | 27/40 [03:00<01:26,  6.69s/it] 70%|███████   | 28/40 [03:07<01:21,  6.83s/it] 72%|███████▎  | 29/40 [03:14<01:14,  6.81s/it] 75%|███████▌  | 30/40 [03:20<01:07,  6.71s/it] 78%|███████▊  | 31/40 [03:27<01:01,  6.81s/it] 80%|████████  | 32/40 [03:34<00:54,  6.82s/it] 82%|████████▎ | 33/40 [03:42<00:50,  7.20s/it] 85%|████████▌ | 34/40 [03:49<00:42,  7.08s/it] 88%|████████▊ | 35/40 [03:56<00:35,  7.06s/it] 90%|█████████ | 36/40 [04:03<00:27,  6.98s/it] 92%|█████████▎| 37/40 [04:10<00:20,  6.95s/it] 95%|█████████▌| 38/40 [04:17<00:13,  6.95s/it] 98%|█████████▊| 39/40 [04:24<00:06,  6.90s/it]100%|██████████| 40/40 [04:30<00:00,  6.87s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:30<00:00,  6.87s/it]100%|██████████| 40/40 [04:30<00:00,  6.77s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_iontransporters_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 270.9328, 'train_samples_per_second': 9.633, 'train_steps_per_second': 0.148, 'train_loss': 0.7170417785644532, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:18,  9.71s/it]  5%|▌         | 2/40 [00:19<05:59,  9.47s/it]  8%|▊         | 3/40 [00:28<05:53,  9.55s/it] 10%|█         | 4/40 [00:38<05:47,  9.64s/it] 12%|█▎        | 5/40 [00:48<05:41,  9.74s/it] 15%|█▌        | 6/40 [00:57<05:27,  9.63s/it] 18%|█▊        | 7/40 [01:07<05:20,  9.72s/it] 20%|██        | 8/40 [01:16<05:06,  9.56s/it] 22%|██▎       | 9/40 [01:27<05:08,  9.95s/it] 25%|██▌       | 10/40 [01:37<04:54,  9.81s/it] 28%|██▊       | 11/40 [01:46<04:43,  9.77s/it] 30%|███       | 12/40 [01:56<04:30,  9.66s/it] 32%|███▎      | 13/40 [02:06<04:23,  9.75s/it] 35%|███▌      | 14/40 [02:15<04:13,  9.74s/it] 38%|███▊      | 15/40 [02:26<04:05,  9.84s/it] 40%|████      | 16/40 [02:35<03:53,  9.72s/it] 42%|████▎     | 17/40 [02:46<03:55, 10.22s/it] 45%|████▌     | 18/40 [02:56<03:41, 10.05s/it] 48%|████▊     | 19/40 [03:05<03:26,  9.84s/it] 50%|█████     | 20/40 [03:15<03:14,  9.72s/it] 52%|█████▎    | 21/40 [03:25<03:04,  9.72s/it] 55%|█████▌    | 22/40 [03:34<02:52,  9.61s/it] 57%|█████▊    | 23/40 [03:44<02:45,  9.74s/it] 60%|██████    | 24/40 [03:53<02:33,  9.59s/it] 62%|██████▎   | 25/40 [04:04<02:29,  9.99s/it] 65%|██████▌   | 26/40 [04:14<02:17,  9.84s/it] 68%|██████▊   | 27/40 [04:23<02:04,  9.57s/it] 70%|███████   | 28/40 [04:32<01:55,  9.63s/it] 72%|███████▎  | 29/40 [04:42<01:45,  9.59s/it] 75%|███████▌  | 30/40 [04:51<01:34,  9.44s/it] 78%|███████▊  | 31/40 [05:01<01:25,  9.51s/it] 80%|████████  | 32/40 [05:10<01:15,  9.49s/it] 82%|████████▎ | 33/40 [05:21<01:09,  9.97s/it] 85%|████████▌ | 34/40 [05:30<00:58,  9.74s/it] 88%|████████▊ | 35/40 [05:40<00:48,  9.75s/it] 90%|█████████ | 36/40 [05:49<00:38,  9.62s/it] 92%|█████████▎| 37/40 [05:59<00:28,  9.62s/it] 95%|█████████▌| 38/40 [06:08<00:19,  9.55s/it] 98%|█████████▊| 39/40 [06:18<00:09,  9.42s/it]100%|██████████| 40/40 [06:27<00:00,  9.30s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:27<00:00,  9.30s/it]100%|██████████| 40/40 [06:27<00:00,  9.68s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_iontransporters_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 522
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 387.0505, 'train_samples_per_second': 6.743, 'train_steps_per_second': 0.103, 'train_loss': 0.6958465576171875, 'epoch': 4.98}
Dataset:  ionchannels_iontransporters_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:41, 10.31s/it]  5%|▌         | 2/40 [00:20<06:22, 10.05s/it]  8%|▊         | 3/40 [00:30<06:15, 10.16s/it] 10%|█         | 4/40 [00:41<06:11, 10.32s/it] 12%|█▎        | 5/40 [00:52<06:09, 10.57s/it] 15%|█▌        | 6/40 [01:02<05:55, 10.47s/it] 18%|█▊        | 7/40 [01:12<05:46, 10.51s/it] 20%|██        | 8/40 [01:23<05:32, 10.38s/it] 22%|██▎       | 9/40 [01:34<05:33, 10.77s/it] 25%|██▌       | 10/40 [01:44<05:18, 10.63s/it] 28%|██▊       | 11/40 [01:55<05:08, 10.65s/it] 30%|███       | 12/40 [02:06<04:55, 10.56s/it] 32%|███▎      | 13/40 [02:16<04:47, 10.64s/it] 35%|███▌      | 14/40 [02:27<04:35, 10.58s/it] 38%|███▊      | 15/40 [02:38<04:25, 10.64s/it] 40%|████      | 16/40 [02:48<04:14, 10.59s/it] 42%|████▎     | 17/40 [03:00<04:15, 11.12s/it] 45%|████▌     | 18/40 [03:11<04:01, 10.96s/it] 48%|████▊     | 19/40 [03:21<03:44, 10.71s/it] 50%|█████     | 20/40 [03:31<03:31, 10.56s/it] 52%|█████▎    | 21/40 [03:42<03:20, 10.53s/it] 55%|█████▌    | 22/40 [03:52<03:08, 10.47s/it] 57%|█████▊    | 23/40 [04:03<02:58, 10.52s/it] 60%|██████    | 24/40 [04:13<02:47, 10.44s/it] 62%|██████▎   | 25/40 [04:25<02:43, 10.87s/it] 65%|██████▌   | 26/40 [04:35<02:30, 10.78s/it] 68%|██████▊   | 27/40 [04:45<02:17, 10.56s/it] 70%|███████   | 28/40 [04:56<02:07, 10.64s/it] 72%|███████▎  | 29/40 [05:07<01:56, 10.57s/it] 75%|███████▌  | 30/40 [05:17<01:44, 10.46s/it] 78%|███████▊  | 31/40 [05:28<01:34, 10.55s/it] 80%|████████  | 32/40 [05:38<01:24, 10.54s/it] 82%|████████▎ | 33/40 [05:50<01:17, 11.04s/it] 85%|████████▌ | 34/40 [06:01<01:04, 10.78s/it] 88%|████████▊ | 35/40 [06:11<00:53, 10.75s/it] 90%|█████████ | 36/40 [06:21<00:42, 10.57s/it] 92%|█████████▎| 37/40 [06:32<00:31, 10.55s/it] 95%|█████████▌| 38/40 [06:42<00:21, 10.51s/it] 98%|█████████▊| 39/40 [06:52<00:10, 10.37s/it]100%|██████████| 40/40 [07:02<00:00, 10.25s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:02<00:00, 10.25s/it]100%|██████████| 40/40 [07:02<00:00, 10.57s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_iontransporters_train/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_iontransporters_train/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_iontransporters_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'train_runtime': 422.8136, 'train_samples_per_second': 6.173, 'train_steps_per_second': 0.095, 'train_loss': 0.7543684005737304, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:05<27:37,  5.83s/it]  1%|          | 2/285 [00:11<28:14,  5.99s/it]  1%|          | 3/285 [00:18<29:58,  6.38s/it]  1%|▏         | 4/285 [00:25<30:07,  6.43s/it]  2%|▏         | 5/285 [00:31<30:15,  6.48s/it]  2%|▏         | 6/285 [00:38<30:06,  6.47s/it]  2%|▏         | 7/285 [00:44<30:12,  6.52s/it]  3%|▎         | 8/285 [00:51<29:47,  6.45s/it]  3%|▎         | 9/285 [00:57<29:34,  6.43s/it]  4%|▎         | 10/285 [01:04<29:41,  6.48s/it]  4%|▍         | 11/285 [01:10<29:17,  6.41s/it]  4%|▍         | 12/285 [01:16<29:12,  6.42s/it]  5%|▍         | 13/285 [01:22<28:30,  6.29s/it]  5%|▍         | 14/285 [01:28<27:51,  6.17s/it]  5%|▌         | 15/285 [01:34<26:57,  5.99s/it]  6%|▌         | 16/285 [01:40<26:38,  5.94s/it]  6%|▌         | 17/285 [01:46<26:50,  6.01s/it]  6%|▋         | 18/285 [01:52<26:57,  6.06s/it]  7%|▋         | 19/285 [01:58<26:51,  6.06s/it]  7%|▋         | 20/285 [02:04<26:45,  6.06s/it]  7%|▋         | 21/285 [02:10<26:37,  6.05s/it]  8%|▊         | 22/285 [02:16<26:29,  6.04s/it]  8%|▊         | 23/285 [02:22<26:07,  5.98s/it]  8%|▊         | 24/285 [02:28<25:54,  5.95s/it]  9%|▉         | 25/285 [02:34<25:43,  5.94s/it]  9%|▉         | 26/285 [02:40<25:51,  5.99s/it]  9%|▉         | 27/285 [02:46<25:49,  6.01s/it] 10%|▉         | 28/285 [02:52<25:43,  6.01s/it] 10%|█         | 29/285 [02:58<25:20,  5.94s/it] 11%|█         | 30/285 [03:04<25:10,  5.92s/it] 11%|█         | 31/285 [03:09<24:56,  5.89s/it] 11%|█         | 32/285 [03:16<25:11,  5.98s/it] 12%|█▏        | 33/285 [03:21<24:46,  5.90s/it] 12%|█▏        | 34/285 [03:27<24:19,  5.82s/it] 12%|█▏        | 35/285 [03:33<24:19,  5.84s/it] 13%|█▎        | 36/285 [03:39<24:06,  5.81s/it] 13%|█▎        | 37/285 [03:45<24:25,  5.91s/it] 13%|█▎        | 38/285 [03:50<24:02,  5.84s/it] 14%|█▎        | 39/285 [03:56<23:53,  5.83s/it] 14%|█▍        | 40/285 [04:02<23:45,  5.82s/it] 14%|█▍        | 41/285 [04:08<23:32,  5.79s/it] 15%|█▍        | 42/285 [04:14<23:37,  5.83s/it] 15%|█▌        | 43/285 [04:20<23:48,  5.90s/it] 15%|█▌        | 44/285 [04:26<23:30,  5.85s/it] 16%|█▌        | 45/285 [04:31<23:17,  5.82s/it] 16%|█▌        | 46/285 [04:38<23:48,  5.98s/it] 16%|█▋        | 47/285 [04:43<23:36,  5.95s/it] 17%|█▋        | 48/285 [04:50<23:58,  6.07s/it] 17%|█▋        | 49/285 [04:56<23:46,  6.05s/it] 18%|█▊        | 50/285 [05:02<23:40,  6.04s/it] 18%|█▊        | 51/285 [05:08<23:46,  6.10s/it] 18%|█▊        | 52/285 [05:14<23:26,  6.04s/it] 19%|█▊        | 53/285 [05:20<23:18,  6.03s/it] 19%|█▉        | 54/285 [05:26<23:06,  6.00s/it] 19%|█▉        | 55/285 [05:32<23:01,  6.00s/it] 20%|█▉        | 56/285 [05:38<22:47,  5.97s/it] 20%|██        | 57/285 [05:44<22:34,  5.94s/it] 20%|██        | 58/285 [05:54<27:46,  7.34s/it] 21%|██        | 59/285 [06:00<26:14,  6.97s/it] 21%|██        | 60/285 [06:07<25:14,  6.73s/it] 21%|██▏       | 61/285 [06:12<24:10,  6.48s/it] 22%|██▏       | 62/285 [06:18<22:57,  6.17s/it] 22%|██▏       | 63/285 [06:24<22:40,  6.13s/it] 22%|██▏       | 64/285 [06:30<22:40,  6.15s/it] 23%|██▎       | 65/285 [06:36<22:34,  6.16s/it] 23%|██▎       | 66/285 [06:42<22:02,  6.04s/it] 24%|██▎       | 67/285 [06:48<21:57,  6.04s/it] 24%|██▍       | 68/285 [06:54<22:00,  6.08s/it] 24%|██▍       | 69/285 [07:00<21:22,  5.94s/it] 25%|██▍       | 70/285 [07:06<21:05,  5.89s/it] 25%|██▍       | 71/285 [07:12<21:12,  5.94s/it] 25%|██▌       | 72/285 [07:18<21:11,  5.97s/it] 26%|██▌       | 73/285 [07:24<20:51,  5.91s/it] 26%|██▌       | 74/285 [07:29<20:42,  5.89s/it] 26%|██▋       | 75/285 [07:36<20:55,  5.98s/it] 27%|██▋       | 76/285 [07:42<21:21,  6.13s/it] 27%|██▋       | 77/285 [07:48<21:17,  6.14s/it] 27%|██▋       | 78/285 [07:54<21:16,  6.16s/it] 28%|██▊       | 79/285 [08:01<21:24,  6.23s/it] 28%|██▊       | 80/285 [08:07<21:22,  6.26s/it] 28%|██▊       | 81/285 [08:13<20:55,  6.15s/it] 29%|██▉       | 82/285 [08:19<20:37,  6.09s/it] 29%|██▉       | 83/285 [08:25<20:33,  6.11s/it] 29%|██▉       | 84/285 [08:32<20:54,  6.24s/it] 30%|██▉       | 85/285 [08:38<21:10,  6.35s/it] 30%|███       | 86/285 [08:45<20:54,  6.31s/it] 31%|███       | 87/285 [08:51<20:53,  6.33s/it] 31%|███       | 88/285 [08:58<21:09,  6.45s/it] 31%|███       | 89/285 [09:04<21:18,  6.52s/it] 32%|███▏      | 90/285 [09:11<21:22,  6.58s/it] 32%|███▏      | 91/285 [09:18<21:13,  6.57s/it] 32%|███▏      | 92/285 [09:24<20:53,  6.49s/it] 33%|███▎      | 93/285 [09:30<20:47,  6.50s/it] 33%|███▎      | 94/285 [09:37<20:40,  6.49s/it] 33%|███▎      | 95/285 [09:43<20:38,  6.52s/it] 34%|███▎      | 96/285 [09:50<20:25,  6.48s/it] 34%|███▍      | 97/285 [09:56<20:20,  6.49s/it] 34%|███▍      | 98/285 [10:03<19:52,  6.38s/it] 35%|███▍      | 99/285 [10:09<19:37,  6.33s/it] 35%|███▌      | 100/285 [10:15<19:42,  6.39s/it] 35%|███▌      | 101/285 [10:22<19:52,  6.48s/it] 36%|███▌      | 102/285 [10:28<19:37,  6.44s/it] 36%|███▌      | 103/285 [10:35<19:27,  6.42s/it] 36%|███▋      | 104/285 [10:41<19:24,  6.43s/it] 37%|███▋      | 105/285 [10:47<19:05,  6.37s/it] 37%|███▋      | 106/285 [10:54<19:03,  6.39s/it] 38%|███▊      | 107/285 [11:00<18:49,  6.35s/it] 38%|███▊      | 108/285 [11:06<18:42,  6.34s/it] 38%|███▊      | 109/285 [11:13<18:26,  6.29s/it] 39%|███▊      | 110/285 [11:19<18:34,  6.37s/it] 39%|███▉      | 111/285 [11:26<18:59,  6.55s/it] 39%|███▉      | 112/285 [11:33<19:19,  6.70s/it] 40%|███▉      | 113/285 [11:40<19:12,  6.70s/it] 40%|████      | 114/285 [11:47<19:18,  6.77s/it] 40%|████      | 115/285 [11:59<23:36,  8.33s/it] 41%|████      | 116/285 [12:06<22:18,  7.92s/it] 41%|████      | 117/285 [12:13<21:25,  7.65s/it] 41%|████▏     | 118/285 [12:20<20:44,  7.45s/it] 42%|████▏     | 119/285 [12:26<19:43,  7.13s/it] 42%|████▏     | 120/285 [12:33<19:12,  6.99s/it] 42%|████▏     | 121/285 [12:39<18:41,  6.84s/it] 43%|████▎     | 122/285 [12:46<18:22,  6.76s/it] 43%|████▎     | 123/285 [12:53<18:13,  6.75s/it] 44%|████▎     | 124/285 [12:59<17:58,  6.70s/it] 44%|████▍     | 125/285 [13:06<17:53,  6.71s/it] 44%|████▍     | 126/285 [13:12<17:45,  6.70s/it] 45%|████▍     | 127/285 [13:19<17:28,  6.63s/it] 45%|████▍     | 128/285 [13:26<17:29,  6.69s/it] 45%|████▌     | 129/285 [13:32<17:10,  6.60s/it] 46%|████▌     | 130/285 [13:39<17:09,  6.64s/it] 46%|████▌     | 131/285 [13:46<17:09,  6.68s/it] 46%|████▋     | 132/285 [13:52<17:00,  6.67s/it] 47%|████▋     | 133/285 [13:59<16:57,  6.69s/it] 47%|████▋     | 134/285 [14:06<16:54,  6.72s/it] 47%|████▋     | 135/285 [14:13<16:44,  6.70s/it] 48%|████▊     | 136/285 [14:19<16:26,  6.62s/it] 48%|████▊     | 137/285 [14:26<16:20,  6.62s/it] 48%|████▊     | 138/285 [14:32<16:10,  6.60s/it] 49%|████▉     | 139/285 [14:38<15:51,  6.52s/it] 49%|████▉     | 140/285 [14:45<15:44,  6.52s/it] 49%|████▉     | 141/285 [14:51<15:37,  6.51s/it] 50%|████▉     | 142/285 [14:58<15:30,  6.51s/it] 50%|█████     | 143/285 [15:04<15:05,  6.38s/it] 51%|█████     | 144/285 [15:11<15:06,  6.43s/it] 51%|█████     | 145/285 [15:17<15:11,  6.51s/it] 51%|█████     | 146/285 [15:24<15:01,  6.48s/it] 52%|█████▏    | 147/285 [15:30<14:56,  6.49s/it] 52%|█████▏    | 148/285 [15:37<14:47,  6.48s/it] 52%|█████▏    | 149/285 [15:43<14:47,  6.53s/it] 53%|█████▎    | 150/285 [15:50<14:44,  6.55s/it] 53%|█████▎    | 151/285 [15:56<14:28,  6.48s/it] 53%|█████▎    | 152/285 [16:03<14:21,  6.48s/it] 54%|█████▎    | 153/285 [16:09<14:12,  6.46s/it] 54%|█████▍    | 154/285 [16:16<14:13,  6.52s/it] 54%|█████▍    | 155/285 [16:22<14:12,  6.56s/it] 55%|█████▍    | 156/285 [16:29<14:02,  6.53s/it] 55%|█████▌    | 157/285 [16:35<13:52,  6.50s/it] 55%|█████▌    | 158/285 [16:42<13:45,  6.50s/it] 56%|█████▌    | 159/285 [16:49<13:47,  6.57s/it] 56%|█████▌    | 160/285 [16:55<13:34,  6.52s/it] 56%|█████▋    | 161/285 [17:02<13:32,  6.55s/it] 57%|█████▋    | 162/285 [17:08<13:22,  6.53s/it] 57%|█████▋    | 163/285 [17:15<13:14,  6.51s/it] 58%|█████▊    | 164/285 [17:21<13:00,  6.45s/it] 58%|█████▊    | 165/285 [17:28<13:18,  6.66s/it] 58%|█████▊    | 166/285 [17:34<13:05,  6.60s/it] 59%|█████▊    | 167/285 [17:41<12:58,  6.60s/it] 59%|█████▉    | 168/285 [17:47<12:43,  6.52s/it] 59%|█████▉    | 169/285 [17:54<12:25,  6.43s/it] 60%|█████▉    | 170/285 [18:00<12:19,  6.43s/it] 60%|██████    | 171/285 [18:06<12:05,  6.37s/it] 60%|██████    | 172/285 [18:17<14:11,  7.54s/it] 61%|██████    | 173/285 [18:23<13:27,  7.21s/it] 61%|██████    | 174/285 [18:29<12:50,  6.94s/it] 61%|██████▏   | 175/285 [18:36<12:20,  6.73s/it] 62%|██████▏   | 176/285 [18:42<11:56,  6.57s/it] 62%|██████▏   | 177/285 [18:48<11:43,  6.52s/it] 62%|██████▏   | 178/285 [18:54<11:30,  6.46s/it] 63%|██████▎   | 179/285 [19:01<11:21,  6.43s/it] 63%|██████▎   | 180/285 [19:07<11:09,  6.38s/it] 64%|██████▎   | 181/285 [19:14<11:13,  6.48s/it] 64%|██████▍   | 182/285 [19:20<11:10,  6.51s/it] 64%|██████▍   | 183/285 [19:27<11:06,  6.53s/it] 65%|██████▍   | 184/285 [19:33<10:47,  6.41s/it] 65%|██████▍   | 185/285 [19:40<10:53,  6.53s/it] 65%|██████▌   | 186/285 [19:47<11:06,  6.73s/it] 66%|██████▌   | 187/285 [19:54<10:51,  6.65s/it] 66%|██████▌   | 188/285 [20:00<10:31,  6.51s/it] 66%|██████▋   | 189/285 [20:06<10:23,  6.49s/it] 67%|██████▋   | 190/285 [20:12<10:07,  6.39s/it] 67%|██████▋   | 191/285 [20:19<10:02,  6.41s/it] 67%|██████▋   | 192/285 [20:25<09:57,  6.43s/it] 68%|██████▊   | 193/285 [20:32<09:48,  6.39s/it] 68%|██████▊   | 194/285 [20:38<09:37,  6.35s/it] 68%|██████▊   | 195/285 [20:44<09:35,  6.40s/it] 69%|██████▉   | 196/285 [20:50<09:19,  6.29s/it] 69%|██████▉   | 197/285 [20:57<09:16,  6.33s/it] 69%|██████▉   | 198/285 [21:03<09:14,  6.38s/it] 70%|██████▉   | 199/285 [21:09<09:01,  6.30s/it] 70%|███████   | 200/285 [21:16<08:57,  6.32s/it]                                                  70%|███████   | 200/285 [21:16<08:57,  6.32s/it] 71%|███████   | 201/285 [21:22<08:53,  6.35s/it] 71%|███████   | 202/285 [21:28<08:45,  6.34s/it] 71%|███████   | 203/285 [21:35<08:38,  6.33s/it] 72%|███████▏  | 204/285 [21:41<08:36,  6.37s/it] 72%|███████▏  | 205/285 [21:48<08:28,  6.36s/it] 72%|███████▏  | 206/285 [21:54<08:14,  6.26s/it] 73%|███████▎  | 207/285 [22:00<08:08,  6.27s/it] 73%|███████▎  | 208/285 [22:06<07:55,  6.18s/it] 73%|███████▎  | 209/285 [22:12<07:46,  6.14s/it] 74%|███████▎  | 210/285 [22:18<07:38,  6.11s/it] 74%|███████▍  | 211/285 [22:24<07:36,  6.17s/it] 74%|███████▍  | 212/285 [22:31<07:37,  6.26s/it] 75%|███████▍  | 213/285 [22:38<07:44,  6.45s/it] 75%|███████▌  | 214/285 [22:44<07:42,  6.51s/it] 75%|███████▌  | 215/285 [22:51<07:42,  6.61s/it] 76%|███████▌  | 216/285 [22:58<07:37,  6.63s/it] 76%|███████▌  | 217/285 [23:05<07:33,  6.66s/it] 76%|███████▋  | 218/285 [23:11<07:25,  6.65s/it] 77%|███████▋  | 219/285 [23:18<07:20,  6.67s/it] 77%|███████▋  | 220/285 [23:25<07:15,  6.70s/it] 78%|███████▊  | 221/285 [23:32<07:13,  6.77s/it] 78%|███████▊  | 222/285 [23:38<07:00,  6.67s/it] 78%|███████▊  | 223/285 [23:44<06:47,  6.57s/it] 79%|███████▊  | 224/285 [23:51<06:46,  6.66s/it] 79%|███████▉  | 225/285 [23:58<06:37,  6.62s/it] 79%|███████▉  | 226/285 [24:04<06:30,  6.63s/it] 80%|███████▉  | 227/285 [24:11<06:20,  6.57s/it] 80%|████████  | 228/285 [24:17<06:09,  6.49s/it] 80%|████████  | 229/285 [24:28<07:21,  7.88s/it] 81%|████████  | 230/285 [24:35<06:52,  7.49s/it] 81%|████████  | 231/285 [24:41<06:29,  7.22s/it] 81%|████████▏ | 232/285 [24:48<06:11,  7.02s/it] 82%|████████▏ | 233/285 [24:54<05:54,  6.82s/it] 82%|████████▏ | 234/285 [25:01<05:42,  6.71s/it] 82%|████████▏ | 235/285 [25:07<05:30,  6.61s/it] 83%|████████▎ | 236/285 [25:13<05:18,  6.49s/it] 83%|████████▎ | 237/285 [25:19<05:03,  6.33s/it] 84%|████████▎ | 238/285 [25:26<04:59,  6.37s/it] 84%|████████▍ | 239/285 [25:32<04:49,  6.29s/it] 84%|████████▍ | 240/285 [25:38<04:43,  6.31s/it] 85%|████████▍ | 241/285 [25:45<04:38,  6.33s/it] 85%|████████▍ | 242/285 [25:51<04:31,  6.32s/it] 85%|████████▌ | 243/285 [25:57<04:22,  6.26s/it] 86%|████████▌ | 244/285 [26:03<04:16,  6.25s/it] 86%|████████▌ | 245/285 [26:10<04:12,  6.31s/it] 86%|████████▋ | 246/285 [26:16<04:06,  6.32s/it] 87%|████████▋ | 247/285 [26:23<04:04,  6.44s/it] 87%|████████▋ | 248/285 [26:29<03:57,  6.41s/it] 87%|████████▋ | 249/285 [26:36<03:52,  6.45s/it] 88%|████████▊ | 250/285 [26:42<03:46,  6.46s/it] 88%|████████▊ | 251/285 [26:48<03:38,  6.43s/it] 88%|████████▊ | 252/285 [26:55<03:36,  6.56s/it] 89%|████████▉ | 253/285 [27:02<03:30,  6.58s/it] 89%|████████▉ | 254/285 [27:09<03:24,  6.61s/it] 89%|████████▉ | 255/285 [27:15<03:19,  6.65s/it] 90%|████████▉ | 256/285 [27:22<03:11,  6.59s/it] 90%|█████████ | 257/285 [27:28<03:03,  6.54s/it] 91%|█████████ | 258/285 [27:35<02:55,  6.51s/it] 91%|█████████ | 259/285 [27:41<02:48,  6.48s/it] 91%|█████████ | 260/285 [27:47<02:40,  6.41s/it] 92%|█████████▏| 261/285 [27:54<02:34,  6.45s/it] 92%|█████████▏| 262/285 [28:00<02:26,  6.37s/it] 92%|█████████▏| 263/285 [28:06<02:18,  6.29s/it] 93%|█████████▎| 264/285 [28:12<02:11,  6.27s/it] 93%|█████████▎| 265/285 [28:19<02:05,  6.28s/it] 93%|█████████▎| 266/285 [28:25<02:00,  6.32s/it] 94%|█████████▎| 267/285 [28:32<01:56,  6.45s/it] 94%|█████████▍| 268/285 [28:38<01:49,  6.42s/it] 94%|█████████▍| 269/285 [28:45<01:43,  6.47s/it] 95%|█████████▍| 270/285 [28:51<01:37,  6.52s/it] 95%|█████████▌| 271/285 [28:58<01:32,  6.63s/it] 95%|█████████▌| 272/285 [29:05<01:27,  6.76s/it] 96%|█████████▌| 273/285 [29:12<01:19,  6.65s/it] 96%|█████████▌| 274/285 [29:19<01:13,  6.68s/it] 96%|█████████▋| 275/285 [29:25<01:05,  6.58s/it] 97%|█████████▋| 276/285 [29:31<00:58,  6.51s/it] 97%|█████████▋| 277/285 [29:38<00:51,  6.46s/it] 98%|█████████▊| 278/285 [29:44<00:44,  6.41s/it] 98%|█████████▊| 279/285 [29:50<00:38,  6.39s/it] 98%|█████████▊| 280/285 [29:57<00:32,  6.43s/it] 99%|█████████▊| 281/285 [30:03<00:25,  6.40s/it] 99%|█████████▉| 282/285 [30:09<00:19,  6.39s/it] 99%|█████████▉| 283/285 [30:16<00:12,  6.41s/it]100%|█████████▉| 284/285 [30:23<00:06,  6.48s/it]100%|██████████| 285/285 [30:29<00:00,  6.43s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [30:29<00:00,  6.43s/it]100%|██████████| 285/285 [30:29<00:00,  6.42s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'loss': 0.3619, 'learning_rate': 9.85e-06, 'epoch': 3.5}
{'train_runtime': 1829.3728, 'train_samples_per_second': 10.096, 'train_steps_per_second': 0.156, 'train_loss': 0.2911775555527001, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:06<30:56,  6.54s/it]  1%|          | 2/285 [00:13<30:48,  6.53s/it]  1%|          | 3/285 [00:19<31:04,  6.61s/it]  1%|▏         | 4/285 [00:26<31:04,  6.63s/it]  2%|▏         | 5/285 [00:33<30:50,  6.61s/it]  2%|▏         | 6/285 [00:39<30:35,  6.58s/it]  2%|▏         | 7/285 [00:45<30:04,  6.49s/it]  3%|▎         | 8/285 [00:51<29:28,  6.39s/it]  3%|▎         | 9/285 [00:58<29:00,  6.30s/it]  4%|▎         | 10/285 [01:04<29:17,  6.39s/it]  4%|▍         | 11/285 [01:11<29:17,  6.41s/it]  4%|▍         | 12/285 [01:17<29:21,  6.45s/it]  5%|▍         | 13/285 [01:24<29:02,  6.40s/it]  5%|▍         | 14/285 [01:30<28:53,  6.40s/it]  5%|▌         | 15/285 [01:36<28:24,  6.31s/it]  6%|▌         | 16/285 [01:42<28:15,  6.30s/it]  6%|▌         | 17/285 [01:49<28:29,  6.38s/it]  6%|▋         | 18/285 [01:55<28:39,  6.44s/it]  7%|▋         | 19/285 [02:02<28:43,  6.48s/it]  7%|▋         | 20/285 [02:09<28:44,  6.51s/it]  7%|▋         | 21/285 [02:15<28:23,  6.45s/it]  8%|▊         | 22/285 [02:21<28:14,  6.44s/it]  8%|▊         | 23/285 [02:28<27:54,  6.39s/it]  8%|▊         | 24/285 [02:34<28:19,  6.51s/it]  9%|▉         | 25/285 [02:41<28:12,  6.51s/it]  9%|▉         | 26/285 [02:47<28:12,  6.53s/it]  9%|▉         | 27/285 [02:54<28:22,  6.60s/it] 10%|▉         | 28/285 [03:01<28:21,  6.62s/it] 10%|█         | 29/285 [03:07<28:06,  6.59s/it] 11%|█         | 30/285 [03:14<28:05,  6.61s/it] 11%|█         | 31/285 [03:21<28:20,  6.69s/it] 11%|█         | 32/285 [03:28<28:29,  6.76s/it] 12%|█▏        | 33/285 [03:34<28:10,  6.71s/it] 12%|█▏        | 34/285 [03:41<27:57,  6.68s/it] 12%|█▏        | 35/285 [03:48<27:37,  6.63s/it] 13%|█▎        | 36/285 [03:54<27:22,  6.60s/it] 13%|█▎        | 37/285 [04:01<27:25,  6.64s/it] 13%|█▎        | 38/285 [04:07<27:06,  6.58s/it] 14%|█▎        | 39/285 [04:14<26:38,  6.50s/it] 14%|█▍        | 40/285 [04:20<26:13,  6.42s/it] 14%|█▍        | 41/285 [04:26<26:08,  6.43s/it] 15%|█▍        | 42/285 [04:33<26:36,  6.57s/it] 15%|█▌        | 43/285 [04:40<26:31,  6.58s/it] 15%|█▌        | 44/285 [04:46<26:09,  6.51s/it] 16%|█▌        | 45/285 [04:53<26:18,  6.58s/it] 16%|█▌        | 46/285 [05:00<26:32,  6.66s/it] 16%|█▋        | 47/285 [05:06<26:04,  6.57s/it] 17%|█▋        | 48/285 [05:13<26:06,  6.61s/it] 17%|█▋        | 49/285 [05:19<25:55,  6.59s/it] 18%|█▊        | 50/285 [05:26<25:48,  6.59s/it] 18%|█▊        | 51/285 [05:33<25:56,  6.65s/it] 18%|█▊        | 52/285 [05:39<25:41,  6.62s/it] 19%|█▊        | 53/285 [05:46<25:34,  6.61s/it] 19%|█▉        | 54/285 [05:52<25:05,  6.52s/it] 19%|█▉        | 55/285 [05:59<25:01,  6.53s/it] 20%|█▉        | 56/285 [06:05<24:49,  6.50s/it] 20%|██        | 57/285 [06:12<24:34,  6.47s/it] 20%|██        | 58/285 [06:23<29:50,  7.89s/it] 21%|██        | 59/285 [06:29<28:11,  7.48s/it] 21%|██        | 60/285 [06:36<27:01,  7.21s/it] 21%|██▏       | 61/285 [06:42<26:01,  6.97s/it] 22%|██▏       | 62/285 [06:48<24:53,  6.70s/it] 22%|██▏       | 63/285 [06:55<24:31,  6.63s/it] 22%|██▏       | 64/285 [07:01<24:27,  6.64s/it] 23%|██▎       | 65/285 [07:08<24:12,  6.60s/it] 23%|██▎       | 66/285 [07:14<23:34,  6.46s/it] 24%|██▎       | 67/285 [07:21<23:29,  6.47s/it] 24%|██▍       | 68/285 [07:27<23:23,  6.47s/it] 24%|██▍       | 69/285 [07:33<22:46,  6.33s/it] 25%|██▍       | 70/285 [07:39<22:25,  6.26s/it] 25%|██▍       | 71/285 [07:46<22:26,  6.29s/it] 25%|██▌       | 72/285 [07:52<22:23,  6.31s/it] 26%|██▌       | 73/285 [07:58<21:57,  6.22s/it] 26%|██▌       | 74/285 [08:04<21:37,  6.15s/it] 26%|██▋       | 75/285 [08:10<21:43,  6.21s/it] 27%|██▋       | 76/285 [08:17<21:56,  6.30s/it] 27%|██▋       | 77/285 [08:23<21:41,  6.26s/it] 27%|██▋       | 78/285 [08:29<21:37,  6.27s/it] 28%|██▊       | 79/285 [08:36<21:53,  6.37s/it] 28%|██▊       | 80/285 [08:42<21:49,  6.39s/it] 28%|██▊       | 81/285 [08:48<21:33,  6.34s/it] 29%|██▉       | 82/285 [08:55<21:28,  6.35s/it] 29%|██▉       | 83/285 [09:01<21:30,  6.39s/it] 29%|██▉       | 84/285 [09:08<21:29,  6.42s/it] 30%|██▉       | 85/285 [09:14<21:06,  6.33s/it] 30%|███       | 86/285 [09:20<21:12,  6.39s/it] 31%|███       | 87/285 [09:26<20:33,  6.23s/it] 31%|███       | 88/285 [09:33<20:30,  6.25s/it] 31%|███       | 89/285 [09:39<20:29,  6.28s/it] 32%|███▏      | 90/285 [09:45<20:28,  6.30s/it] 32%|███▏      | 91/285 [09:52<20:48,  6.44s/it] 32%|███▏      | 92/285 [09:58<20:34,  6.40s/it] 33%|███▎      | 93/285 [10:05<20:16,  6.34s/it] 33%|███▎      | 94/285 [10:11<20:09,  6.33s/it] 33%|███▎      | 95/285 [10:17<19:55,  6.29s/it] 34%|███▎      | 96/285 [10:23<19:40,  6.24s/it] 34%|███▍      | 97/285 [10:30<20:04,  6.41s/it] 34%|███▍      | 98/285 [10:36<19:46,  6.34s/it] 35%|███▍      | 99/285 [10:42<19:26,  6.27s/it] 35%|███▌      | 100/285 [10:49<19:35,  6.35s/it] 35%|███▌      | 101/285 [10:56<19:49,  6.46s/it] 36%|███▌      | 102/285 [11:02<19:32,  6.41s/it] 36%|███▌      | 103/285 [11:08<19:22,  6.39s/it] 36%|███▋      | 104/285 [11:15<19:34,  6.49s/it] 37%|███▋      | 105/285 [11:21<19:26,  6.48s/it] 37%|███▋      | 106/285 [11:28<19:36,  6.57s/it] 38%|███▊      | 107/285 [11:35<19:43,  6.65s/it] 38%|███▊      | 108/285 [11:42<19:44,  6.69s/it] 38%|███▊      | 109/285 [11:48<19:31,  6.66s/it] 39%|███▊      | 110/285 [11:55<19:16,  6.61s/it] 39%|███▉      | 111/285 [12:01<19:05,  6.59s/it] 39%|███▉      | 112/285 [12:08<18:55,  6.56s/it] 40%|███▉      | 113/285 [12:14<18:44,  6.54s/it] 40%|████      | 114/285 [12:21<18:48,  6.60s/it] 40%|████      | 115/285 [12:32<22:47,  8.05s/it] 41%|████      | 116/285 [12:39<21:23,  7.60s/it] 41%|████      | 117/285 [12:46<20:26,  7.30s/it] 41%|████▏     | 118/285 [12:52<19:33,  7.03s/it] 42%|████▏     | 119/285 [12:58<18:47,  6.80s/it] 42%|████▏     | 120/285 [13:05<18:19,  6.67s/it] 42%|████▏     | 121/285 [13:11<18:01,  6.60s/it] 43%|████▎     | 122/285 [13:18<18:06,  6.67s/it] 43%|████▎     | 123/285 [13:25<18:12,  6.74s/it] 44%|████▎     | 124/285 [13:32<18:06,  6.75s/it] 44%|████▍     | 125/285 [13:38<17:48,  6.68s/it] 44%|████▍     | 126/285 [13:45<17:37,  6.65s/it] 45%|████▍     | 127/285 [13:51<17:27,  6.63s/it] 45%|████▍     | 128/285 [13:58<17:20,  6.63s/it] 45%|████▌     | 129/285 [14:04<16:58,  6.53s/it] 46%|████▌     | 130/285 [14:10<16:32,  6.40s/it] 46%|████▌     | 131/285 [14:17<16:39,  6.49s/it] 46%|████▋     | 132/285 [14:23<16:26,  6.45s/it] 47%|████▋     | 133/285 [14:30<16:07,  6.37s/it] 47%|████▋     | 134/285 [14:36<16:06,  6.40s/it] 47%|████▋     | 135/285 [14:42<15:40,  6.27s/it] 48%|████▊     | 136/285 [14:48<15:20,  6.18s/it] 48%|████▊     | 137/285 [14:54<15:20,  6.22s/it] 48%|████▊     | 138/285 [15:01<15:19,  6.25s/it] 49%|████▉     | 139/285 [15:07<15:09,  6.23s/it] 49%|████▉     | 140/285 [15:13<14:57,  6.19s/it] 49%|████▉     | 141/285 [15:19<14:58,  6.24s/it] 50%|████▉     | 142/285 [15:26<15:03,  6.32s/it] 50%|█████     | 143/285 [15:32<14:41,  6.21s/it] 51%|█████     | 144/285 [15:38<14:49,  6.31s/it] 51%|█████     | 145/285 [15:45<14:53,  6.38s/it] 51%|█████     | 146/285 [15:51<14:38,  6.32s/it] 52%|█████▏    | 147/285 [15:57<14:25,  6.27s/it] 52%|█████▏    | 148/285 [16:03<14:11,  6.22s/it] 52%|█████▏    | 149/285 [16:10<14:30,  6.40s/it] 53%|█████▎    | 150/285 [16:17<14:28,  6.44s/it] 53%|█████▎    | 151/285 [16:23<14:07,  6.33s/it] 53%|█████▎    | 152/285 [16:29<13:56,  6.29s/it] 54%|█████▎    | 153/285 [16:35<13:47,  6.27s/it] 54%|█████▍    | 154/285 [16:42<13:53,  6.36s/it] 54%|█████▍    | 155/285 [16:48<13:44,  6.34s/it] 55%|█████▍    | 156/285 [16:54<13:38,  6.35s/it] 55%|█████▌    | 157/285 [17:01<13:31,  6.34s/it] 55%|█████▌    | 158/285 [17:07<13:22,  6.32s/it] 56%|█████▌    | 159/285 [17:13<13:23,  6.38s/it] 56%|█████▌    | 160/285 [17:20<13:09,  6.32s/it] 56%|█████▋    | 161/285 [17:26<13:08,  6.36s/it] 57%|█████▋    | 162/285 [17:32<12:55,  6.31s/it] 57%|█████▋    | 163/285 [17:38<12:48,  6.30s/it] 58%|█████▊    | 164/285 [17:44<12:29,  6.19s/it] 58%|█████▊    | 165/285 [17:51<12:45,  6.38s/it] 58%|█████▊    | 166/285 [17:58<12:40,  6.39s/it] 59%|█████▊    | 167/285 [18:04<12:37,  6.42s/it] 59%|█████▉    | 168/285 [18:10<12:25,  6.37s/it] 59%|█████▉    | 169/285 [18:17<12:15,  6.34s/it] 60%|█████▉    | 170/285 [18:23<12:08,  6.33s/it] 60%|██████    | 171/285 [18:29<12:01,  6.33s/it] 60%|██████    | 172/285 [18:40<14:08,  7.51s/it] 61%|██████    | 173/285 [18:46<13:12,  7.08s/it] 61%|██████    | 174/285 [18:52<12:43,  6.88s/it] 61%|██████▏   | 175/285 [18:59<12:24,  6.77s/it] 62%|██████▏   | 176/285 [19:05<12:02,  6.63s/it] 62%|██████▏   | 177/285 [19:12<11:57,  6.64s/it] 62%|██████▏   | 178/285 [19:18<11:45,  6.59s/it] 63%|██████▎   | 179/285 [19:24<11:31,  6.52s/it] 63%|██████▎   | 180/285 [19:31<11:17,  6.45s/it] 64%|██████▎   | 181/285 [19:38<11:23,  6.58s/it] 64%|██████▍   | 182/285 [19:44<11:09,  6.50s/it] 64%|██████▍   | 183/285 [19:50<11:02,  6.49s/it] 65%|██████▍   | 184/285 [19:57<10:47,  6.41s/it] 65%|██████▍   | 185/285 [20:03<10:47,  6.47s/it] 65%|██████▌   | 186/285 [20:10<10:38,  6.45s/it] 66%|██████▌   | 187/285 [20:16<10:26,  6.39s/it] 66%|██████▌   | 188/285 [20:22<10:15,  6.34s/it] 66%|██████▋   | 189/285 [20:29<10:13,  6.39s/it] 67%|██████▋   | 190/285 [20:35<10:05,  6.37s/it] 67%|██████▋   | 191/285 [20:41<09:59,  6.38s/it] 67%|██████▋   | 192/285 [20:48<09:52,  6.37s/it] 68%|██████▊   | 193/285 [20:54<09:52,  6.45s/it] 68%|██████▊   | 194/285 [21:01<09:43,  6.42s/it] 68%|██████▊   | 195/285 [21:07<09:41,  6.46s/it] 69%|██████▉   | 196/285 [21:13<09:28,  6.39s/it] 69%|██████▉   | 197/285 [21:20<09:26,  6.44s/it] 69%|██████▉   | 198/285 [21:26<09:20,  6.44s/it] 70%|██████▉   | 199/285 [21:33<09:10,  6.40s/it] 70%|███████   | 200/285 [21:40<09:17,  6.56s/it]                                                  70%|███████   | 200/285 [21:40<09:17,  6.56s/it] 71%|███████   | 201/285 [21:46<09:14,  6.61s/it] 71%|███████   | 202/285 [21:53<09:07,  6.60s/it] 71%|███████   | 203/285 [22:00<09:02,  6.62s/it] 72%|███████▏  | 204/285 [22:06<08:58,  6.64s/it] 72%|███████▏  | 205/285 [22:13<08:48,  6.60s/it] 72%|███████▏  | 206/285 [22:19<08:35,  6.53s/it] 73%|███████▎  | 207/285 [22:26<08:25,  6.48s/it] 73%|███████▎  | 208/285 [22:32<08:12,  6.39s/it] 73%|███████▎  | 209/285 [22:38<08:02,  6.34s/it] 74%|███████▎  | 210/285 [22:44<07:54,  6.33s/it] 74%|███████▍  | 211/285 [22:51<08:00,  6.50s/it] 74%|███████▍  | 212/285 [22:57<07:49,  6.44s/it] 75%|███████▍  | 213/285 [23:04<07:47,  6.49s/it] 75%|███████▌  | 214/285 [23:10<07:34,  6.40s/it] 75%|███████▌  | 215/285 [23:16<07:20,  6.30s/it] 76%|███████▌  | 216/285 [23:23<07:15,  6.30s/it] 76%|███████▌  | 217/285 [23:29<07:03,  6.23s/it] 76%|███████▋  | 218/285 [23:34<06:49,  6.11s/it] 77%|███████▋  | 219/285 [23:40<06:40,  6.07s/it] 77%|███████▋  | 220/285 [23:47<06:36,  6.09s/it] 78%|███████▊  | 221/285 [23:53<06:34,  6.17s/it] 78%|███████▊  | 222/285 [23:59<06:19,  6.02s/it] 78%|███████▊  | 223/285 [24:05<06:11,  6.00s/it] 79%|███████▊  | 224/285 [24:11<06:11,  6.10s/it] 79%|███████▉  | 225/285 [24:17<06:07,  6.13s/it] 79%|███████▉  | 226/285 [24:24<06:09,  6.26s/it] 80%|███████▉  | 227/285 [24:30<06:04,  6.28s/it] 80%|████████  | 228/285 [24:36<05:58,  6.29s/it] 80%|████████  | 229/285 [24:47<07:12,  7.73s/it] 81%|████████  | 230/285 [24:54<06:47,  7.40s/it] 81%|████████  | 231/285 [25:00<06:22,  7.08s/it] 81%|████████▏ | 232/285 [25:06<05:56,  6.73s/it] 82%|████████▏ | 233/285 [25:12<05:36,  6.47s/it] 82%|████████▏ | 234/285 [25:18<05:21,  6.30s/it] 82%|████████▏ | 235/285 [25:24<05:11,  6.24s/it] 83%|████████▎ | 236/285 [25:30<05:04,  6.20s/it] 83%|████████▎ | 237/285 [25:36<04:47,  6.00s/it] 84%|████████▎ | 238/285 [25:42<04:41,  5.99s/it] 84%|████████▍ | 239/285 [25:48<04:36,  6.01s/it] 84%|████████▍ | 240/285 [25:54<04:30,  6.02s/it] 85%|████████▍ | 241/285 [26:00<04:26,  6.05s/it] 85%|████████▍ | 242/285 [26:06<04:16,  5.96s/it] 85%|████████▌ | 243/285 [26:11<04:04,  5.83s/it] 86%|████████▌ | 244/285 [26:17<03:57,  5.78s/it] 86%|████████▌ | 245/285 [26:23<03:55,  5.88s/it] 86%|████████▋ | 246/285 [26:29<03:47,  5.84s/it] 87%|████████▋ | 247/285 [26:35<03:47,  5.98s/it] 87%|████████▋ | 248/285 [26:41<03:41,  5.99s/it] 87%|████████▋ | 249/285 [26:47<03:37,  6.04s/it] 88%|████████▊ | 250/285 [26:53<03:30,  6.02s/it] 88%|████████▊ | 251/285 [26:59<03:21,  5.94s/it] 88%|████████▊ | 252/285 [27:05<03:18,  6.03s/it] 89%|████████▉ | 253/285 [27:11<03:11,  5.98s/it] 89%|████████▉ | 254/285 [27:17<03:06,  6.02s/it] 89%|████████▉ | 255/285 [27:23<03:01,  6.04s/it] 90%|████████▉ | 256/285 [27:29<02:54,  6.02s/it] 90%|█████████ | 257/285 [27:35<02:48,  6.01s/it] 91%|█████████ | 258/285 [27:41<02:43,  6.04s/it] 91%|█████████ | 259/285 [27:47<02:37,  6.06s/it] 91%|█████████ | 260/285 [27:53<02:29,  6.00s/it] 92%|█████████▏| 261/285 [27:59<02:24,  6.04s/it] 92%|█████████▏| 262/285 [28:05<02:18,  6.03s/it] 92%|█████████▏| 263/285 [28:12<02:13,  6.07s/it] 93%|█████████▎| 264/285 [28:18<02:07,  6.07s/it] 93%|█████████▎| 265/285 [28:23<01:59,  5.98s/it] 93%|█████████▎| 266/285 [28:30<01:54,  6.01s/it] 94%|█████████▎| 267/285 [28:36<01:48,  6.05s/it] 94%|█████████▍| 268/285 [28:42<01:42,  6.00s/it] 94%|█████████▍| 269/285 [28:48<01:36,  6.03s/it] 95%|█████████▍| 270/285 [28:53<01:29,  5.96s/it] 95%|█████████▌| 271/285 [29:00<01:23,  5.99s/it] 95%|█████████▌| 272/285 [29:06<01:18,  6.00s/it] 96%|█████████▌| 273/285 [29:11<01:11,  5.98s/it] 96%|█████████▌| 274/285 [29:17<01:05,  5.98s/it] 96%|█████████▋| 275/285 [29:23<00:59,  5.98s/it] 97%|█████████▋| 276/285 [29:29<00:53,  5.99s/it] 97%|█████████▋| 277/285 [29:36<00:48,  6.03s/it] 98%|█████████▊| 278/285 [29:42<00:42,  6.05s/it] 98%|█████████▊| 279/285 [29:48<00:36,  6.09s/it] 98%|█████████▊| 280/285 [29:54<00:30,  6.17s/it] 99%|█████████▊| 281/285 [30:00<00:24,  6.15s/it] 99%|█████████▉| 282/285 [30:07<00:18,  6.18s/it] 99%|█████████▉| 283/285 [30:13<00:12,  6.21s/it]100%|█████████▉| 284/285 [30:19<00:06,  6.26s/it]100%|██████████| 285/285 [30:25<00:00,  6.20s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [30:25<00:00,  6.20s/it]100%|██████████| 285/285 [30:25<00:00,  6.41s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652359063
{'loss': 0.4603, 'learning_rate': 9.950000000000001e-06, 'epoch': 3.5}
{'train_runtime': 1825.7807, 'train_samples_per_second': 10.116, 'train_steps_per_second': 0.156, 'train_loss': 0.35049673046982077, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:08<39:13,  8.29s/it]  1%|          | 2/285 [00:16<38:36,  8.18s/it]  1%|          | 3/285 [00:24<39:20,  8.37s/it]  1%|▏         | 4/285 [00:33<39:41,  8.48s/it]  2%|▏         | 5/285 [00:42<39:50,  8.54s/it]  2%|▏         | 6/285 [00:50<39:45,  8.55s/it]  2%|▏         | 7/285 [00:59<39:02,  8.43s/it]  3%|▎         | 8/285 [01:07<38:17,  8.30s/it]  3%|▎         | 9/285 [01:15<37:48,  8.22s/it]  4%|▎         | 10/285 [01:23<38:15,  8.35s/it]  4%|▍         | 11/285 [01:32<38:02,  8.33s/it]  4%|▍         | 12/285 [01:40<38:42,  8.51s/it]  5%|▍         | 13/285 [01:49<38:29,  8.49s/it]  5%|▍         | 14/285 [01:57<38:09,  8.45s/it]  5%|▌         | 15/285 [02:05<37:17,  8.29s/it]  6%|▌         | 16/285 [02:13<36:50,  8.22s/it]  6%|▌         | 17/285 [02:22<37:24,  8.38s/it]  6%|▋         | 18/285 [02:31<37:31,  8.43s/it]  7%|▋         | 19/285 [02:39<37:38,  8.49s/it]  7%|▋         | 20/285 [02:48<37:42,  8.54s/it]  7%|▋         | 21/285 [02:56<37:29,  8.52s/it]  8%|▊         | 22/285 [03:05<37:33,  8.57s/it]  8%|▊         | 23/285 [03:13<36:51,  8.44s/it]  8%|▊         | 24/285 [03:21<36:28,  8.39s/it]  9%|▉         | 25/285 [03:30<36:15,  8.37s/it]  9%|▉         | 26/285 [03:38<36:11,  8.38s/it]  9%|▉         | 27/285 [03:47<36:09,  8.41s/it] 10%|▉         | 28/285 [03:55<36:08,  8.44s/it] 10%|█         | 29/285 [04:03<35:46,  8.38s/it] 11%|█         | 30/285 [04:12<35:41,  8.40s/it] 11%|█         | 31/285 [04:20<35:29,  8.38s/it] 11%|█         | 32/285 [04:29<36:07,  8.57s/it] 12%|█▏        | 33/285 [04:37<35:30,  8.46s/it] 12%|█▏        | 34/285 [04:45<34:56,  8.35s/it] 12%|█▏        | 35/285 [04:54<35:34,  8.54s/it] 13%|█▎        | 36/285 [05:03<35:31,  8.56s/it] 13%|█▎        | 37/285 [05:12<36:10,  8.75s/it] 13%|█▎        | 38/285 [05:21<35:31,  8.63s/it] 14%|█▎        | 39/285 [05:29<35:16,  8.61s/it] 14%|█▍        | 40/285 [05:38<35:30,  8.70s/it] 14%|█▍        | 41/285 [05:46<34:46,  8.55s/it] 15%|█▍        | 42/285 [05:55<34:36,  8.55s/it] 15%|█▌        | 43/285 [06:03<34:40,  8.60s/it] 15%|█▌        | 44/285 [06:12<34:13,  8.52s/it] 16%|█▌        | 45/285 [06:20<33:47,  8.45s/it] 16%|█▌        | 46/285 [06:29<34:41,  8.71s/it] 16%|█▋        | 47/285 [06:38<34:07,  8.60s/it] 17%|█▋        | 48/285 [06:47<34:33,  8.75s/it] 17%|█▋        | 49/285 [06:55<34:18,  8.72s/it] 18%|█▊        | 50/285 [07:04<34:17,  8.75s/it] 18%|█▊        | 51/285 [07:13<34:31,  8.85s/it] 18%|█▊        | 52/285 [07:22<33:51,  8.72s/it] 19%|█▊        | 53/285 [07:30<33:31,  8.67s/it] 19%|█▉        | 54/285 [07:39<33:20,  8.66s/it] 19%|█▉        | 55/285 [07:48<33:37,  8.77s/it] 20%|█▉        | 56/285 [07:57<33:15,  8.72s/it] 20%|██        | 57/285 [08:05<32:54,  8.66s/it] 20%|██        | 58/285 [08:20<40:05, 10.60s/it] 21%|██        | 59/285 [08:29<37:52, 10.06s/it] 21%|██        | 60/285 [08:38<36:09,  9.64s/it] 21%|██▏       | 61/285 [08:46<34:46,  9.32s/it] 22%|██▏       | 62/285 [08:54<33:13,  8.94s/it] 22%|██▏       | 63/285 [09:03<32:49,  8.87s/it] 22%|██▏       | 64/285 [09:12<32:39,  8.86s/it] 23%|██▎       | 65/285 [09:21<32:18,  8.81s/it] 23%|██▎       | 66/285 [09:29<31:32,  8.64s/it] 24%|██▎       | 67/285 [09:37<31:22,  8.63s/it] 24%|██▍       | 68/285 [09:46<31:33,  8.73s/it] 24%|██▍       | 69/285 [09:55<30:45,  8.54s/it] 25%|██▍       | 70/285 [10:03<30:38,  8.55s/it] 25%|██▍       | 71/285 [10:12<30:42,  8.61s/it] 25%|██▌       | 72/285 [10:21<30:37,  8.63s/it] 26%|██▌       | 73/285 [10:29<30:02,  8.50s/it] 26%|██▌       | 74/285 [10:37<29:43,  8.45s/it] 26%|██▋       | 75/285 [10:46<29:48,  8.52s/it] 27%|██▋       | 76/285 [10:55<30:08,  8.65s/it] 27%|██▋       | 77/285 [11:03<29:36,  8.54s/it] 27%|██▋       | 78/285 [11:11<29:26,  8.53s/it] 28%|██▊       | 79/285 [11:20<29:45,  8.67s/it] 28%|██▊       | 80/285 [11:29<29:43,  8.70s/it] 28%|██▊       | 81/285 [11:38<29:15,  8.61s/it] 29%|██▉       | 82/285 [11:46<29:14,  8.64s/it] 29%|██▉       | 83/285 [11:55<28:59,  8.61s/it] 29%|██▉       | 84/285 [12:04<28:54,  8.63s/it] 30%|██▉       | 85/285 [12:12<28:17,  8.49s/it] 30%|███       | 86/285 [12:21<28:27,  8.58s/it] 31%|███       | 87/285 [12:29<28:06,  8.52s/it] 31%|███       | 88/285 [12:38<28:12,  8.59s/it] 31%|███       | 89/285 [12:46<28:04,  8.60s/it] 32%|███▏      | 90/285 [12:55<28:10,  8.67s/it] 32%|███▏      | 91/285 [13:04<28:01,  8.67s/it] 32%|███▏      | 92/285 [13:12<27:28,  8.54s/it] 33%|███▎      | 93/285 [13:21<27:19,  8.54s/it] 33%|███▎      | 94/285 [13:29<27:24,  8.61s/it] 33%|███▎      | 95/285 [13:38<26:52,  8.49s/it] 34%|███▎      | 96/285 [13:46<26:36,  8.45s/it] 34%|███▍      | 97/285 [13:54<26:30,  8.46s/it] 34%|███▍      | 98/285 [14:03<26:08,  8.39s/it] 35%|███▍      | 99/285 [14:11<25:34,  8.25s/it] 35%|███▌      | 100/285 [14:19<25:32,  8.29s/it] 35%|███▌      | 101/285 [14:28<26:04,  8.50s/it] 36%|███▌      | 102/285 [14:36<25:37,  8.40s/it] 36%|███▌      | 103/285 [14:44<25:21,  8.36s/it] 36%|███▋      | 104/285 [14:53<25:25,  8.43s/it] 37%|███▋      | 105/285 [15:01<25:15,  8.42s/it] 37%|███▋      | 106/285 [15:10<25:32,  8.56s/it] 38%|███▊      | 107/285 [15:19<25:27,  8.58s/it] 38%|███▊      | 108/285 [15:27<25:17,  8.57s/it] 38%|███▊      | 109/285 [15:36<25:00,  8.52s/it] 39%|███▊      | 110/285 [15:45<25:06,  8.61s/it] 39%|███▉      | 111/285 [15:54<25:16,  8.72s/it] 39%|███▉      | 112/285 [16:02<25:16,  8.77s/it] 40%|███▉      | 113/285 [16:11<25:01,  8.73s/it] 40%|████      | 114/285 [16:20<25:05,  8.80s/it] 40%|████      | 115/285 [16:35<30:10, 10.65s/it] 41%|████      | 116/285 [16:44<28:17, 10.04s/it] 41%|████      | 117/285 [16:52<26:53,  9.60s/it] 41%|████▏     | 118/285 [17:00<25:36,  9.20s/it] 42%|████▏     | 119/285 [17:09<24:40,  8.92s/it] 42%|████▏     | 120/285 [17:17<24:15,  8.82s/it] 42%|████▏     | 121/285 [17:25<23:30,  8.60s/it] 43%|████▎     | 122/285 [17:34<23:00,  8.47s/it] 43%|████▎     | 123/285 [17:42<23:00,  8.52s/it] 44%|████▎     | 124/285 [17:50<22:37,  8.43s/it] 44%|████▍     | 125/285 [17:59<22:32,  8.45s/it] 44%|████▍     | 126/285 [18:08<22:45,  8.59s/it] 45%|████▍     | 127/285 [18:17<22:43,  8.63s/it] 45%|████▍     | 128/285 [18:25<22:24,  8.56s/it] 45%|████▌     | 129/285 [18:33<22:03,  8.48s/it] 46%|████▌     | 130/285 [18:42<21:46,  8.43s/it] 46%|████▌     | 131/285 [18:51<22:03,  8.59s/it] 46%|████▋     | 132/285 [18:59<22:02,  8.64s/it] 47%|████▋     | 133/285 [19:08<21:49,  8.62s/it] 47%|████▋     | 134/285 [19:17<22:01,  8.75s/it] 47%|████▋     | 135/285 [19:25<21:31,  8.61s/it] 48%|████▊     | 136/285 [19:34<21:12,  8.54s/it] 48%|████▊     | 137/285 [19:42<21:11,  8.59s/it] 48%|████▊     | 138/285 [19:51<21:09,  8.63s/it] 49%|████▉     | 139/285 [19:59<20:49,  8.56s/it] 49%|████▉     | 140/285 [20:08<20:41,  8.56s/it] 49%|████▉     | 141/285 [20:17<20:41,  8.62s/it] 50%|████▉     | 142/285 [20:25<20:34,  8.63s/it] 50%|█████     | 143/285 [20:34<20:08,  8.51s/it] 51%|█████     | 144/285 [20:43<20:29,  8.72s/it] 51%|█████     | 145/285 [20:52<20:39,  8.86s/it] 51%|█████     | 146/285 [21:01<20:23,  8.81s/it] 52%|█████▏    | 147/285 [21:09<19:57,  8.68s/it] 52%|█████▏    | 148/285 [21:18<19:49,  8.68s/it] 52%|█████▏    | 149/285 [21:27<19:56,  8.80s/it] 53%|█████▎    | 150/285 [21:36<19:54,  8.85s/it] 53%|█████▎    | 151/285 [21:44<19:19,  8.65s/it] 53%|█████▎    | 152/285 [21:52<19:02,  8.59s/it] 54%|█████▎    | 153/285 [22:01<18:41,  8.50s/it] 54%|█████▍    | 154/285 [22:10<18:43,  8.58s/it] 54%|█████▍    | 155/285 [22:18<18:38,  8.61s/it] 55%|█████▍    | 156/285 [22:27<18:31,  8.61s/it] 55%|█████▌    | 157/285 [22:35<18:21,  8.60s/it] 55%|█████▌    | 158/285 [22:44<18:04,  8.54s/it] 56%|█████▌    | 159/285 [22:53<18:09,  8.65s/it] 56%|█████▌    | 160/285 [23:01<17:50,  8.56s/it] 56%|█████▋    | 161/285 [23:10<17:54,  8.66s/it] 57%|█████▋    | 162/285 [23:18<17:35,  8.58s/it] 57%|█████▋    | 163/285 [23:27<17:20,  8.52s/it] 58%|█████▊    | 164/285 [23:35<16:54,  8.39s/it] 58%|█████▊    | 165/285 [23:44<17:22,  8.69s/it] 58%|█████▊    | 166/285 [23:52<17:00,  8.58s/it] 59%|█████▊    | 167/285 [24:01<16:52,  8.58s/it] 59%|█████▉    | 168/285 [24:09<16:31,  8.47s/it] 59%|█████▉    | 169/285 [24:18<16:17,  8.43s/it] 60%|█████▉    | 170/285 [24:26<16:13,  8.47s/it] 60%|██████    | 171/285 [24:35<16:00,  8.42s/it] 60%|██████    | 172/285 [24:48<18:45,  9.96s/it] 61%|██████    | 173/285 [24:56<17:32,  9.40s/it] 61%|██████    | 174/285 [25:05<16:49,  9.09s/it] 61%|██████▏   | 175/285 [25:13<16:10,  8.83s/it] 62%|██████▏   | 176/285 [25:21<15:35,  8.58s/it] 62%|██████▏   | 177/285 [25:29<15:29,  8.61s/it] 62%|██████▏   | 178/285 [25:38<15:12,  8.53s/it] 63%|██████▎   | 179/285 [25:46<15:05,  8.54s/it] 63%|██████▎   | 180/285 [25:55<14:52,  8.50s/it] 64%|██████▎   | 181/285 [26:04<15:07,  8.73s/it] 64%|██████▍   | 182/285 [26:13<14:52,  8.66s/it] 64%|██████▍   | 183/285 [26:22<14:54,  8.77s/it] 65%|██████▍   | 184/285 [26:30<14:44,  8.75s/it] 65%|██████▍   | 185/285 [26:40<14:53,  8.94s/it] 65%|██████▌   | 186/285 [26:49<14:50,  8.99s/it] 66%|██████▌   | 187/285 [26:58<14:43,  9.02s/it] 66%|██████▌   | 188/285 [27:07<14:32,  8.99s/it] 66%|██████▋   | 189/285 [27:16<14:21,  8.97s/it] 67%|██████▋   | 190/285 [27:24<13:59,  8.83s/it] 67%|██████▋   | 191/285 [27:33<13:53,  8.86s/it] 67%|██████▋   | 192/285 [27:42<13:48,  8.91s/it] 68%|██████▊   | 193/285 [27:51<13:45,  8.98s/it] 68%|██████▊   | 194/285 [28:00<13:24,  8.84s/it] 68%|██████▊   | 195/285 [28:09<13:20,  8.89s/it] 69%|██████▉   | 196/285 [28:17<12:57,  8.73s/it] 69%|██████▉   | 197/285 [28:26<12:49,  8.74s/it] 69%|██████▉   | 198/285 [28:34<12:35,  8.68s/it] 70%|██████▉   | 199/285 [28:43<12:17,  8.58s/it] 70%|███████   | 200/285 [28:52<12:24,  8.76s/it]                                                  70%|███████   | 200/285 [28:52<12:24,  8.76s/it] 71%|███████   | 201/285 [29:01<12:21,  8.83s/it] 71%|███████   | 202/285 [29:09<12:02,  8.70s/it] 71%|███████   | 203/285 [29:18<11:58,  8.76s/it] 72%|███████▏  | 204/285 [29:27<11:57,  8.85s/it] 72%|███████▏  | 205/285 [29:36<11:48,  8.85s/it] 72%|███████▏  | 206/285 [29:45<11:27,  8.70s/it] 73%|███████▎  | 207/285 [29:53<11:18,  8.70s/it] 73%|███████▎  | 208/285 [30:01<10:56,  8.52s/it] 73%|███████▎  | 209/285 [30:10<10:43,  8.46s/it] 74%|███████▎  | 210/285 [30:18<10:38,  8.52s/it] 74%|███████▍  | 211/285 [30:27<10:32,  8.55s/it] 74%|███████▍  | 212/285 [30:35<10:19,  8.48s/it] 75%|███████▍  | 213/285 [30:44<10:18,  8.59s/it] 75%|███████▌  | 214/285 [30:53<10:07,  8.55s/it] 75%|███████▌  | 215/285 [31:01<09:58,  8.55s/it] 76%|███████▌  | 216/285 [31:10<09:49,  8.55s/it] 76%|███████▌  | 217/285 [31:18<09:41,  8.55s/it] 76%|███████▋  | 218/285 [31:27<09:30,  8.51s/it] 77%|███████▋  | 219/285 [31:35<09:19,  8.47s/it] 77%|███████▋  | 220/285 [31:44<09:20,  8.62s/it] 78%|███████▊  | 221/285 [31:53<09:19,  8.75s/it] 78%|███████▊  | 222/285 [32:01<09:02,  8.61s/it] 78%|███████▊  | 223/285 [32:10<08:53,  8.61s/it] 79%|███████▊  | 224/285 [32:19<08:51,  8.72s/it] 79%|███████▉  | 225/285 [32:27<08:37,  8.63s/it] 79%|███████▉  | 226/285 [32:37<08:40,  8.82s/it] 80%|███████▉  | 227/285 [32:45<08:29,  8.79s/it] 80%|████████  | 228/285 [32:54<08:17,  8.73s/it] 80%|████████  | 229/285 [33:09<09:50, 10.54s/it] 81%|████████  | 230/285 [33:18<09:12, 10.04s/it] 81%|████████  | 231/285 [33:26<08:41,  9.67s/it] 81%|████████▏ | 232/285 [33:35<08:09,  9.24s/it] 82%|████████▏ | 233/285 [33:43<07:46,  8.98s/it] 82%|████████▏ | 234/285 [33:51<07:30,  8.83s/it] 82%|████████▏ | 235/285 [34:00<07:16,  8.73s/it] 83%|████████▎ | 236/285 [34:08<07:03,  8.65s/it] 83%|████████▎ | 237/285 [34:17<06:48,  8.52s/it] 84%|████████▎ | 238/285 [34:25<06:44,  8.61s/it] 84%|████████▍ | 239/285 [34:34<06:34,  8.59s/it] 84%|████████▍ | 240/285 [34:43<06:28,  8.64s/it] 85%|████████▍ | 241/285 [34:51<06:21,  8.67s/it] 85%|████████▍ | 242/285 [35:00<06:10,  8.61s/it] 85%|████████▌ | 243/285 [35:08<05:56,  8.49s/it] 86%|████████▌ | 244/285 [35:16<05:44,  8.40s/it] 86%|████████▌ | 245/285 [35:25<05:41,  8.53s/it] 86%|████████▋ | 246/285 [35:33<05:28,  8.42s/it] 87%|████████▋ | 247/285 [35:42<05:26,  8.59s/it] 87%|████████▋ | 248/285 [35:51<05:17,  8.58s/it] 87%|████████▋ | 249/285 [36:00<05:13,  8.72s/it] 88%|████████▊ | 250/285 [36:09<05:05,  8.72s/it] 88%|████████▊ | 251/285 [36:17<04:51,  8.57s/it] 88%|████████▊ | 252/285 [36:26<04:46,  8.70s/it] 89%|████████▉ | 253/285 [36:34<04:35,  8.62s/it] 89%|████████▉ | 254/285 [36:43<04:30,  8.72s/it] 89%|████████▉ | 255/285 [36:52<04:22,  8.76s/it] 90%|████████▉ | 256/285 [37:01<04:11,  8.69s/it] 90%|█████████ | 257/285 [37:09<04:02,  8.65s/it] 91%|█████████ | 258/285 [37:18<03:57,  8.79s/it] 91%|█████████ | 259/285 [37:27<03:51,  8.91s/it] 91%|█████████ | 260/285 [37:36<03:42,  8.91s/it] 92%|█████████▏| 261/285 [37:46<03:36,  9.03s/it] 92%|█████████▏| 262/285 [37:55<03:27,  9.03s/it] 92%|█████████▏| 263/285 [38:04<03:20,  9.12s/it] 93%|█████████▎| 264/285 [38:13<03:09,  9.05s/it] 93%|█████████▎| 265/285 [38:21<02:58,  8.90s/it] 93%|█████████▎| 266/285 [38:30<02:49,  8.92s/it] 94%|█████████▎| 267/285 [38:39<02:41,  8.95s/it] 94%|█████████▍| 268/285 [38:48<02:32,  8.95s/it] 94%|█████████▍| 269/285 [38:58<02:24,  9.01s/it] 95%|█████████▍| 270/285 [39:06<02:13,  8.89s/it] 95%|█████████▌| 271/285 [39:15<02:04,  8.91s/it] 95%|█████████▌| 272/285 [39:24<01:55,  8.89s/it] 96%|█████████▌| 273/285 [39:33<01:46,  8.89s/it] 96%|█████████▌| 274/285 [39:42<01:37,  8.87s/it] 96%|█████████▋| 275/285 [39:50<01:27,  8.75s/it] 97%|█████████▋| 276/285 [39:59<01:18,  8.73s/it] 97%|█████████▋| 277/285 [40:08<01:10,  8.80s/it] 98%|█████████▊| 278/285 [40:17<01:01,  8.78s/it] 98%|█████████▊| 279/285 [40:26<00:53,  8.90s/it] 98%|█████████▊| 280/285 [40:35<00:44,  8.97s/it] 99%|█████████▊| 281/285 [40:44<00:35,  8.91s/it] 99%|█████████▉| 282/285 [40:53<00:26,  8.96s/it] 99%|█████████▉| 283/285 [41:02<00:18,  9.03s/it]100%|█████████▉| 284/285 [41:11<00:09,  9.05s/it]100%|██████████| 285/285 [41:19<00:00,  8.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [41:19<00:00,  8.82s/it]100%|██████████| 285/285 [41:19<00:00,  8.70s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3694
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652356503
{'loss': 0.2753, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 2479.7472, 'train_samples_per_second': 7.448, 'train_steps_per_second': 0.115, 'train_loss': 0.19999463223574454, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_imbalanced_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:09<45:43,  9.66s/it]  1%|          | 2/285 [00:19<45:39,  9.68s/it]  1%|          | 3/285 [00:29<45:49,  9.75s/it]  1%|▏         | 4/285 [00:39<45:52,  9.80s/it]  2%|▏         | 5/285 [00:49<45:58,  9.85s/it]  2%|▏         | 6/285 [00:59<46:16,  9.95s/it]  2%|▏         | 7/285 [01:08<45:40,  9.86s/it]  3%|▎         | 8/285 [01:18<44:36,  9.66s/it]  3%|▎         | 9/285 [01:27<44:12,  9.61s/it]  4%|▎         | 10/285 [01:37<44:17,  9.66s/it]  4%|▍         | 11/285 [01:46<44:04,  9.65s/it]  4%|▍         | 12/285 [01:57<44:34,  9.80s/it]  5%|▍         | 13/285 [02:06<44:03,  9.72s/it]  5%|▍         | 14/285 [02:16<43:36,  9.66s/it]  5%|▌         | 15/285 [02:25<42:56,  9.54s/it]  6%|▌         | 16/285 [02:34<42:44,  9.53s/it]  6%|▌         | 17/285 [02:45<43:42,  9.78s/it]  6%|▋         | 18/285 [02:55<43:47,  9.84s/it]  7%|▋         | 19/285 [03:05<43:34,  9.83s/it]  7%|▋         | 20/285 [03:15<43:35,  9.87s/it]  7%|▋         | 21/285 [03:24<43:02,  9.78s/it]  8%|▊         | 22/285 [03:34<43:10,  9.85s/it]  8%|▊         | 23/285 [03:44<42:30,  9.74s/it]  8%|▊         | 24/285 [03:53<41:52,  9.63s/it]  9%|▉         | 25/285 [04:02<41:23,  9.55s/it]  9%|▉         | 26/285 [04:12<41:47,  9.68s/it]  9%|▉         | 27/285 [04:22<41:40,  9.69s/it] 10%|▉         | 28/285 [04:32<41:33,  9.70s/it] 10%|█         | 29/285 [04:41<41:16,  9.67s/it] 11%|█         | 30/285 [04:51<41:06,  9.67s/it] 11%|█         | 31/285 [05:01<41:08,  9.72s/it] 11%|█         | 32/285 [05:11<41:50,  9.92s/it] 12%|█▏        | 33/285 [05:21<41:20,  9.84s/it] 12%|█▏        | 34/285 [05:30<40:09,  9.60s/it] 12%|█▏        | 35/285 [05:40<40:08,  9.63s/it] 13%|█▎        | 36/285 [05:49<39:16,  9.46s/it] 13%|█▎        | 37/285 [05:58<39:20,  9.52s/it] 13%|█▎        | 38/285 [06:07<38:34,  9.37s/it] 14%|█▎        | 39/285 [06:16<38:01,  9.28s/it] 14%|█▍        | 40/285 [06:26<38:14,  9.36s/it] 14%|█▍        | 41/285 [06:36<38:25,  9.45s/it] 15%|█▍        | 42/285 [06:45<38:34,  9.52s/it] 15%|█▌        | 43/285 [06:56<39:11,  9.72s/it] 15%|█▌        | 44/285 [07:05<38:51,  9.67s/it] 16%|█▌        | 45/285 [07:15<38:59,  9.75s/it] 16%|█▌        | 46/285 [07:25<39:26,  9.90s/it] 16%|█▋        | 47/285 [07:35<38:51,  9.80s/it] 17%|█▋        | 48/285 [07:45<38:50,  9.83s/it] 17%|█▋        | 49/285 [07:54<38:21,  9.75s/it] 18%|█▊        | 50/285 [08:04<38:17,  9.78s/it] 18%|█▊        | 51/285 [08:14<38:30,  9.87s/it] 18%|█▊        | 52/285 [08:24<38:00,  9.79s/it] 19%|█▊        | 53/285 [08:33<37:29,  9.70s/it] 19%|█▉        | 54/285 [08:43<36:52,  9.58s/it] 19%|█▉        | 55/285 [08:52<36:37,  9.56s/it] 20%|█▉        | 56/285 [09:01<36:04,  9.45s/it] 20%|██        | 57/285 [09:11<35:41,  9.39s/it] 20%|██        | 58/285 [09:27<43:39, 11.54s/it] 21%|██        | 59/285 [09:37<41:11, 10.93s/it] 21%|██        | 60/285 [09:46<39:17, 10.48s/it] 21%|██▏       | 61/285 [09:55<37:53, 10.15s/it] 22%|██▏       | 62/285 [10:04<36:12,  9.74s/it] 22%|██▏       | 63/285 [10:14<35:48,  9.68s/it] 22%|██▏       | 64/285 [10:23<35:37,  9.67s/it] 23%|██▎       | 65/285 [10:33<35:20,  9.64s/it] 23%|██▎       | 66/285 [10:42<34:40,  9.50s/it] 24%|██▎       | 67/285 [10:52<34:30,  9.50s/it] 24%|██▍       | 68/285 [11:01<34:34,  9.56s/it] 24%|██▍       | 69/285 [11:10<33:18,  9.25s/it] 25%|██▍       | 70/285 [11:19<32:48,  9.16s/it] 25%|██▍       | 71/285 [11:28<32:50,  9.21s/it] 25%|██▌       | 72/285 [11:37<32:41,  9.21s/it] 26%|██▌       | 73/285 [11:46<31:58,  9.05s/it] 26%|██▌       | 74/285 [11:55<31:39,  9.00s/it] 26%|██▋       | 75/285 [12:04<31:48,  9.09s/it] 27%|██▋       | 76/285 [12:14<32:12,  9.25s/it] 27%|██▋       | 77/285 [12:23<31:52,  9.19s/it] 27%|██▋       | 78/285 [12:32<31:43,  9.19s/it] 28%|██▊       | 79/285 [12:42<32:01,  9.33s/it] 28%|██▊       | 80/285 [12:51<32:00,  9.37s/it] 28%|██▊       | 81/285 [13:00<31:31,  9.27s/it] 29%|██▉       | 82/285 [13:10<31:23,  9.28s/it] 29%|██▉       | 83/285 [13:19<31:07,  9.25s/it] 29%|██▉       | 84/285 [13:28<31:01,  9.26s/it] 30%|██▉       | 85/285 [13:37<30:21,  9.11s/it] 30%|███       | 86/285 [13:46<30:32,  9.21s/it] 31%|███       | 87/285 [13:55<30:08,  9.13s/it] 31%|███       | 88/285 [14:05<30:12,  9.20s/it] 31%|███       | 89/285 [14:14<30:03,  9.20s/it] 32%|███▏      | 90/285 [14:23<30:10,  9.28s/it] 32%|███▏      | 91/285 [14:32<29:58,  9.27s/it] 32%|███▏      | 92/285 [14:41<29:17,  9.11s/it] 33%|███▎      | 93/285 [14:50<29:05,  9.09s/it] 33%|███▎      | 94/285 [15:00<29:08,  9.15s/it] 33%|███▎      | 95/285 [15:09<28:47,  9.09s/it] 34%|███▎      | 96/285 [15:18<28:34,  9.07s/it] 34%|███▍      | 97/285 [15:27<28:34,  9.12s/it] 34%|███▍      | 98/285 [15:36<28:17,  9.08s/it] 35%|███▍      | 99/285 [15:45<27:52,  8.99s/it] 35%|███▌      | 100/285 [15:54<27:44,  9.00s/it] 35%|███▌      | 101/285 [16:03<28:13,  9.20s/it] 36%|███▌      | 102/285 [16:12<27:53,  9.14s/it] 36%|███▌      | 103/285 [16:21<27:38,  9.11s/it] 36%|███▋      | 104/285 [16:31<27:41,  9.18s/it] 37%|███▋      | 105/285 [16:40<27:29,  9.17s/it] 37%|███▋      | 106/285 [16:49<27:46,  9.31s/it] 38%|███▊      | 107/285 [16:59<27:39,  9.33s/it] 38%|███▊      | 108/285 [17:08<27:27,  9.31s/it] 38%|███▊      | 109/285 [17:18<27:43,  9.45s/it] 39%|███▊      | 110/285 [17:28<27:57,  9.59s/it] 39%|███▉      | 111/285 [17:38<28:15,  9.75s/it] 39%|███▉      | 112/285 [17:48<28:26,  9.87s/it] 40%|███▉      | 113/285 [17:58<28:04,  9.79s/it] 40%|████      | 114/285 [18:07<27:43,  9.73s/it] 40%|████      | 115/285 [18:23<33:07, 11.69s/it] 41%|████      | 116/285 [18:33<30:46, 10.93s/it] 41%|████      | 117/285 [18:42<29:09, 10.41s/it] 41%|████▏     | 118/285 [18:51<27:48,  9.99s/it] 42%|████▏     | 119/285 [19:00<26:55,  9.73s/it] 42%|████▏     | 120/285 [19:09<26:27,  9.62s/it] 42%|████▏     | 121/285 [19:18<25:40,  9.39s/it] 43%|████▎     | 122/285 [19:27<25:11,  9.27s/it] 43%|████▎     | 123/285 [19:37<25:09,  9.32s/it] 44%|████▎     | 124/285 [19:46<24:46,  9.24s/it] 44%|████▍     | 125/285 [19:55<24:47,  9.30s/it] 44%|████▍     | 126/285 [20:05<24:51,  9.38s/it] 45%|████▍     | 127/285 [20:14<24:46,  9.41s/it] 45%|████▍     | 128/285 [20:23<24:35,  9.40s/it] 45%|████▌     | 129/285 [20:33<24:10,  9.30s/it] 46%|████▌     | 130/285 [20:42<23:53,  9.25s/it] 46%|████▌     | 131/285 [20:51<24:01,  9.36s/it] 46%|████▋     | 132/285 [21:01<23:51,  9.36s/it] 47%|████▋     | 133/285 [21:10<24:00,  9.48s/it] 47%|████▋     | 134/285 [21:20<23:59,  9.54s/it] 47%|████▋     | 135/285 [21:29<23:20,  9.34s/it] 48%|████▊     | 136/285 [21:38<22:55,  9.23s/it] 48%|████▊     | 137/285 [21:47<22:48,  9.25s/it] 48%|████▊     | 138/285 [21:57<22:46,  9.30s/it] 49%|████▉     | 139/285 [22:06<22:23,  9.20s/it] 49%|████▉     | 140/285 [22:15<22:14,  9.20s/it] 49%|████▉     | 141/285 [22:24<22:14,  9.27s/it] 50%|████▉     | 142/285 [22:33<22:04,  9.26s/it] 50%|█████     | 143/285 [22:42<21:30,  9.09s/it] 51%|█████     | 144/285 [22:52<21:45,  9.26s/it] 51%|█████     | 145/285 [23:01<21:50,  9.36s/it] 51%|█████     | 146/285 [23:10<21:23,  9.24s/it] 52%|█████▏    | 147/285 [23:19<21:08,  9.19s/it] 52%|█████▏    | 148/285 [23:29<21:00,  9.20s/it] 52%|█████▏    | 149/285 [23:38<21:12,  9.36s/it] 53%|█████▎    | 150/285 [23:48<21:15,  9.45s/it] 53%|█████▎    | 151/285 [23:57<20:41,  9.26s/it] 53%|█████▎    | 152/285 [24:06<20:24,  9.21s/it] 54%|█████▎    | 153/285 [24:15<20:11,  9.18s/it] 54%|█████▍    | 154/285 [24:25<20:20,  9.32s/it] 54%|█████▍    | 155/285 [24:34<20:12,  9.32s/it] 55%|█████▍    | 156/285 [24:43<20:06,  9.35s/it] 55%|█████▌    | 157/285 [24:53<19:54,  9.33s/it] 55%|█████▌    | 158/285 [25:02<19:38,  9.28s/it] 56%|█████▌    | 159/285 [25:12<19:47,  9.43s/it] 56%|█████▌    | 160/285 [25:21<19:21,  9.29s/it] 56%|█████▋    | 161/285 [25:30<19:24,  9.39s/it] 57%|█████▋    | 162/285 [25:39<19:03,  9.29s/it] 57%|█████▋    | 163/285 [25:48<18:45,  9.22s/it] 58%|█████▊    | 164/285 [25:57<18:19,  9.08s/it] 58%|█████▊    | 165/285 [26:07<18:46,  9.39s/it] 58%|█████▊    | 166/285 [26:16<18:22,  9.27s/it] 59%|█████▊    | 167/285 [26:26<18:26,  9.38s/it] 59%|█████▉    | 168/285 [26:35<18:08,  9.30s/it] 59%|█████▉    | 169/285 [26:44<17:48,  9.21s/it] 60%|█████▉    | 170/285 [26:53<17:40,  9.22s/it] 60%|██████    | 171/285 [27:02<17:27,  9.19s/it] 60%|██████    | 172/285 [27:17<20:35, 10.93s/it] 61%|██████    | 173/285 [27:26<19:22, 10.38s/it] 61%|██████    | 174/285 [27:36<18:36, 10.06s/it] 61%|██████▏   | 175/285 [27:45<17:55,  9.78s/it] 62%|██████▏   | 176/285 [27:54<17:20,  9.55s/it] 62%|██████▏   | 177/285 [28:04<17:31,  9.74s/it] 62%|██████▏   | 178/285 [28:14<17:23,  9.75s/it] 63%|██████▎   | 179/285 [28:24<17:18,  9.79s/it] 63%|██████▎   | 180/285 [28:34<17:12,  9.83s/it] 64%|██████▎   | 181/285 [28:44<17:30, 10.10s/it] 64%|██████▍   | 182/285 [28:54<17:06,  9.96s/it] 64%|██████▍   | 183/285 [29:04<16:48,  9.89s/it] 65%|██████▍   | 184/285 [29:13<16:26,  9.77s/it] 65%|██████▍   | 185/285 [29:24<16:33,  9.94s/it] 65%|██████▌   | 186/285 [29:34<16:28,  9.99s/it] 66%|██████▌   | 187/285 [29:44<16:17,  9.97s/it] 66%|██████▌   | 188/285 [29:54<16:08,  9.98s/it] 66%|██████▋   | 189/285 [30:04<16:03, 10.04s/it] 67%|██████▋   | 190/285 [30:14<15:48,  9.99s/it] 67%|██████▋   | 191/285 [30:24<15:42, 10.03s/it] 67%|██████▋   | 192/285 [30:34<15:32, 10.03s/it] 68%|██████▊   | 193/285 [30:44<15:20, 10.01s/it] 68%|██████▊   | 194/285 [30:53<15:00,  9.90s/it] 68%|██████▊   | 195/285 [31:04<15:04, 10.05s/it] 69%|██████▉   | 196/285 [31:13<14:42,  9.92s/it] 69%|██████▉   | 197/285 [31:23<14:36,  9.96s/it] 69%|██████▉   | 198/285 [31:33<14:18,  9.87s/it] 70%|██████▉   | 199/285 [31:43<14:02,  9.80s/it] 70%|███████   | 200/285 [31:53<14:06,  9.96s/it]                                                  70%|███████   | 200/285 [31:53<14:06,  9.96s/it] 71%|███████   | 201/285 [32:03<14:01, 10.02s/it] 71%|███████   | 202/285 [32:13<13:41,  9.89s/it] 71%|███████   | 203/285 [32:23<13:37,  9.97s/it] 72%|███████▏  | 204/285 [32:33<13:33, 10.05s/it] 72%|███████▏  | 205/285 [32:43<13:23, 10.04s/it] 72%|███████▏  | 206/285 [32:53<13:01,  9.89s/it] 73%|███████▎  | 207/285 [33:03<12:54,  9.93s/it] 73%|███████▎  | 208/285 [33:12<12:36,  9.82s/it] 73%|███████▎  | 209/285 [33:22<12:29,  9.86s/it] 74%|███████▎  | 210/285 [33:32<12:17,  9.83s/it] 74%|███████▍  | 211/285 [33:42<12:07,  9.83s/it] 74%|███████▍  | 212/285 [33:51<11:50,  9.74s/it] 75%|███████▍  | 213/285 [34:02<11:51,  9.89s/it] 75%|███████▌  | 214/285 [34:11<11:35,  9.79s/it] 75%|███████▌  | 215/285 [34:21<11:30,  9.86s/it] 76%|███████▌  | 216/285 [34:31<11:18,  9.84s/it] 76%|███████▌  | 217/285 [34:41<11:14,  9.91s/it] 76%|███████▋  | 218/285 [34:51<10:56,  9.81s/it] 77%|███████▋  | 219/285 [35:00<10:42,  9.74s/it] 77%|███████▋  | 220/285 [35:11<10:44,  9.92s/it] 78%|███████▊  | 221/285 [35:21<10:42, 10.03s/it] 78%|███████▊  | 222/285 [35:31<10:24,  9.92s/it] 78%|███████▊  | 223/285 [35:41<10:17,  9.96s/it] 79%|███████▊  | 224/285 [35:51<10:13, 10.05s/it] 79%|███████▉  | 225/285 [36:01<09:59,  9.99s/it] 79%|███████▉  | 226/285 [36:11<09:50, 10.01s/it] 80%|███████▉  | 227/285 [36:21<09:38,  9.98s/it] 80%|████████  | 228/285 [36:31<09:25,  9.92s/it] 80%|████████  | 229/285 [36:47<11:13, 12.03s/it] 81%|████████  | 230/285 [36:58<10:30, 11.46s/it] 81%|████████  | 231/285 [37:08<09:56, 11.05s/it] 81%|████████▏ | 232/285 [37:17<09:20, 10.58s/it] 82%|████████▏ | 233/285 [37:27<08:52, 10.24s/it] 82%|████████▏ | 234/285 [37:36<08:31, 10.03s/it] 82%|████████▏ | 235/285 [37:46<08:14,  9.89s/it] 83%|████████▎ | 236/285 [37:55<07:59,  9.78s/it] 83%|████████▎ | 237/285 [38:04<07:40,  9.59s/it] 84%|████████▎ | 238/285 [38:14<07:35,  9.69s/it] 84%|████████▍ | 239/285 [38:24<07:25,  9.68s/it] 84%|████████▍ | 240/285 [38:34<07:23,  9.86s/it] 85%|████████▍ | 241/285 [38:45<07:21, 10.04s/it] 85%|████████▍ | 242/285 [38:55<07:13, 10.08s/it] 85%|████████▌ | 243/285 [39:05<07:03, 10.07s/it] 86%|████████▌ | 244/285 [39:15<06:47,  9.94s/it] 86%|████████▌ | 245/285 [39:25<06:45, 10.13s/it] 86%|████████▋ | 246/285 [39:35<06:32, 10.06s/it] 87%|████████▋ | 247/285 [39:45<06:24, 10.13s/it] 87%|████████▋ | 248/285 [39:55<06:12, 10.07s/it] 87%|████████▋ | 249/285 [40:05<06:03, 10.10s/it] 88%|████████▊ | 250/285 [40:16<05:57, 10.21s/it] 88%|████████▊ | 251/285 [40:25<05:39,  9.99s/it] 88%|████████▊ | 252/285 [40:35<05:30, 10.00s/it] 89%|████████▉ | 253/285 [40:45<05:16,  9.90s/it] 89%|████████▉ | 254/285 [40:55<05:10, 10.02s/it] 89%|████████▉ | 255/285 [41:05<04:59,  9.97s/it] 90%|████████▉ | 256/285 [41:15<04:45,  9.85s/it] 90%|█████████ | 257/285 [41:24<04:34,  9.79s/it] 91%|█████████ | 258/285 [41:34<04:25,  9.82s/it] 91%|█████████ | 259/285 [41:44<04:12,  9.69s/it] 91%|█████████ | 260/285 [41:53<03:59,  9.57s/it] 92%|█████████▏| 261/285 [42:03<03:52,  9.67s/it] 92%|█████████▏| 262/285 [42:12<03:39,  9.56s/it] 92%|█████████▏| 263/285 [42:22<03:31,  9.60s/it] 93%|█████████▎| 264/285 [42:32<03:22,  9.63s/it] 93%|█████████▎| 265/285 [42:41<03:12,  9.60s/it] 93%|█████████▎| 266/285 [42:51<03:03,  9.64s/it] 94%|█████████▎| 267/285 [43:01<02:54,  9.67s/it] 94%|█████████▍| 268/285 [43:10<02:43,  9.62s/it] 94%|█████████▍| 269/285 [43:20<02:35,  9.74s/it] 95%|█████████▍| 270/285 [43:30<02:24,  9.65s/it] 95%|█████████▌| 271/285 [43:39<02:15,  9.71s/it] 95%|█████████▌| 272/285 [43:49<02:06,  9.77s/it] 96%|█████████▌| 273/285 [43:59<01:57,  9.78s/it] 96%|█████████▌| 274/285 [44:09<01:47,  9.77s/it] 96%|█████████▋| 275/285 [44:18<01:36,  9.66s/it] 97%|█████████▋| 276/285 [44:28<01:26,  9.59s/it] 97%|█████████▋| 277/285 [44:37<01:16,  9.60s/it] 98%|█████████▊| 278/285 [44:47<01:07,  9.59s/it] 98%|█████████▊| 279/285 [44:57<00:58,  9.69s/it] 98%|█████████▊| 280/285 [45:07<00:48,  9.77s/it] 99%|█████████▊| 281/285 [45:16<00:38,  9.70s/it] 99%|█████████▉| 282/285 [45:26<00:29,  9.69s/it] 99%|█████████▉| 283/285 [45:36<00:19,  9.71s/it]100%|█████████▉| 284/285 [45:46<00:09,  9.89s/it]100%|██████████| 285/285 [45:56<00:00,  9.90s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [45:56<00:00,  9.90s/it]100%|██████████| 285/285 [45:56<00:00,  9.67s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'loss': 0.4041, 'learning_rate': 1e-05, 'epoch': 3.5}
{'train_runtime': 2756.4977, 'train_samples_per_second': 6.701, 'train_steps_per_second': 0.103, 'train_loss': 0.2956205016688297, 'epoch': 4.99}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:10,  6.42s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:19<03:54,  6.34s/it] 10%|█         | 4/40 [00:25<03:47,  6.33s/it] 12%|█▎        | 5/40 [00:31<03:44,  6.43s/it] 15%|█▌        | 6/40 [00:38<03:36,  6.38s/it] 18%|█▊        | 7/40 [00:44<03:27,  6.28s/it] 20%|██        | 8/40 [00:50<03:22,  6.33s/it] 22%|██▎       | 9/40 [01:01<04:01,  7.77s/it] 25%|██▌       | 10/40 [01:07<03:38,  7.29s/it] 28%|██▊       | 11/40 [01:14<03:23,  7.03s/it] 30%|███       | 12/40 [01:20<03:10,  6.81s/it] 32%|███▎      | 13/40 [01:26<02:59,  6.65s/it] 35%|███▌      | 14/40 [01:33<02:53,  6.67s/it] 38%|███▊      | 15/40 [01:40<02:49,  6.76s/it] 40%|████      | 16/40 [01:47<02:39,  6.65s/it] 42%|████▎     | 17/40 [01:58<03:03,  7.98s/it] 45%|████▌     | 18/40 [02:04<02:44,  7.48s/it] 48%|████▊     | 19/40 [02:10<02:29,  7.12s/it] 50%|█████     | 20/40 [02:16<02:16,  6.81s/it] 52%|█████▎    | 21/40 [02:23<02:07,  6.71s/it] 55%|█████▌    | 22/40 [02:29<02:00,  6.70s/it] 57%|█████▊    | 23/40 [02:36<01:52,  6.62s/it] 60%|██████    | 24/40 [02:42<01:43,  6.46s/it] 62%|██████▎   | 25/40 [02:53<01:58,  7.90s/it] 65%|██████▌   | 26/40 [03:00<01:46,  7.63s/it] 68%|██████▊   | 27/40 [03:07<01:36,  7.40s/it] 70%|███████   | 28/40 [03:14<01:27,  7.27s/it] 72%|███████▎  | 29/40 [03:21<01:17,  7.06s/it] 75%|███████▌  | 30/40 [03:27<01:09,  6.95s/it] 78%|███████▊  | 31/40 [03:34<01:01,  6.85s/it] 80%|████████  | 32/40 [03:41<00:54,  6.86s/it] 82%|████████▎ | 33/40 [03:52<00:57,  8.28s/it] 85%|████████▌ | 34/40 [03:59<00:46,  7.81s/it] 88%|████████▊ | 35/40 [04:06<00:37,  7.50s/it] 90%|█████████ | 36/40 [04:12<00:28,  7.14s/it] 92%|█████████▎| 37/40 [04:19<00:20,  6.99s/it] 95%|█████████▌| 38/40 [04:25<00:13,  6.85s/it] 98%|█████████▊| 39/40 [04:32<00:06,  6.78s/it]100%|██████████| 40/40 [04:39<00:00,  6.72s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:39<00:00,  6.72s/it]100%|██████████| 40/40 [04:39<00:00,  6.98s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 279.0343, 'train_samples_per_second': 10.053, 'train_steps_per_second': 0.143, 'train_loss': 0.7463733673095703, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:27,  6.85s/it]  5%|▌         | 2/40 [00:13<04:10,  6.60s/it]  8%|▊         | 3/40 [00:19<04:03,  6.59s/it] 10%|█         | 4/40 [00:26<03:57,  6.59s/it] 12%|█▎        | 5/40 [00:33<03:53,  6.66s/it] 15%|█▌        | 6/40 [00:39<03:44,  6.60s/it] 18%|█▊        | 7/40 [00:46<03:34,  6.50s/it] 20%|██        | 8/40 [00:52<03:29,  6.54s/it] 22%|██▎       | 9/40 [01:04<04:10,  8.08s/it] 25%|██▌       | 10/40 [01:10<03:47,  7.58s/it] 28%|██▊       | 11/40 [01:17<03:33,  7.38s/it] 30%|███       | 12/40 [01:24<03:20,  7.17s/it] 32%|███▎      | 13/40 [01:30<03:07,  6.95s/it] 35%|███▌      | 14/40 [01:37<02:59,  6.90s/it] 38%|███▊      | 15/40 [01:44<02:50,  6.82s/it] 40%|████      | 16/40 [01:50<02:43,  6.79s/it] 42%|████▎     | 17/40 [02:01<03:06,  8.12s/it] 45%|████▌     | 18/40 [02:08<02:46,  7.58s/it] 48%|████▊     | 19/40 [02:14<02:33,  7.31s/it] 50%|█████     | 20/40 [02:21<02:21,  7.05s/it] 52%|█████▎    | 21/40 [02:28<02:11,  6.94s/it] 55%|█████▌    | 22/40 [02:34<02:03,  6.88s/it] 57%|█████▊    | 23/40 [02:41<01:56,  6.85s/it] 60%|██████    | 24/40 [02:47<01:46,  6.68s/it] 62%|██████▎   | 25/40 [02:59<02:00,  8.05s/it] 65%|██████▌   | 26/40 [03:05<01:45,  7.57s/it] 68%|██████▊   | 27/40 [03:12<01:34,  7.25s/it] 70%|███████   | 28/40 [03:18<01:24,  7.03s/it] 72%|███████▎  | 29/40 [03:25<01:15,  6.90s/it] 75%|███████▌  | 30/40 [03:31<01:07,  6.78s/it] 78%|███████▊  | 31/40 [03:38<01:00,  6.71s/it] 80%|████████  | 32/40 [03:45<00:54,  6.78s/it] 82%|████████▎ | 33/40 [03:56<00:57,  8.18s/it] 85%|████████▌ | 34/40 [04:03<00:46,  7.75s/it] 88%|████████▊ | 35/40 [04:10<00:37,  7.42s/it] 90%|█████████ | 36/40 [04:16<00:28,  7.11s/it] 92%|█████████▎| 37/40 [04:22<00:20,  6.91s/it] 95%|█████████▌| 38/40 [04:29<00:13,  6.78s/it] 98%|█████████▊| 39/40 [04:36<00:06,  6.74s/it]100%|██████████| 40/40 [04:42<00:00,  6.75s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:42<00:00,  6.75s/it]100%|██████████| 40/40 [04:42<00:00,  7.07s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 282.7818, 'train_samples_per_second': 9.919, 'train_steps_per_second': 0.141, 'train_loss': 0.7505083084106445, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:54,  9.09s/it]  5%|▌         | 2/40 [00:18<05:43,  9.03s/it]  8%|▊         | 3/40 [00:27<05:34,  9.05s/it] 10%|█         | 4/40 [00:36<05:24,  9.01s/it] 12%|█▎        | 5/40 [00:45<05:20,  9.15s/it] 15%|█▌        | 6/40 [00:54<05:08,  9.08s/it] 18%|█▊        | 7/40 [01:02<04:53,  8.90s/it] 20%|██        | 8/40 [01:12<04:46,  8.96s/it] 22%|██▎       | 9/40 [01:27<05:44, 11.12s/it] 25%|██▌       | 10/40 [01:36<05:12, 10.41s/it] 28%|██▊       | 11/40 [01:46<04:51, 10.06s/it] 30%|███       | 12/40 [01:54<04:31,  9.71s/it] 32%|███▎      | 13/40 [02:03<04:12,  9.37s/it] 35%|███▌      | 14/40 [02:12<04:02,  9.34s/it] 38%|███▊      | 15/40 [02:21<03:51,  9.26s/it] 40%|████      | 16/40 [02:30<03:40,  9.18s/it] 42%|████▎     | 17/40 [02:46<04:14, 11.07s/it] 45%|████▌     | 18/40 [02:55<03:48, 10.37s/it] 48%|████▊     | 19/40 [03:04<03:28,  9.94s/it] 50%|█████     | 20/40 [03:12<03:11,  9.57s/it] 52%|█████▎    | 21/40 [03:21<02:57,  9.35s/it] 55%|█████▌    | 22/40 [03:30<02:48,  9.34s/it] 57%|█████▊    | 23/40 [03:39<02:37,  9.28s/it] 60%|██████    | 24/40 [03:48<02:26,  9.15s/it] 62%|██████▎   | 25/40 [04:04<02:47, 11.19s/it] 65%|██████▌   | 26/40 [04:13<02:27, 10.54s/it] 68%|██████▊   | 27/40 [04:22<02:11, 10.12s/it] 70%|███████   | 28/40 [04:32<01:58,  9.84s/it] 72%|███████▎  | 29/40 [04:41<01:46,  9.65s/it] 75%|███████▌  | 30/40 [04:50<01:34,  9.46s/it] 78%|███████▊  | 31/40 [04:59<01:23,  9.25s/it] 80%|████████  | 32/40 [05:08<01:13,  9.23s/it] 82%|████████▎ | 33/40 [05:23<01:17, 11.01s/it] 85%|████████▌ | 34/40 [05:32<01:02, 10.46s/it] 88%|████████▊ | 35/40 [05:41<00:50, 10.08s/it] 90%|█████████ | 36/40 [05:50<00:38,  9.62s/it] 92%|█████████▎| 37/40 [05:58<00:27,  9.31s/it] 95%|█████████▌| 38/40 [06:07<00:18,  9.07s/it] 98%|█████████▊| 39/40 [06:16<00:09,  9.03s/it]100%|██████████| 40/40 [06:25<00:00,  8.93s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:25<00:00,  8.93s/it]100%|██████████| 40/40 [06:25<00:00,  9.63s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 385.1224, 'train_samples_per_second': 7.283, 'train_steps_per_second': 0.104, 'train_loss': 0.7569923400878906, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_7.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:36, 10.17s/it]  5%|▌         | 2/40 [00:19<06:17,  9.93s/it]  8%|▊         | 3/40 [00:29<06:06,  9.91s/it] 10%|█         | 4/40 [00:39<05:56,  9.91s/it] 12%|█▎        | 5/40 [00:50<05:53, 10.10s/it] 15%|█▌        | 6/40 [01:00<05:41, 10.04s/it] 18%|█▊        | 7/40 [01:09<05:26,  9.88s/it] 20%|██        | 8/40 [01:19<05:19, 10.00s/it] 22%|██▎       | 9/40 [01:37<06:22, 12.35s/it] 25%|██▌       | 10/40 [01:47<05:45, 11.52s/it] 28%|██▊       | 11/40 [01:57<05:21, 11.09s/it] 30%|███       | 12/40 [02:07<04:59, 10.71s/it] 32%|███▎      | 13/40 [02:16<04:40, 10.40s/it] 35%|███▌      | 14/40 [02:27<04:31, 10.43s/it] 38%|███▊      | 15/40 [02:37<04:17, 10.29s/it] 40%|████      | 16/40 [02:47<04:04, 10.21s/it] 42%|████▎     | 17/40 [03:04<04:42, 12.29s/it] 45%|████▌     | 18/40 [03:14<04:14, 11.58s/it] 48%|████▊     | 19/40 [03:24<03:54, 11.15s/it] 50%|█████     | 20/40 [03:34<03:36, 10.84s/it] 52%|█████▎    | 21/40 [03:44<03:21, 10.61s/it] 55%|█████▌    | 22/40 [03:55<03:10, 10.60s/it] 57%|█████▊    | 23/40 [04:05<02:57, 10.47s/it] 60%|██████    | 24/40 [04:14<02:43, 10.22s/it] 62%|██████▎   | 25/40 [04:31<03:03, 12.26s/it] 65%|██████▌   | 26/40 [04:41<02:42, 11.58s/it] 68%|██████▊   | 27/40 [04:51<02:24, 11.08s/it] 70%|███████   | 28/40 [05:01<02:08, 10.72s/it] 72%|███████▎  | 29/40 [05:11<01:55, 10.46s/it] 75%|███████▌  | 30/40 [05:21<01:43, 10.35s/it] 78%|███████▊  | 31/40 [05:31<01:32, 10.27s/it] 80%|████████  | 32/40 [05:42<01:22, 10.31s/it] 82%|████████▎ | 33/40 [05:59<01:26, 12.42s/it] 85%|████████▌ | 34/40 [06:09<01:10, 11.83s/it] 88%|████████▊ | 35/40 [06:20<00:56, 11.32s/it] 90%|█████████ | 36/40 [06:29<00:43, 10.83s/it] 92%|█████████▎| 37/40 [06:39<00:31, 10.50s/it] 95%|█████████▌| 38/40 [06:48<00:20, 10.15s/it] 98%|█████████▊| 39/40 [06:58<00:10, 10.08s/it]100%|██████████| 40/40 [07:08<00:00, 10.00s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:08<00:00, 10.00s/it]100%|██████████| 40/40 [07:08<00:00, 10.71s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 428.586, 'train_samples_per_second': 6.545, 'train_steps_per_second': 0.093, 'train_loss': 0.7835698127746582, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:12,  6.47s/it]  5%|▌         | 2/40 [00:13<04:11,  6.62s/it]  8%|▊         | 3/40 [00:20<04:09,  6.73s/it] 10%|█         | 4/40 [00:27<04:05,  6.81s/it] 12%|█▎        | 5/40 [00:34<04:03,  6.95s/it] 15%|█▌        | 6/40 [00:41<03:54,  6.91s/it] 18%|█▊        | 7/40 [00:47<03:46,  6.87s/it] 20%|██        | 8/40 [00:54<03:40,  6.90s/it] 22%|██▎       | 9/40 [01:02<03:44,  7.23s/it] 25%|██▌       | 10/40 [01:09<03:34,  7.14s/it] 28%|██▊       | 11/40 [01:16<03:22,  7.00s/it] 30%|███       | 12/40 [01:22<03:12,  6.88s/it] 32%|███▎      | 13/40 [01:29<03:05,  6.88s/it] 35%|███▌      | 14/40 [01:36<02:58,  6.86s/it] 38%|███▊      | 15/40 [01:43<02:52,  6.90s/it] 40%|████      | 16/40 [01:50<02:46,  6.95s/it] 42%|████▎     | 17/40 [01:58<02:45,  7.18s/it] 45%|████▌     | 18/40 [02:05<02:37,  7.14s/it] 48%|████▊     | 19/40 [02:12<02:28,  7.06s/it] 50%|█████     | 20/40 [02:19<02:19,  6.98s/it] 52%|█████▎    | 21/40 [02:25<02:11,  6.94s/it] 55%|█████▌    | 22/40 [02:32<02:03,  6.85s/it] 57%|█████▊    | 23/40 [02:39<01:56,  6.85s/it] 60%|██████    | 24/40 [02:46<01:50,  6.89s/it] 62%|██████▎   | 25/40 [02:54<01:48,  7.22s/it] 65%|██████▌   | 26/40 [03:01<01:39,  7.08s/it] 68%|██████▊   | 27/40 [03:07<01:30,  6.97s/it] 70%|███████   | 28/40 [03:14<01:22,  6.89s/it] 72%|███████▎  | 29/40 [03:21<01:15,  6.86s/it] 75%|███████▌  | 30/40 [03:27<01:07,  6.75s/it] 78%|███████▊  | 31/40 [03:35<01:01,  6.88s/it] 80%|████████  | 32/40 [03:41<00:54,  6.86s/it] 82%|████████▎ | 33/40 [03:49<00:49,  7.13s/it] 85%|████████▌ | 34/40 [03:56<00:42,  7.08s/it] 88%|████████▊ | 35/40 [04:03<00:35,  7.06s/it] 90%|█████████ | 36/40 [04:10<00:27,  6.97s/it] 92%|█████████▎| 37/40 [04:17<00:20,  6.89s/it] 95%|█████████▌| 38/40 [04:23<00:13,  6.85s/it] 98%|█████████▊| 39/40 [04:30<00:06,  6.77s/it]100%|██████████| 40/40 [04:36<00:00,  6.61s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:36<00:00,  6.61s/it]100%|██████████| 40/40 [04:36<00:00,  6.92s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 276.6957, 'train_samples_per_second': 9.415, 'train_steps_per_second': 0.145, 'train_loss': 0.6989841461181641, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:57,  6.10s/it]  5%|▌         | 2/40 [00:12<03:59,  6.31s/it]  8%|▊         | 3/40 [00:18<03:51,  6.26s/it] 10%|█         | 4/40 [00:24<03:44,  6.24s/it] 12%|█▎        | 5/40 [00:31<03:43,  6.40s/it] 15%|█▌        | 6/40 [00:37<03:36,  6.37s/it] 18%|█▊        | 7/40 [00:44<03:28,  6.32s/it] 20%|██        | 8/40 [00:50<03:24,  6.39s/it] 22%|██▎       | 9/40 [00:58<03:27,  6.68s/it] 25%|██▌       | 10/40 [01:04<03:17,  6.58s/it] 28%|██▊       | 11/40 [01:10<03:08,  6.50s/it] 30%|███       | 12/40 [01:16<03:00,  6.44s/it] 32%|███▎      | 13/40 [01:23<02:53,  6.41s/it] 35%|███▌      | 14/40 [01:29<02:46,  6.41s/it] 38%|███▊      | 15/40 [01:36<02:41,  6.44s/it] 40%|████      | 16/40 [01:42<02:35,  6.49s/it] 42%|████▎     | 17/40 [01:50<02:36,  6.80s/it] 45%|████▌     | 18/40 [01:56<02:27,  6.70s/it] 48%|████▊     | 19/40 [02:03<02:19,  6.62s/it] 50%|█████     | 20/40 [02:09<02:10,  6.51s/it] 52%|█████▎    | 21/40 [02:15<02:02,  6.45s/it] 55%|█████▌    | 22/40 [02:22<01:54,  6.37s/it] 57%|█████▊    | 23/40 [02:28<01:47,  6.33s/it] 60%|██████    | 24/40 [02:34<01:42,  6.40s/it] 62%|██████▎   | 25/40 [02:41<01:39,  6.63s/it] 65%|██████▌   | 26/40 [02:48<01:31,  6.54s/it] 68%|██████▊   | 27/40 [02:54<01:24,  6.48s/it] 70%|███████   | 28/40 [03:00<01:16,  6.39s/it] 72%|███████▎  | 29/40 [03:07<01:10,  6.41s/it] 75%|███████▌  | 30/40 [03:13<01:03,  6.37s/it] 78%|███████▊  | 31/40 [03:19<00:57,  6.38s/it] 80%|████████  | 32/40 [03:26<00:51,  6.38s/it] 82%|████████▎ | 33/40 [03:33<00:46,  6.68s/it] 85%|████████▌ | 34/40 [03:40<00:39,  6.66s/it] 88%|████████▊ | 35/40 [03:47<00:33,  6.70s/it] 90%|█████████ | 36/40 [03:53<00:26,  6.68s/it] 92%|█████████▎| 37/40 [04:00<00:19,  6.66s/it] 95%|█████████▌| 38/40 [04:06<00:13,  6.61s/it] 98%|█████████▊| 39/40 [04:13<00:06,  6.53s/it]100%|██████████| 40/40 [04:19<00:00,  6.37s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:19<00:00,  6.37s/it]100%|██████████| 40/40 [04:19<00:00,  6.48s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 259.2653, 'train_samples_per_second': 10.048, 'train_steps_per_second': 0.154, 'train_loss': 0.7157560348510742, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:36,  8.63s/it]  5%|▌         | 2/40 [00:17<05:39,  8.95s/it]  8%|▊         | 3/40 [00:26<05:25,  8.80s/it] 10%|█         | 4/40 [00:35<05:15,  8.77s/it] 12%|█▎        | 5/40 [00:44<05:16,  9.04s/it] 15%|█▌        | 6/40 [00:53<05:05,  8.98s/it] 18%|█▊        | 7/40 [01:02<04:55,  8.94s/it] 20%|██        | 8/40 [01:11<04:49,  9.03s/it] 22%|██▎       | 9/40 [01:22<04:53,  9.47s/it] 25%|██▌       | 10/40 [01:30<04:37,  9.27s/it] 28%|██▊       | 11/40 [01:39<04:25,  9.15s/it] 30%|███       | 12/40 [01:49<04:18,  9.24s/it] 32%|███▎      | 13/40 [01:58<04:10,  9.27s/it] 35%|███▌      | 14/40 [02:07<04:01,  9.31s/it] 38%|███▊      | 15/40 [02:17<03:54,  9.38s/it] 40%|████      | 16/40 [02:27<03:47,  9.47s/it] 42%|████▎     | 17/40 [02:37<03:43,  9.74s/it] 45%|████▌     | 18/40 [02:46<03:31,  9.59s/it] 48%|████▊     | 19/40 [02:55<03:17,  9.39s/it] 50%|█████     | 20/40 [03:04<03:04,  9.24s/it] 52%|█████▎    | 21/40 [03:13<02:54,  9.17s/it] 55%|█████▌    | 22/40 [03:22<02:42,  9.05s/it] 57%|█████▊    | 23/40 [03:31<02:32,  8.96s/it] 60%|██████    | 24/40 [03:40<02:25,  9.10s/it] 62%|██████▎   | 25/40 [03:50<02:21,  9.43s/it] 65%|██████▌   | 26/40 [03:59<02:09,  9.26s/it] 68%|██████▊   | 27/40 [04:08<01:59,  9.19s/it] 70%|███████   | 28/40 [04:17<01:48,  9.05s/it] 72%|███████▎  | 29/40 [04:26<01:39,  9.07s/it] 75%|███████▌  | 30/40 [04:35<01:30,  9.01s/it] 78%|███████▊  | 31/40 [04:44<01:21,  9.04s/it] 80%|████████  | 32/40 [04:53<01:12,  9.03s/it] 82%|████████▎ | 33/40 [05:03<01:06,  9.43s/it] 85%|████████▌ | 34/40 [05:13<00:56,  9.39s/it] 88%|████████▊ | 35/40 [05:22<00:46,  9.32s/it] 90%|█████████ | 36/40 [05:31<00:36,  9.16s/it] 92%|█████████▎| 37/40 [05:39<00:27,  9.06s/it] 95%|█████████▌| 38/40 [05:49<00:18,  9.11s/it] 98%|█████████▊| 39/40 [05:58<00:09,  9.05s/it]100%|██████████| 40/40 [06:06<00:00,  8.83s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:06<00:00,  8.83s/it]100%|██████████| 40/40 [06:06<00:00,  9.16s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 366.3295, 'train_samples_per_second': 7.111, 'train_steps_per_second': 0.109, 'train_loss': 0.6979462623596191, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_1.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:08,  9.45s/it]  5%|▌         | 2/40 [00:19<06:13,  9.82s/it]  8%|▊         | 3/40 [00:29<06:05,  9.87s/it] 10%|█         | 4/40 [00:39<05:54,  9.84s/it] 12%|█▎        | 5/40 [00:49<05:52, 10.06s/it] 15%|█▌        | 6/40 [00:59<05:39,  9.98s/it] 18%|█▊        | 7/40 [01:09<05:27,  9.91s/it] 20%|██        | 8/40 [01:20<05:25, 10.18s/it] 22%|██▎       | 9/40 [01:31<05:30, 10.65s/it] 25%|██▌       | 10/40 [01:41<05:11, 10.38s/it] 28%|██▊       | 11/40 [01:51<04:57, 10.25s/it] 30%|███       | 12/40 [02:01<04:43, 10.13s/it] 32%|███▎      | 13/40 [02:11<04:32, 10.09s/it] 35%|███▌      | 14/40 [02:21<04:19,  9.98s/it] 38%|███▊      | 15/40 [02:31<04:10, 10.02s/it] 40%|████      | 16/40 [02:41<04:04, 10.19s/it] 42%|████▎     | 17/40 [02:53<04:03, 10.57s/it] 45%|████▌     | 18/40 [03:03<03:48, 10.41s/it] 48%|████▊     | 19/40 [03:13<03:35, 10.25s/it] 50%|█████     | 20/40 [03:23<03:24, 10.24s/it] 52%|█████▎    | 21/40 [03:33<03:14, 10.26s/it] 55%|█████▌    | 22/40 [03:43<03:03, 10.17s/it] 57%|█████▊    | 23/40 [03:53<02:51, 10.07s/it] 60%|██████    | 24/40 [04:04<02:43, 10.24s/it] 62%|██████▎   | 25/40 [04:15<02:39, 10.60s/it] 65%|██████▌   | 26/40 [04:25<02:25, 10.41s/it] 68%|██████▊   | 27/40 [04:35<02:13, 10.25s/it] 70%|███████   | 28/40 [04:44<02:00, 10.03s/it] 72%|███████▎  | 29/40 [04:54<01:50, 10.02s/it] 75%|███████▌  | 30/40 [05:04<01:39,  9.93s/it] 78%|███████▊  | 31/40 [05:14<01:29,  9.98s/it] 80%|████████  | 32/40 [05:24<01:20, 10.01s/it] 82%|████████▎ | 33/40 [05:36<01:14, 10.58s/it] 85%|████████▌ | 34/40 [05:47<01:03, 10.53s/it] 88%|████████▊ | 35/40 [05:57<00:52, 10.42s/it] 90%|█████████ | 36/40 [06:07<00:41, 10.46s/it] 92%|█████████▎| 37/40 [06:18<00:31, 10.60s/it] 95%|█████████▌| 38/40 [06:29<00:21, 10.55s/it] 98%|█████████▊| 39/40 [06:39<00:10, 10.46s/it]100%|██████████| 40/40 [06:49<00:00, 10.26s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:49<00:00, 10.26s/it]100%|██████████| 40/40 [06:49<00:00, 10.23s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 409.2186, 'train_samples_per_second': 6.366, 'train_steps_per_second': 0.098, 'train_loss': 0.7552658081054687, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:05,  6.30s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:19<03:54,  6.35s/it] 10%|█         | 4/40 [00:25<03:49,  6.37s/it] 12%|█▎        | 5/40 [00:31<03:39,  6.27s/it] 15%|█▌        | 6/40 [00:37<03:31,  6.21s/it] 18%|█▊        | 7/40 [00:43<03:25,  6.24s/it] 20%|██        | 8/40 [00:50<03:21,  6.30s/it] 22%|██▎       | 9/40 [00:57<03:22,  6.55s/it] 25%|██▌       | 10/40 [01:03<03:12,  6.43s/it] 28%|██▊       | 11/40 [01:09<03:02,  6.28s/it] 30%|███       | 12/40 [01:15<02:55,  6.27s/it] 32%|███▎      | 13/40 [01:22<02:51,  6.34s/it] 35%|███▌      | 14/40 [01:28<02:47,  6.43s/it] 38%|███▊      | 15/40 [01:35<02:44,  6.57s/it] 40%|████      | 16/40 [01:42<02:36,  6.54s/it] 42%|████▎     | 17/40 [01:49<02:33,  6.66s/it] 45%|████▌     | 18/40 [01:55<02:24,  6.57s/it] 48%|████▊     | 19/40 [02:01<02:16,  6.51s/it] 50%|█████     | 20/40 [02:08<02:07,  6.39s/it] 52%|█████▎    | 21/40 [02:14<02:02,  6.43s/it] 55%|█████▌    | 22/40 [02:20<01:55,  6.40s/it] 57%|█████▊    | 23/40 [02:27<01:49,  6.42s/it] 60%|██████    | 24/40 [02:34<01:44,  6.50s/it] 62%|██████▎   | 25/40 [02:41<01:40,  6.71s/it] 65%|██████▌   | 26/40 [02:47<01:31,  6.56s/it] 68%|██████▊   | 27/40 [02:53<01:25,  6.54s/it] 70%|███████   | 28/40 [03:00<01:17,  6.42s/it] 72%|███████▎  | 29/40 [03:06<01:10,  6.39s/it] 75%|███████▌  | 30/40 [03:12<01:03,  6.34s/it] 78%|███████▊  | 31/40 [03:19<00:57,  6.42s/it] 80%|████████  | 32/40 [03:25<00:51,  6.42s/it] 82%|████████▎ | 33/40 [03:32<00:46,  6.64s/it] 85%|████████▌ | 34/40 [03:39<00:39,  6.61s/it] 88%|████████▊ | 35/40 [03:45<00:33,  6.60s/it] 90%|█████████ | 36/40 [03:52<00:25,  6.47s/it] 92%|█████████▎| 37/40 [03:58<00:19,  6.48s/it] 95%|█████████▌| 38/40 [04:04<00:12,  6.36s/it] 98%|█████████▊| 39/40 [04:11<00:06,  6.38s/it]100%|██████████| 40/40 [04:17<00:00,  6.26s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:17<00:00,  6.26s/it]100%|██████████| 40/40 [04:17<00:00,  6.43s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 257.1429, 'train_samples_per_second': 10.131, 'train_steps_per_second': 0.156, 'train_loss': 0.6989320755004883, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:00,  6.16s/it]  5%|▌         | 2/40 [00:12<04:00,  6.32s/it]  8%|▊         | 3/40 [00:19<03:56,  6.40s/it] 10%|█         | 4/40 [00:25<03:51,  6.43s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.33s/it] 15%|█▌        | 6/40 [00:37<03:33,  6.29s/it] 18%|█▊        | 7/40 [00:44<03:27,  6.30s/it] 20%|██        | 8/40 [00:50<03:23,  6.36s/it] 22%|██▎       | 9/40 [00:57<03:25,  6.63s/it] 25%|██▌       | 10/40 [01:04<03:15,  6.52s/it] 28%|██▊       | 11/40 [01:10<03:04,  6.37s/it] 30%|███       | 12/40 [01:16<02:59,  6.41s/it] 32%|███▎      | 13/40 [01:23<02:53,  6.44s/it] 35%|███▌      | 14/40 [01:29<02:46,  6.40s/it] 38%|███▊      | 15/40 [01:36<02:40,  6.44s/it] 40%|████      | 16/40 [01:42<02:34,  6.45s/it] 42%|████▎     | 17/40 [01:49<02:31,  6.60s/it] 45%|████▌     | 18/40 [01:55<02:23,  6.52s/it] 48%|████▊     | 19/40 [02:02<02:15,  6.46s/it] 50%|█████     | 20/40 [02:08<02:05,  6.30s/it] 52%|█████▎    | 21/40 [02:14<02:00,  6.35s/it] 55%|█████▌    | 22/40 [02:20<01:53,  6.29s/it] 57%|█████▊    | 23/40 [02:26<01:46,  6.27s/it] 60%|██████    | 24/40 [02:33<01:42,  6.40s/it] 62%|██████▎   | 25/40 [02:40<01:40,  6.68s/it] 65%|██████▌   | 26/40 [02:47<01:31,  6.53s/it] 68%|██████▊   | 27/40 [02:53<01:24,  6.51s/it] 70%|███████   | 28/40 [02:59<01:16,  6.34s/it] 72%|███████▎  | 29/40 [03:05<01:09,  6.31s/it] 75%|███████▌  | 30/40 [03:12<01:02,  6.28s/it] 78%|███████▊  | 31/40 [03:18<00:57,  6.34s/it] 80%|████████  | 32/40 [03:24<00:50,  6.30s/it] 82%|████████▎ | 33/40 [03:31<00:45,  6.51s/it] 85%|████████▌ | 34/40 [03:38<00:38,  6.47s/it] 88%|████████▊ | 35/40 [03:44<00:32,  6.49s/it] 90%|█████████ | 36/40 [03:50<00:25,  6.38s/it] 92%|█████████▎| 37/40 [03:57<00:19,  6.40s/it] 95%|█████████▌| 38/40 [04:03<00:12,  6.28s/it] 98%|█████████▊| 39/40 [04:09<00:06,  6.32s/it]100%|██████████| 40/40 [04:15<00:00,  6.21s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:15<00:00,  6.21s/it]100%|██████████| 40/40 [04:15<00:00,  6.39s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 255.568, 'train_samples_per_second': 10.193, 'train_steps_per_second': 0.157, 'train_loss': 0.7157247543334961, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:43,  8.80s/it]  5%|▌         | 2/40 [00:17<05:40,  8.97s/it]  8%|▊         | 3/40 [00:26<05:34,  9.03s/it] 10%|█         | 4/40 [00:36<05:27,  9.09s/it] 12%|█▎        | 5/40 [00:44<05:11,  8.90s/it] 15%|█▌        | 6/40 [00:53<05:03,  8.93s/it] 18%|█▊        | 7/40 [01:03<05:00,  9.10s/it] 20%|██        | 8/40 [01:12<04:56,  9.27s/it] 22%|██▎       | 9/40 [01:23<04:59,  9.67s/it] 25%|██▌       | 10/40 [01:32<04:47,  9.58s/it] 28%|██▊       | 11/40 [01:41<04:30,  9.34s/it] 30%|███       | 12/40 [01:50<04:18,  9.24s/it] 32%|███▎      | 13/40 [01:59<04:11,  9.31s/it] 35%|███▌      | 14/40 [02:08<03:57,  9.14s/it] 38%|███▊      | 15/40 [02:18<03:49,  9.18s/it] 40%|████      | 16/40 [02:27<03:41,  9.22s/it] 42%|████▎     | 17/40 [02:37<03:38,  9.49s/it] 45%|████▌     | 18/40 [02:46<03:26,  9.37s/it] 48%|████▊     | 19/40 [02:55<03:15,  9.32s/it] 50%|█████     | 20/40 [03:04<03:01,  9.07s/it] 52%|█████▎    | 21/40 [03:13<02:54,  9.19s/it] 55%|█████▌    | 22/40 [03:22<02:43,  9.06s/it] 57%|█████▊    | 23/40 [03:31<02:33,  9.03s/it] 60%|██████    | 24/40 [03:41<02:28,  9.25s/it] 62%|██████▎   | 25/40 [03:51<02:23,  9.58s/it] 65%|██████▌   | 26/40 [04:00<02:10,  9.33s/it] 68%|██████▊   | 27/40 [04:09<02:01,  9.35s/it] 70%|███████   | 28/40 [04:18<01:51,  9.27s/it] 72%|███████▎  | 29/40 [04:27<01:41,  9.20s/it] 75%|███████▌  | 30/40 [04:36<01:31,  9.14s/it] 78%|███████▊  | 31/40 [04:46<01:23,  9.23s/it] 80%|████████  | 32/40 [04:55<01:13,  9.19s/it] 82%|████████▎ | 33/40 [05:05<01:06,  9.45s/it] 85%|████████▌ | 34/40 [05:14<00:56,  9.38s/it] 88%|████████▊ | 35/40 [05:24<00:46,  9.39s/it] 90%|█████████ | 36/40 [05:33<00:37,  9.26s/it] 92%|█████████▎| 37/40 [05:42<00:27,  9.30s/it] 95%|█████████▌| 38/40 [05:51<00:18,  9.13s/it] 98%|█████████▊| 39/40 [06:00<00:09,  9.20s/it]100%|██████████| 40/40 [06:09<00:00,  9.02s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:09<00:00,  9.02s/it]100%|██████████| 40/40 [06:09<00:00,  9.23s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 369.1089, 'train_samples_per_second': 7.058, 'train_steps_per_second': 0.108, 'train_loss': 0.6985294342041015, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_3.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:31, 10.03s/it]  5%|▌         | 2/40 [00:20<06:24, 10.12s/it]  8%|▊         | 3/40 [00:30<06:16, 10.18s/it] 10%|█         | 4/40 [00:40<06:06, 10.18s/it] 12%|█▎        | 5/40 [00:50<05:50, 10.01s/it] 15%|█▌        | 6/40 [01:00<05:40, 10.00s/it] 18%|█▊        | 7/40 [01:10<05:32, 10.09s/it] 20%|██        | 8/40 [01:21<05:31, 10.36s/it] 22%|██▎       | 9/40 [01:33<05:38, 10.92s/it] 25%|██▌       | 10/40 [01:44<05:23, 10.80s/it] 28%|██▊       | 11/40 [01:53<05:00, 10.37s/it] 30%|███       | 12/40 [02:03<04:46, 10.25s/it] 32%|███▎      | 13/40 [02:13<04:37, 10.29s/it] 35%|███▌      | 14/40 [02:23<04:23, 10.15s/it] 38%|███▊      | 15/40 [02:34<04:14, 10.19s/it] 40%|████      | 16/40 [02:44<04:05, 10.21s/it] 42%|████▎     | 17/40 [02:55<04:00, 10.46s/it] 45%|████▌     | 18/40 [03:05<03:47, 10.34s/it] 48%|████▊     | 19/40 [03:15<03:35, 10.28s/it] 50%|█████     | 20/40 [03:25<03:20, 10.03s/it] 52%|█████▎    | 21/40 [03:35<03:12, 10.12s/it] 55%|█████▌    | 22/40 [03:45<03:00, 10.00s/it] 57%|█████▊    | 23/40 [03:54<02:49,  9.95s/it] 60%|██████    | 24/40 [04:05<02:42, 10.15s/it] 62%|██████▎   | 25/40 [04:17<02:38, 10.55s/it] 65%|██████▌   | 26/40 [04:26<02:24, 10.33s/it] 68%|██████▊   | 27/40 [04:37<02:14, 10.31s/it] 70%|███████   | 28/40 [04:46<02:00, 10.08s/it] 72%|███████▎  | 29/40 [04:56<01:50, 10.07s/it] 75%|███████▌  | 30/40 [05:06<01:40, 10.09s/it] 78%|███████▊  | 31/40 [05:17<01:33, 10.36s/it] 80%|████████  | 32/40 [05:28<01:23, 10.44s/it] 82%|████████▎ | 33/40 [05:40<01:15, 10.84s/it] 85%|████████▌ | 34/40 [05:50<01:04, 10.81s/it] 88%|████████▊ | 35/40 [06:01<00:54, 10.85s/it] 90%|█████████ | 36/40 [06:11<00:42, 10.56s/it] 92%|█████████▎| 37/40 [06:22<00:31, 10.49s/it] 95%|█████████▌| 38/40 [06:31<00:20, 10.24s/it] 98%|█████████▊| 39/40 [06:41<00:10, 10.22s/it]100%|██████████| 40/40 [06:51<00:00, 10.00s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:51<00:00, 10.00s/it]100%|██████████| 40/40 [06:51<00:00, 10.29s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 411.4348, 'train_samples_per_second': 6.332, 'train_steps_per_second': 0.097, 'train_loss': 0.755159568786621, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:03,  6.24s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:18<03:53,  6.32s/it] 10%|█         | 4/40 [00:25<03:46,  6.29s/it] 12%|█▎        | 5/40 [00:31<03:39,  6.28s/it] 15%|█▌        | 6/40 [00:37<03:32,  6.25s/it] 18%|█▊        | 7/40 [00:43<03:24,  6.20s/it] 20%|██        | 8/40 [00:50<03:21,  6.30s/it] 22%|██▎       | 9/40 [00:57<03:25,  6.64s/it] 25%|██▌       | 10/40 [01:03<03:12,  6.41s/it] 28%|██▊       | 11/40 [01:09<03:03,  6.34s/it] 30%|███       | 12/40 [01:15<02:55,  6.26s/it] 32%|███▎      | 13/40 [01:22<02:49,  6.27s/it] 35%|███▌      | 14/40 [01:28<02:42,  6.24s/it] 38%|███▊      | 15/40 [01:34<02:37,  6.30s/it] 40%|████      | 16/40 [01:41<02:33,  6.38s/it] 42%|████▎     | 17/40 [01:48<02:30,  6.54s/it] 45%|████▌     | 18/40 [01:54<02:22,  6.47s/it] 48%|████▊     | 19/40 [02:00<02:14,  6.40s/it] 50%|█████     | 20/40 [02:06<02:06,  6.32s/it] 52%|█████▎    | 21/40 [02:13<02:00,  6.32s/it] 55%|█████▌    | 22/40 [02:19<01:53,  6.29s/it] 57%|█████▊    | 23/40 [02:25<01:47,  6.30s/it] 60%|██████    | 24/40 [02:32<01:41,  6.37s/it] 62%|██████▎   | 25/40 [02:39<01:40,  6.68s/it] 65%|██████▌   | 26/40 [02:45<01:31,  6.52s/it] 68%|██████▊   | 27/40 [02:52<01:24,  6.47s/it] 70%|███████   | 28/40 [02:58<01:16,  6.34s/it] 72%|███████▎  | 29/40 [03:04<01:09,  6.29s/it] 75%|███████▌  | 30/40 [03:10<01:03,  6.35s/it] 78%|███████▊  | 31/40 [03:17<00:58,  6.47s/it] 80%|████████  | 32/40 [03:24<00:51,  6.46s/it] 82%|████████▎ | 33/40 [03:31<00:46,  6.70s/it] 85%|████████▌ | 34/40 [03:37<00:39,  6.62s/it] 88%|████████▊ | 35/40 [03:44<00:33,  6.66s/it] 90%|█████████ | 36/40 [03:51<00:26,  6.67s/it] 92%|█████████▎| 37/40 [03:57<00:20,  6.69s/it] 95%|█████████▌| 38/40 [04:04<00:13,  6.64s/it] 98%|█████████▊| 39/40 [04:10<00:06,  6.52s/it]100%|██████████| 40/40 [04:16<00:00,  6.40s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:16<00:00,  6.40s/it]100%|██████████| 40/40 [04:16<00:00,  6.42s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 256.8148, 'train_samples_per_second': 10.143, 'train_steps_per_second': 0.156, 'train_loss': 0.6988674163818359, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:54,  6.02s/it]  5%|▌         | 2/40 [00:12<03:56,  6.21s/it]  8%|▊         | 3/40 [00:18<03:49,  6.20s/it] 10%|█         | 4/40 [00:24<03:42,  6.18s/it] 12%|█▎        | 5/40 [00:30<03:36,  6.18s/it] 15%|█▌        | 6/40 [00:37<03:31,  6.21s/it] 18%|█▊        | 7/40 [00:43<03:22,  6.13s/it] 20%|██        | 8/40 [00:49<03:19,  6.24s/it] 22%|██▎       | 9/40 [00:56<03:24,  6.59s/it] 25%|██▌       | 10/40 [01:02<03:10,  6.34s/it] 28%|██▊       | 11/40 [01:08<03:02,  6.29s/it] 30%|███       | 12/40 [01:15<02:54,  6.25s/it] 32%|███▎      | 13/40 [01:21<02:49,  6.28s/it] 35%|███▌      | 14/40 [01:27<02:42,  6.27s/it] 38%|███▊      | 15/40 [01:34<02:38,  6.34s/it] 40%|████      | 16/40 [01:40<02:33,  6.40s/it] 42%|████▎     | 17/40 [01:47<02:31,  6.57s/it] 45%|████▌     | 18/40 [01:53<02:22,  6.49s/it] 48%|████▊     | 19/40 [02:00<02:14,  6.41s/it] 50%|█████     | 20/40 [02:06<02:07,  6.36s/it] 52%|█████▎    | 21/40 [02:12<02:01,  6.37s/it] 55%|█████▌    | 22/40 [02:19<01:54,  6.36s/it] 57%|█████▊    | 23/40 [02:25<01:48,  6.36s/it] 60%|██████    | 24/40 [02:32<01:42,  6.40s/it] 62%|██████▎   | 25/40 [02:39<01:40,  6.70s/it] 65%|██████▌   | 26/40 [02:45<01:31,  6.54s/it] 68%|██████▊   | 27/40 [02:51<01:24,  6.49s/it] 70%|███████   | 28/40 [02:57<01:16,  6.36s/it] 72%|███████▎  | 29/40 [03:04<01:09,  6.29s/it] 75%|███████▌  | 30/40 [03:10<01:02,  6.28s/it] 78%|███████▊  | 31/40 [03:16<00:56,  6.28s/it] 80%|████████  | 32/40 [03:23<00:50,  6.33s/it] 82%|████████▎ | 33/40 [03:30<00:46,  6.60s/it] 85%|████████▌ | 34/40 [03:36<00:39,  6.55s/it] 88%|████████▊ | 35/40 [03:43<00:32,  6.52s/it] 90%|█████████ | 36/40 [03:49<00:25,  6.43s/it] 92%|█████████▎| 37/40 [03:55<00:19,  6.40s/it] 95%|█████████▌| 38/40 [04:02<00:12,  6.37s/it] 98%|█████████▊| 39/40 [04:08<00:06,  6.34s/it]100%|██████████| 40/40 [04:14<00:00,  6.27s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:14<00:00,  6.27s/it]100%|██████████| 40/40 [04:14<00:00,  6.36s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 254.4283, 'train_samples_per_second': 10.239, 'train_steps_per_second': 0.157, 'train_loss': 0.7159580230712891, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:53,  9.08s/it]  5%|▌         | 2/40 [00:18<05:54,  9.32s/it]  8%|▊         | 3/40 [00:27<05:43,  9.28s/it] 10%|█         | 4/40 [00:36<05:32,  9.23s/it] 12%|█▎        | 5/40 [00:46<05:22,  9.21s/it] 15%|█▌        | 6/40 [00:55<05:11,  9.16s/it] 18%|█▊        | 7/40 [01:04<04:58,  9.05s/it] 20%|██        | 8/40 [01:13<04:55,  9.25s/it] 22%|██▎       | 9/40 [01:24<05:01,  9.74s/it] 25%|██▌       | 10/40 [01:33<04:40,  9.36s/it] 28%|██▊       | 11/40 [01:42<04:29,  9.29s/it] 30%|███       | 12/40 [01:51<04:17,  9.19s/it] 32%|███▎      | 13/40 [02:00<04:06,  9.13s/it] 35%|███▌      | 14/40 [02:08<03:54,  9.00s/it] 38%|███▊      | 15/40 [02:18<03:47,  9.11s/it] 40%|████      | 16/40 [02:27<03:40,  9.21s/it] 42%|████▎     | 17/40 [02:37<03:37,  9.45s/it] 45%|████▌     | 18/40 [02:46<03:26,  9.37s/it] 48%|████▊     | 19/40 [02:55<03:13,  9.22s/it] 50%|█████     | 20/40 [03:04<03:02,  9.13s/it] 52%|█████▎    | 21/40 [03:13<02:53,  9.12s/it] 55%|█████▌    | 22/40 [03:22<02:42,  9.00s/it] 57%|█████▊    | 23/40 [03:31<02:32,  9.00s/it] 60%|██████    | 24/40 [03:40<02:24,  9.04s/it] 62%|██████▎   | 25/40 [03:50<02:21,  9.46s/it] 65%|██████▌   | 26/40 [03:59<02:09,  9.22s/it] 68%|██████▊   | 27/40 [04:08<02:00,  9.23s/it] 70%|███████   | 28/40 [04:17<01:49,  9.09s/it] 72%|███████▎  | 29/40 [04:26<01:39,  9.02s/it] 75%|███████▌  | 30/40 [04:35<01:30,  9.09s/it] 78%|███████▊  | 31/40 [04:44<01:22,  9.11s/it] 80%|████████  | 32/40 [04:54<01:13,  9.16s/it] 82%|████████▎ | 33/40 [05:04<01:06,  9.50s/it] 85%|████████▌ | 34/40 [05:13<00:56,  9.42s/it] 88%|████████▊ | 35/40 [05:22<00:46,  9.33s/it] 90%|█████████ | 36/40 [05:31<00:36,  9.19s/it] 92%|█████████▎| 37/40 [05:40<00:27,  9.10s/it] 95%|█████████▌| 38/40 [05:49<00:18,  9.08s/it] 98%|█████████▊| 39/40 [05:58<00:09,  9.13s/it]100%|██████████| 40/40 [06:07<00:00,  9.06s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:07<00:00,  9.06s/it]100%|██████████| 40/40 [06:07<00:00,  9.19s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 367.7934, 'train_samples_per_second': 7.083, 'train_steps_per_second': 0.109, 'train_loss': 0.6981109619140625, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_6.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:18,  9.70s/it]  5%|▌         | 2/40 [00:20<06:23, 10.10s/it]  8%|▊         | 3/40 [00:30<06:14, 10.12s/it] 10%|█         | 4/40 [00:39<05:58,  9.97s/it] 12%|█▎        | 5/40 [00:49<05:48,  9.97s/it] 15%|█▌        | 6/40 [01:00<05:42, 10.07s/it] 18%|█▊        | 7/40 [01:09<05:25,  9.86s/it] 20%|██        | 8/40 [01:19<05:19,  9.98s/it] 22%|██▎       | 9/40 [01:31<05:28, 10.61s/it] 25%|██▌       | 10/40 [01:41<05:06, 10.22s/it] 28%|██▊       | 11/40 [01:51<04:53, 10.11s/it] 30%|███       | 12/40 [02:00<04:40, 10.00s/it] 32%|███▎      | 13/40 [02:10<04:29,  9.99s/it] 35%|███▌      | 14/40 [02:20<04:17,  9.89s/it] 38%|███▊      | 15/40 [02:30<04:09,  9.98s/it] 40%|████      | 16/40 [02:40<04:00, 10.03s/it] 42%|████▎     | 17/40 [02:51<03:56, 10.26s/it] 45%|████▌     | 18/40 [03:01<03:42, 10.13s/it] 48%|████▊     | 19/40 [03:10<03:28,  9.95s/it] 50%|█████     | 20/40 [03:20<03:17,  9.86s/it] 52%|█████▎    | 21/40 [03:30<03:07,  9.89s/it] 55%|█████▌    | 22/40 [03:40<02:57,  9.87s/it] 57%|█████▊    | 23/40 [03:50<02:48,  9.90s/it] 60%|██████    | 24/40 [04:00<02:40, 10.01s/it] 62%|██████▎   | 25/40 [04:12<02:36, 10.44s/it] 65%|██████▌   | 26/40 [04:21<02:23, 10.25s/it] 68%|██████▊   | 27/40 [04:32<02:14, 10.34s/it] 70%|███████   | 28/40 [04:42<02:02, 10.23s/it] 72%|███████▎  | 29/40 [04:52<01:51, 10.17s/it] 75%|███████▌  | 30/40 [05:02<01:41, 10.16s/it] 78%|███████▊  | 31/40 [05:12<01:31, 10.17s/it] 80%|████████  | 32/40 [05:23<01:21, 10.23s/it] 82%|████████▎ | 33/40 [05:34<01:14, 10.66s/it] 85%|████████▌ | 34/40 [05:45<01:03, 10.57s/it] 88%|████████▊ | 35/40 [05:55<00:52, 10.53s/it] 90%|█████████ | 36/40 [06:05<00:41, 10.38s/it] 92%|█████████▎| 37/40 [06:15<00:30, 10.28s/it] 95%|█████████▌| 38/40 [06:25<00:20, 10.15s/it] 98%|█████████▊| 39/40 [06:35<00:10, 10.03s/it]100%|██████████| 40/40 [06:44<00:00,  9.89s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:44<00:00,  9.89s/it]100%|██████████| 40/40 [06:44<00:00, 10.12s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 404.7771, 'train_samples_per_second': 6.436, 'train_steps_per_second': 0.099, 'train_loss': 0.7550222396850585, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:48,  5.87s/it]  5%|▌         | 2/40 [00:11<03:46,  5.96s/it]  8%|▊         | 3/40 [00:17<03:39,  5.95s/it] 10%|█         | 4/40 [00:23<03:33,  5.93s/it] 12%|█▎        | 5/40 [00:29<03:30,  6.00s/it] 15%|█▌        | 6/40 [00:35<03:21,  5.92s/it] 18%|█▊        | 7/40 [00:41<03:17,  5.99s/it] 20%|██        | 8/40 [00:47<03:10,  5.97s/it] 22%|██▎       | 9/40 [00:57<03:46,  7.32s/it] 25%|██▌       | 10/40 [01:04<03:30,  7.00s/it] 28%|██▊       | 11/40 [01:10<03:16,  6.76s/it] 30%|███       | 12/40 [01:16<03:03,  6.57s/it] 32%|███▎      | 13/40 [01:22<02:53,  6.44s/it] 35%|███▌      | 14/40 [01:29<02:46,  6.40s/it] 38%|███▊      | 15/40 [01:34<02:35,  6.22s/it] 40%|████      | 16/40 [01:41<02:29,  6.21s/it] 42%|████▎     | 17/40 [01:51<02:51,  7.48s/it] 45%|████▌     | 18/40 [01:57<02:34,  7.01s/it] 48%|████▊     | 19/40 [02:03<02:18,  6.61s/it] 50%|█████     | 20/40 [02:09<02:09,  6.49s/it] 52%|█████▎    | 21/40 [02:14<01:58,  6.25s/it] 55%|█████▌    | 22/40 [02:21<01:52,  6.23s/it] 57%|█████▊    | 23/40 [02:26<01:43,  6.11s/it] 60%|██████    | 24/40 [02:32<01:36,  6.01s/it] 62%|██████▎   | 25/40 [02:42<01:48,  7.21s/it] 65%|██████▌   | 26/40 [02:48<01:34,  6.77s/it] 68%|██████▊   | 27/40 [02:54<01:24,  6.48s/it] 70%|███████   | 28/40 [03:00<01:15,  6.26s/it] 72%|███████▎  | 29/40 [03:06<01:08,  6.27s/it] 75%|███████▌  | 30/40 [03:12<01:02,  6.24s/it] 78%|███████▊  | 31/40 [03:18<00:55,  6.13s/it] 80%|████████  | 32/40 [03:24<00:49,  6.13s/it] 82%|████████▎ | 33/40 [03:34<00:51,  7.32s/it] 85%|████████▌ | 34/40 [03:40<00:41,  6.96s/it] 88%|████████▊ | 35/40 [03:46<00:33,  6.63s/it] 90%|█████████ | 36/40 [03:52<00:25,  6.45s/it] 92%|█████████▎| 37/40 [03:58<00:18,  6.31s/it] 95%|█████████▌| 38/40 [04:04<00:12,  6.17s/it] 98%|█████████▊| 39/40 [04:10<00:06,  6.19s/it]100%|██████████| 40/40 [04:16<00:00,  6.01s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:16<00:00,  6.01s/it]100%|██████████| 40/40 [04:16<00:00,  6.41s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 256.3019, 'train_samples_per_second': 10.944, 'train_steps_per_second': 0.156, 'train_loss': 0.7464151382446289, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:05,  6.29s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:18<03:54,  6.33s/it] 10%|█         | 4/40 [00:25<03:46,  6.29s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.34s/it] 15%|█▌        | 6/40 [00:37<03:30,  6.21s/it] 18%|█▊        | 7/40 [00:43<03:26,  6.24s/it] 20%|██        | 8/40 [00:50<03:19,  6.23s/it] 22%|██▎       | 9/40 [01:00<03:57,  7.68s/it] 25%|██▌       | 10/40 [01:07<03:37,  7.25s/it] 28%|██▊       | 11/40 [01:13<03:22,  6.99s/it] 30%|███       | 12/40 [01:19<03:08,  6.73s/it] 32%|███▎      | 13/40 [01:25<02:57,  6.57s/it] 35%|███▌      | 14/40 [01:32<02:48,  6.50s/it] 38%|███▊      | 15/40 [01:38<02:39,  6.39s/it] 40%|████      | 16/40 [01:44<02:34,  6.42s/it] 42%|████▎     | 17/40 [01:55<02:59,  7.79s/it] 45%|████▌     | 18/40 [02:02<02:40,  7.32s/it] 48%|████▊     | 19/40 [02:08<02:25,  6.94s/it] 50%|█████     | 20/40 [02:14<02:16,  6.83s/it] 52%|█████▎    | 21/40 [02:20<02:05,  6.62s/it] 55%|█████▌    | 22/40 [02:27<01:58,  6.59s/it] 57%|█████▊    | 23/40 [02:33<01:50,  6.48s/it] 60%|██████    | 24/40 [02:39<01:41,  6.37s/it] 62%|██████▎   | 25/40 [02:50<01:54,  7.65s/it] 65%|██████▌   | 26/40 [02:56<01:40,  7.19s/it] 68%|██████▊   | 27/40 [03:02<01:29,  6.90s/it] 70%|███████   | 28/40 [03:08<01:19,  6.66s/it] 72%|███████▎  | 29/40 [03:15<01:12,  6.61s/it] 75%|███████▌  | 30/40 [03:21<01:04,  6.49s/it] 78%|███████▊  | 31/40 [03:27<00:57,  6.37s/it] 80%|████████  | 32/40 [03:34<00:51,  6.40s/it] 82%|████████▎ | 33/40 [03:45<00:55,  7.94s/it] 85%|████████▌ | 34/40 [03:52<00:45,  7.60s/it] 88%|████████▊ | 35/40 [03:59<00:36,  7.30s/it] 90%|█████████ | 36/40 [04:05<00:28,  7.18s/it] 92%|█████████▎| 37/40 [04:12<00:21,  7.03s/it] 95%|█████████▌| 38/40 [04:19<00:13,  6.89s/it] 98%|█████████▊| 39/40 [04:26<00:06,  6.95s/it]100%|██████████| 40/40 [04:32<00:00,  6.81s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:32<00:00,  6.81s/it]100%|██████████| 40/40 [04:32<00:00,  6.82s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 272.7747, 'train_samples_per_second': 10.283, 'train_steps_per_second': 0.147, 'train_loss': 0.750673484802246, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:45,  8.85s/it]  5%|▌         | 2/40 [00:18<05:43,  9.03s/it]  8%|▊         | 3/40 [00:27<05:34,  9.05s/it] 10%|█         | 4/40 [00:36<05:24,  9.00s/it] 12%|█▎        | 5/40 [00:45<05:16,  9.04s/it] 15%|█▌        | 6/40 [00:53<05:00,  8.85s/it] 18%|█▊        | 7/40 [01:02<04:53,  8.90s/it] 20%|██        | 8/40 [01:11<04:46,  8.95s/it] 22%|██▎       | 9/40 [01:27<05:39, 10.97s/it] 25%|██▌       | 10/40 [01:36<05:10, 10.37s/it] 28%|██▊       | 11/40 [01:45<04:47,  9.93s/it] 30%|███       | 12/40 [01:53<04:26,  9.51s/it] 32%|███▎      | 13/40 [02:02<04:08,  9.20s/it] 35%|███▌      | 14/40 [02:10<03:56,  9.11s/it] 38%|███▊      | 15/40 [02:19<03:43,  8.95s/it] 40%|████      | 16/40 [02:28<03:36,  9.00s/it] 42%|████▎     | 17/40 [02:43<04:09, 10.83s/it] 45%|████▌     | 18/40 [02:52<03:44, 10.20s/it] 48%|████▊     | 19/40 [03:00<03:23,  9.67s/it] 50%|█████     | 20/40 [03:09<03:09,  9.49s/it] 52%|█████▎    | 21/40 [03:18<02:53,  9.15s/it] 55%|█████▌    | 22/40 [03:27<02:44,  9.13s/it] 57%|█████▊    | 23/40 [03:36<02:32,  8.97s/it] 60%|██████    | 24/40 [03:44<02:21,  8.84s/it] 62%|██████▎   | 25/40 [03:59<02:40, 10.68s/it] 65%|██████▌   | 26/40 [04:08<02:20, 10.04s/it] 68%|██████▊   | 27/40 [04:16<02:05,  9.63s/it] 70%|███████   | 28/40 [04:25<01:51,  9.28s/it] 72%|███████▎  | 29/40 [04:34<01:42,  9.30s/it] 75%|███████▌  | 30/40 [04:43<01:32,  9.20s/it] 78%|███████▊  | 31/40 [04:52<01:21,  9.02s/it] 80%|████████  | 32/40 [05:01<01:12,  9.06s/it] 82%|████████▎ | 33/40 [05:15<01:15, 10.74s/it] 85%|████████▌ | 34/40 [05:24<01:01, 10.22s/it] 88%|████████▊ | 35/40 [05:33<00:48,  9.71s/it] 90%|█████████ | 36/40 [05:42<00:37,  9.44s/it] 92%|█████████▎| 37/40 [05:51<00:27,  9.22s/it] 95%|█████████▌| 38/40 [05:59<00:17,  9.00s/it] 98%|█████████▊| 39/40 [06:08<00:08,  9.00s/it]100%|██████████| 40/40 [06:16<00:00,  8.77s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:16<00:00,  8.77s/it]100%|██████████| 40/40 [06:16<00:00,  9.42s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 376.7075, 'train_samples_per_second': 7.446, 'train_steps_per_second': 0.106, 'train_loss': 0.7577497482299804, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_8.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:31, 10.04s/it]  5%|▌         | 2/40 [00:20<06:27, 10.19s/it]  8%|▊         | 3/40 [00:30<06:16, 10.18s/it] 10%|█         | 4/40 [00:40<06:05, 10.14s/it] 12%|█▎        | 5/40 [00:50<05:57, 10.23s/it] 15%|█▌        | 6/40 [01:00<05:40, 10.02s/it] 18%|█▊        | 7/40 [01:11<05:35, 10.17s/it] 20%|██        | 8/40 [01:20<05:21, 10.05s/it] 22%|██▎       | 9/40 [01:38<06:22, 12.35s/it] 25%|██▌       | 10/40 [01:48<05:48, 11.63s/it] 28%|██▊       | 11/40 [01:58<05:25, 11.22s/it] 30%|███       | 12/40 [02:08<05:06, 10.93s/it] 32%|███▎      | 13/40 [02:19<04:50, 10.74s/it] 35%|███▌      | 14/40 [02:29<04:36, 10.64s/it] 38%|███▊      | 15/40 [02:39<04:21, 10.45s/it] 40%|████      | 16/40 [02:50<04:12, 10.53s/it] 42%|████▎     | 17/40 [03:08<04:54, 12.82s/it] 45%|████▌     | 18/40 [03:18<04:26, 12.10s/it] 48%|████▊     | 19/40 [03:28<04:01, 11.48s/it] 50%|█████     | 20/40 [03:39<03:44, 11.24s/it] 52%|█████▎    | 21/40 [03:49<03:27, 10.94s/it] 55%|█████▌    | 22/40 [04:00<03:14, 10.78s/it] 57%|█████▊    | 23/40 [04:10<02:58, 10.51s/it] 60%|██████    | 24/40 [04:20<02:46, 10.40s/it] 62%|██████▎   | 25/40 [04:38<03:09, 12.63s/it] 65%|██████▌   | 26/40 [04:47<02:45, 11.81s/it] 68%|██████▊   | 27/40 [04:58<02:26, 11.28s/it] 70%|███████   | 28/40 [05:07<02:09, 10.82s/it] 72%|███████▎  | 29/40 [05:18<01:58, 10.80s/it] 75%|███████▌  | 30/40 [05:28<01:46, 10.63s/it] 78%|███████▊  | 31/40 [05:38<01:33, 10.44s/it] 80%|████████  | 32/40 [05:49<01:23, 10.42s/it] 82%|████████▎ | 33/40 [06:06<01:27, 12.48s/it] 85%|████████▌ | 34/40 [06:16<01:11, 11.84s/it] 88%|████████▊ | 35/40 [06:26<00:56, 11.22s/it] 90%|█████████ | 36/40 [06:36<00:43, 10.87s/it] 92%|█████████▎| 37/40 [06:46<00:31, 10.61s/it] 95%|█████████▌| 38/40 [06:56<00:20, 10.41s/it] 98%|█████████▊| 39/40 [07:06<00:10, 10.36s/it]100%|██████████| 40/40 [07:16<00:00, 10.10s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:16<00:00, 10.10s/it]100%|██████████| 40/40 [07:16<00:00, 10.91s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 436.2727, 'train_samples_per_second': 6.429, 'train_steps_per_second': 0.092, 'train_loss': 0.7824322700500488, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:09,  6.40s/it]  5%|▌         | 2/40 [00:12<04:04,  6.42s/it]  8%|▊         | 3/40 [00:19<03:59,  6.47s/it] 10%|█         | 4/40 [00:26<03:55,  6.54s/it] 12%|█▎        | 5/40 [00:32<03:49,  6.56s/it] 15%|█▌        | 6/40 [00:39<03:41,  6.52s/it] 18%|█▊        | 7/40 [00:45<03:35,  6.53s/it] 20%|██        | 8/40 [00:51<03:26,  6.46s/it] 22%|██▎       | 9/40 [01:03<04:06,  7.97s/it] 25%|██▌       | 10/40 [01:09<03:44,  7.49s/it] 28%|██▊       | 11/40 [01:16<03:28,  7.19s/it] 30%|███       | 12/40 [01:22<03:12,  6.89s/it] 32%|███▎      | 13/40 [01:28<03:00,  6.68s/it] 35%|███▌      | 14/40 [01:35<02:55,  6.75s/it] 38%|███▊      | 15/40 [01:42<02:48,  6.73s/it] 40%|████      | 16/40 [01:48<02:42,  6.76s/it] 42%|████▎     | 17/40 [02:00<03:08,  8.18s/it] 45%|████▌     | 18/40 [02:06<02:49,  7.69s/it] 48%|████▊     | 19/40 [02:13<02:35,  7.41s/it] 50%|█████     | 20/40 [02:20<02:22,  7.14s/it] 52%|█████▎    | 21/40 [02:26<02:11,  6.92s/it] 55%|█████▌    | 22/40 [02:33<02:04,  6.90s/it] 57%|█████▊    | 23/40 [02:39<01:53,  6.67s/it] 60%|██████    | 24/40 [02:45<01:44,  6.54s/it] 62%|██████▎   | 25/40 [02:56<01:57,  7.81s/it] 65%|██████▌   | 26/40 [03:03<01:43,  7.38s/it] 68%|██████▊   | 27/40 [03:09<01:32,  7.09s/it] 70%|███████   | 28/40 [03:16<01:23,  6.95s/it] 72%|███████▎  | 29/40 [03:22<01:15,  6.84s/it] 75%|███████▌  | 30/40 [03:28<01:06,  6.63s/it] 78%|███████▊  | 31/40 [03:35<00:58,  6.52s/it] 80%|████████  | 32/40 [03:41<00:51,  6.48s/it] 82%|████████▎ | 33/40 [03:52<00:54,  7.84s/it] 85%|████████▌ | 34/40 [03:58<00:44,  7.43s/it] 88%|████████▊ | 35/40 [04:05<00:35,  7.16s/it] 90%|█████████ | 36/40 [04:11<00:27,  6.89s/it] 92%|█████████▎| 37/40 [04:18<00:20,  6.71s/it] 95%|█████████▌| 38/40 [04:24<00:13,  6.61s/it] 98%|█████████▊| 39/40 [04:30<00:06,  6.57s/it]100%|██████████| 40/40 [04:37<00:00,  6.53s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:37<00:00,  6.53s/it]100%|██████████| 40/40 [04:37<00:00,  6.93s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 277.3186, 'train_samples_per_second': 10.115, 'train_steps_per_second': 0.144, 'train_loss': 0.7462547302246094, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:07,  6.35s/it]  5%|▌         | 2/40 [00:12<04:02,  6.37s/it]  8%|▊         | 3/40 [00:19<03:58,  6.45s/it] 10%|█         | 4/40 [00:25<03:51,  6.43s/it] 12%|█▎        | 5/40 [00:32<03:47,  6.50s/it] 15%|█▌        | 6/40 [00:38<03:42,  6.55s/it] 18%|█▊        | 7/40 [00:45<03:34,  6.50s/it] 20%|██        | 8/40 [00:51<03:26,  6.47s/it] 22%|██▎       | 9/40 [01:03<04:07,  7.99s/it] 25%|██▌       | 10/40 [01:09<03:46,  7.54s/it] 28%|██▊       | 11/40 [01:16<03:29,  7.24s/it] 30%|███       | 12/40 [01:23<03:20,  7.15s/it] 32%|███▎      | 13/40 [01:29<03:08,  6.98s/it] 35%|███▌      | 14/40 [01:36<03:00,  6.93s/it] 38%|███▊      | 15/40 [01:43<02:51,  6.85s/it] 40%|████      | 16/40 [01:50<02:44,  6.84s/it] 42%|████▎     | 17/40 [02:01<03:11,  8.31s/it] 45%|████▌     | 18/40 [02:08<02:52,  7.86s/it] 48%|████▊     | 19/40 [02:14<02:36,  7.43s/it] 50%|█████     | 20/40 [02:21<02:24,  7.22s/it] 52%|█████▎    | 21/40 [02:28<02:13,  7.01s/it] 55%|█████▌    | 22/40 [02:35<02:05,  6.95s/it] 57%|█████▊    | 23/40 [02:41<01:54,  6.76s/it] 60%|██████    | 24/40 [02:47<01:47,  6.69s/it] 62%|██████▎   | 25/40 [02:58<01:59,  7.98s/it] 65%|██████▌   | 26/40 [03:05<01:45,  7.53s/it] 68%|██████▊   | 27/40 [03:11<01:33,  7.21s/it] 70%|███████   | 28/40 [03:18<01:25,  7.09s/it] 72%|███████▎  | 29/40 [03:25<01:16,  6.97s/it] 75%|███████▌  | 30/40 [03:31<01:07,  6.77s/it] 78%|███████▊  | 31/40 [03:37<00:59,  6.60s/it] 80%|████████  | 32/40 [03:44<00:52,  6.58s/it] 82%|████████▎ | 33/40 [03:55<00:55,  7.95s/it] 85%|████████▌ | 34/40 [04:02<00:45,  7.55s/it] 88%|████████▊ | 35/40 [04:08<00:35,  7.19s/it] 90%|█████████ | 36/40 [04:15<00:28,  7.05s/it] 92%|█████████▎| 37/40 [04:21<00:20,  6.81s/it] 95%|█████████▌| 38/40 [04:27<00:13,  6.65s/it] 98%|█████████▊| 39/40 [04:34<00:06,  6.60s/it]100%|██████████| 40/40 [04:40<00:00,  6.51s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:40<00:00,  6.51s/it]100%|██████████| 40/40 [04:40<00:00,  7.01s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 280.5005, 'train_samples_per_second': 10.0, 'train_steps_per_second': 0.143, 'train_loss': 0.750364875793457, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:53,  9.06s/it]  5%|▌         | 2/40 [00:18<05:44,  9.06s/it]  8%|▊         | 3/40 [00:27<05:33,  9.01s/it] 10%|█         | 4/40 [00:35<05:21,  8.93s/it] 12%|█▎        | 5/40 [00:44<05:14,  8.99s/it] 15%|█▌        | 6/40 [00:53<05:03,  8.94s/it] 18%|█▊        | 7/40 [01:02<04:56,  8.99s/it] 20%|██        | 8/40 [01:11<04:44,  8.89s/it] 22%|██▎       | 9/40 [01:27<05:40, 10.99s/it] 25%|██▌       | 10/40 [01:36<05:09, 10.33s/it] 28%|██▊       | 11/40 [01:45<04:48,  9.96s/it] 30%|███       | 12/40 [01:53<04:28,  9.60s/it] 32%|███▎      | 13/40 [02:02<04:11,  9.31s/it] 35%|███▌      | 14/40 [02:11<04:01,  9.30s/it] 38%|███▊      | 15/40 [02:20<03:49,  9.19s/it] 40%|████      | 16/40 [02:30<03:41,  9.22s/it] 42%|████▎     | 17/40 [02:46<04:19, 11.29s/it] 45%|████▌     | 18/40 [02:55<03:52, 10.58s/it] 48%|████▊     | 19/40 [03:03<03:28,  9.93s/it] 50%|█████     | 20/40 [03:12<03:13,  9.68s/it] 52%|█████▎    | 21/40 [03:21<02:58,  9.37s/it] 55%|█████▌    | 22/40 [03:30<02:47,  9.30s/it] 57%|█████▊    | 23/40 [03:38<02:33,  9.01s/it] 60%|██████    | 24/40 [03:47<02:21,  8.84s/it] 62%|██████▎   | 25/40 [04:01<02:38, 10.59s/it] 65%|██████▌   | 26/40 [04:10<02:18,  9.92s/it] 68%|██████▊   | 27/40 [04:18<02:03,  9.53s/it] 70%|███████   | 28/40 [04:27<01:51,  9.31s/it] 72%|███████▎  | 29/40 [04:36<01:41,  9.23s/it] 75%|███████▌  | 30/40 [04:45<01:29,  8.98s/it] 78%|███████▊  | 31/40 [04:53<01:18,  8.77s/it] 80%|████████  | 32/40 [05:02<01:10,  8.76s/it] 82%|████████▎ | 33/40 [05:16<01:13, 10.49s/it] 85%|████████▌ | 34/40 [05:25<00:59,  9.95s/it] 88%|████████▊ | 35/40 [05:33<00:47,  9.55s/it] 90%|█████████ | 36/40 [05:42<00:37,  9.26s/it] 92%|█████████▎| 37/40 [05:50<00:26,  9.00s/it] 95%|█████████▌| 38/40 [05:59<00:17,  8.99s/it] 98%|█████████▊| 39/40 [06:09<00:09,  9.07s/it]100%|██████████| 40/40 [06:18<00:00,  9.06s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:18<00:00,  9.06s/it]100%|██████████| 40/40 [06:18<00:00,  9.45s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 378.1809, 'train_samples_per_second': 7.417, 'train_steps_per_second': 0.106, 'train_loss': 0.7576642990112304, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_3.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:08,  9.44s/it]  5%|▌         | 2/40 [00:18<05:58,  9.44s/it]  8%|▊         | 3/40 [00:28<05:56,  9.62s/it] 10%|█         | 4/40 [00:38<05:52,  9.80s/it] 12%|█▎        | 5/40 [00:49<05:56, 10.17s/it] 15%|█▌        | 6/40 [00:59<05:45, 10.15s/it] 18%|█▊        | 7/40 [01:09<05:34, 10.14s/it] 20%|██        | 8/40 [01:19<05:22, 10.07s/it] 22%|██▎       | 9/40 [01:36<06:19, 12.24s/it] 25%|██▌       | 10/40 [01:46<05:46, 11.56s/it] 28%|██▊       | 11/40 [01:56<05:21, 11.08s/it] 30%|███       | 12/40 [02:06<04:58, 10.66s/it] 32%|███▎      | 13/40 [02:16<04:39, 10.37s/it] 35%|███▌      | 14/40 [02:26<04:27, 10.29s/it] 38%|███▊      | 15/40 [02:36<04:13, 10.15s/it] 40%|████      | 16/40 [02:46<04:03, 10.14s/it] 42%|████▎     | 17/40 [03:03<04:42, 12.27s/it] 45%|████▌     | 18/40 [03:13<04:14, 11.55s/it] 48%|████▊     | 19/40 [03:22<03:50, 10.97s/it] 50%|█████     | 20/40 [03:32<03:33, 10.68s/it] 52%|█████▎    | 21/40 [03:42<03:17, 10.37s/it] 55%|█████▌    | 22/40 [03:53<03:06, 10.37s/it] 57%|█████▊    | 23/40 [04:02<02:52, 10.14s/it] 60%|██████    | 24/40 [04:12<02:41, 10.07s/it] 62%|██████▎   | 25/40 [04:29<03:03, 12.25s/it] 65%|██████▌   | 26/40 [04:39<02:40, 11.47s/it] 68%|██████▊   | 27/40 [04:49<02:23, 11.02s/it] 70%|███████   | 28/40 [04:59<02:08, 10.71s/it] 72%|███████▎  | 29/40 [05:09<01:55, 10.52s/it] 75%|███████▌  | 30/40 [05:19<01:42, 10.22s/it] 78%|███████▊  | 31/40 [05:28<01:29, 10.00s/it] 80%|████████  | 32/40 [05:38<01:20, 10.02s/it] 82%|████████▎ | 33/40 [05:55<01:24, 12.10s/it] 85%|████████▌ | 34/40 [06:05<01:08, 11.45s/it] 88%|████████▊ | 35/40 [06:15<00:54, 10.98s/it] 90%|█████████ | 36/40 [06:25<00:42, 10.62s/it] 92%|█████████▎| 37/40 [06:34<00:30, 10.25s/it] 95%|█████████▌| 38/40 [06:44<00:20, 10.23s/it] 98%|█████████▊| 39/40 [06:55<00:10, 10.25s/it]100%|██████████| 40/40 [07:05<00:00, 10.20s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:05<00:00, 10.20s/it]100%|██████████| 40/40 [07:05<00:00, 10.63s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_3/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_3/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_3/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 425.1103, 'train_samples_per_second': 6.598, 'train_steps_per_second': 0.094, 'train_loss': 0.7828170776367187, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:15,  6.56s/it]  5%|▌         | 2/40 [00:12<04:00,  6.33s/it]  8%|▊         | 3/40 [00:19<03:55,  6.35s/it] 10%|█         | 4/40 [00:25<03:48,  6.36s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.34s/it] 15%|█▌        | 6/40 [00:38<03:34,  6.30s/it] 18%|█▊        | 7/40 [00:44<03:24,  6.20s/it] 20%|██        | 8/40 [00:50<03:18,  6.21s/it] 22%|██▎       | 9/40 [01:01<03:59,  7.72s/it] 25%|██▌       | 10/40 [01:07<03:39,  7.33s/it] 28%|██▊       | 11/40 [01:14<03:24,  7.04s/it] 30%|███       | 12/40 [01:20<03:12,  6.87s/it] 32%|███▎      | 13/40 [01:26<03:00,  6.69s/it] 35%|███▌      | 14/40 [01:33<02:53,  6.68s/it] 38%|███▊      | 15/40 [01:39<02:45,  6.61s/it] 40%|████      | 16/40 [01:46<02:37,  6.55s/it] 42%|████▎     | 17/40 [01:57<03:04,  8.01s/it] 45%|████▌     | 18/40 [02:03<02:44,  7.47s/it] 48%|████▊     | 19/40 [02:10<02:30,  7.15s/it] 50%|█████     | 20/40 [02:17<02:19,  7.00s/it] 52%|█████▎    | 21/40 [02:23<02:11,  6.90s/it] 55%|█████▌    | 22/40 [02:30<02:04,  6.91s/it] 57%|█████▊    | 23/40 [02:37<01:55,  6.80s/it] 60%|██████    | 24/40 [02:43<01:47,  6.70s/it] 62%|██████▎   | 25/40 [02:55<02:01,  8.10s/it] 65%|██████▌   | 26/40 [03:01<01:47,  7.64s/it] 68%|██████▊   | 27/40 [03:08<01:35,  7.33s/it] 70%|███████   | 28/40 [03:14<01:24,  7.05s/it] 72%|███████▎  | 29/40 [03:21<01:15,  6.86s/it] 75%|███████▌  | 30/40 [03:27<01:07,  6.73s/it] 78%|███████▊  | 31/40 [03:33<00:59,  6.57s/it] 80%|████████  | 32/40 [03:40<00:52,  6.60s/it] 82%|████████▎ | 33/40 [03:51<00:55,  7.98s/it] 85%|████████▌ | 34/40 [03:58<00:45,  7.62s/it] 88%|████████▊ | 35/40 [04:05<00:36,  7.36s/it] 90%|█████████ | 36/40 [04:11<00:28,  7.13s/it] 92%|█████████▎| 37/40 [04:18<00:20,  6.92s/it] 95%|█████████▌| 38/40 [04:24<00:13,  6.78s/it] 98%|█████████▊| 39/40 [04:31<00:06,  6.79s/it]100%|██████████| 40/40 [04:38<00:00,  6.89s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:38<00:00,  6.89s/it]100%|██████████| 40/40 [04:38<00:00,  6.96s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 278.4434, 'train_samples_per_second': 10.074, 'train_steps_per_second': 0.144, 'train_loss': 0.7466821670532227, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:20,  6.69s/it]  5%|▌         | 2/40 [00:13<04:10,  6.59s/it]  8%|▊         | 3/40 [00:19<04:03,  6.58s/it] 10%|█         | 4/40 [00:26<03:58,  6.63s/it] 12%|█▎        | 5/40 [00:33<03:50,  6.60s/it] 15%|█▌        | 6/40 [00:39<03:42,  6.54s/it] 18%|█▊        | 7/40 [00:45<03:31,  6.41s/it] 20%|██        | 8/40 [00:51<03:23,  6.37s/it] 22%|██▎       | 9/40 [01:03<04:05,  7.91s/it] 25%|██▌       | 10/40 [01:09<03:43,  7.47s/it] 28%|██▊       | 11/40 [01:16<03:27,  7.14s/it] 30%|███       | 12/40 [01:22<03:13,  6.91s/it] 32%|███▎      | 13/40 [01:28<03:01,  6.72s/it] 35%|███▌      | 14/40 [01:35<02:52,  6.62s/it] 38%|███▊      | 15/40 [01:41<02:41,  6.46s/it] 40%|████      | 16/40 [01:47<02:34,  6.43s/it] 42%|████▎     | 17/40 [01:58<03:02,  7.91s/it] 45%|████▌     | 18/40 [02:05<02:46,  7.56s/it] 48%|████▊     | 19/40 [02:11<02:31,  7.20s/it] 50%|█████     | 20/40 [02:18<02:20,  7.01s/it] 52%|█████▎    | 21/40 [02:24<02:09,  6.83s/it] 55%|█████▌    | 22/40 [02:31<02:01,  6.74s/it] 57%|█████▊    | 23/40 [02:37<01:51,  6.58s/it] 60%|██████    | 24/40 [02:43<01:43,  6.44s/it] 62%|██████▎   | 25/40 [02:54<01:56,  7.79s/it] 65%|██████▌   | 26/40 [03:00<01:42,  7.30s/it] 68%|██████▊   | 27/40 [03:07<01:32,  7.10s/it] 70%|███████   | 28/40 [03:13<01:22,  6.87s/it] 72%|███████▎  | 29/40 [03:20<01:13,  6.72s/it] 75%|███████▌  | 30/40 [03:26<01:06,  6.61s/it] 78%|███████▊  | 31/40 [03:32<00:58,  6.47s/it] 80%|████████  | 32/40 [03:39<00:51,  6.50s/it] 82%|████████▎ | 33/40 [03:50<00:54,  7.78s/it] 85%|████████▌ | 34/40 [03:56<00:44,  7.40s/it] 88%|████████▊ | 35/40 [04:03<00:35,  7.10s/it] 90%|█████████ | 36/40 [04:09<00:27,  6.89s/it] 92%|█████████▎| 37/40 [04:15<00:20,  6.69s/it] 95%|█████████▌| 38/40 [04:21<00:13,  6.56s/it] 98%|█████████▊| 39/40 [04:28<00:06,  6.56s/it]100%|██████████| 40/40 [04:35<00:00,  6.56s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:35<00:00,  6.56s/it]100%|██████████| 40/40 [04:35<00:00,  6.88s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 275.0088, 'train_samples_per_second': 10.2, 'train_steps_per_second': 0.145, 'train_loss': 0.7505592346191406, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:58,  9.19s/it]  5%|▌         | 2/40 [00:17<05:36,  8.86s/it]  8%|▊         | 3/40 [00:26<05:29,  8.89s/it] 10%|█         | 4/40 [00:35<05:22,  8.96s/it] 12%|█▎        | 5/40 [00:44<05:13,  8.97s/it] 15%|█▌        | 6/40 [00:53<05:02,  8.90s/it] 18%|█▊        | 7/40 [01:01<04:47,  8.70s/it] 20%|██        | 8/40 [01:10<04:37,  8.68s/it] 22%|██▎       | 9/40 [01:25<05:34, 10.79s/it] 25%|██▌       | 10/40 [01:34<05:07, 10.26s/it] 28%|██▊       | 11/40 [01:43<04:44,  9.82s/it] 30%|███       | 12/40 [01:52<04:25,  9.48s/it] 32%|███▎      | 13/40 [02:01<04:09,  9.26s/it] 35%|███▌      | 14/40 [02:10<03:58,  9.18s/it] 38%|███▊      | 15/40 [02:18<03:44,  8.99s/it] 40%|████      | 16/40 [02:27<03:35,  8.97s/it] 42%|████▎     | 17/40 [02:43<04:14, 11.05s/it] 45%|████▌     | 18/40 [02:52<03:46, 10.30s/it] 48%|████▊     | 19/40 [03:00<03:26,  9.83s/it] 50%|█████     | 20/40 [03:10<03:12,  9.61s/it] 52%|█████▎    | 21/40 [03:18<02:58,  9.38s/it] 55%|█████▌    | 22/40 [03:28<02:47,  9.31s/it] 57%|█████▊    | 23/40 [03:36<02:35,  9.13s/it] 60%|██████    | 24/40 [03:45<02:22,  8.94s/it] 62%|██████▎   | 25/40 [04:00<02:42, 10.81s/it] 65%|██████▌   | 26/40 [04:08<02:21, 10.13s/it] 68%|██████▊   | 27/40 [04:18<02:07,  9.82s/it] 70%|███████   | 28/40 [04:27<01:55,  9.59s/it] 72%|███████▎  | 29/40 [04:36<01:43,  9.41s/it] 75%|███████▌  | 30/40 [04:45<01:33,  9.31s/it] 78%|███████▊  | 31/40 [04:54<01:22,  9.20s/it] 80%|████████  | 32/40 [05:03<01:14,  9.32s/it] 82%|████████▎ | 33/40 [05:19<01:19, 11.29s/it] 85%|████████▌ | 34/40 [05:28<01:03, 10.66s/it] 88%|████████▊ | 35/40 [05:38<00:51, 10.28s/it] 90%|█████████ | 36/40 [05:47<00:39,  9.96s/it] 92%|█████████▎| 37/40 [05:56<00:28,  9.65s/it] 95%|█████████▌| 38/40 [06:05<00:18,  9.42s/it] 98%|█████████▊| 39/40 [06:14<00:09,  9.45s/it]100%|██████████| 40/40 [06:24<00:00,  9.47s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:24<00:00,  9.47s/it]100%|██████████| 40/40 [06:24<00:00,  9.61s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 384.2059, 'train_samples_per_second': 7.301, 'train_steps_per_second': 0.104, 'train_loss': 0.7576536178588867, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_5.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:50, 10.53s/it]  5%|▌         | 2/40 [00:20<06:28, 10.22s/it]  8%|▊         | 3/40 [00:30<06:17, 10.20s/it] 10%|█         | 4/40 [00:40<06:08, 10.24s/it] 12%|█▎        | 5/40 [00:51<06:00, 10.31s/it] 15%|█▌        | 6/40 [01:01<05:45, 10.17s/it] 18%|█▊        | 7/40 [01:10<05:27,  9.92s/it] 20%|██        | 8/40 [01:20<05:18,  9.95s/it] 22%|██▎       | 9/40 [01:38<06:22, 12.34s/it] 25%|██▌       | 10/40 [01:48<05:51, 11.71s/it] 28%|██▊       | 11/40 [01:58<05:23, 11.16s/it] 30%|███       | 12/40 [02:08<05:02, 10.82s/it] 32%|███▎      | 13/40 [02:18<04:45, 10.56s/it] 35%|███▌      | 14/40 [02:28<04:32, 10.48s/it] 38%|███▊      | 15/40 [02:38<04:15, 10.23s/it] 40%|████      | 16/40 [02:48<04:04, 10.19s/it] 42%|████▎     | 17/40 [03:06<04:49, 12.58s/it] 45%|████▌     | 18/40 [03:16<04:18, 11.75s/it] 48%|████▊     | 19/40 [03:26<03:55, 11.20s/it] 50%|█████     | 20/40 [03:36<03:39, 10.97s/it] 52%|█████▎    | 21/40 [03:46<03:22, 10.68s/it] 55%|█████▌    | 22/40 [03:57<03:09, 10.54s/it] 57%|█████▊    | 23/40 [04:07<02:56, 10.40s/it] 60%|██████    | 24/40 [04:16<02:42, 10.15s/it] 62%|██████▎   | 25/40 [04:33<03:02, 12.19s/it] 65%|██████▌   | 26/40 [04:43<02:40, 11.44s/it] 68%|██████▊   | 27/40 [04:53<02:24, 11.08s/it] 70%|███████   | 28/40 [05:03<02:09, 10.82s/it] 72%|███████▎  | 29/40 [05:13<01:56, 10.58s/it] 75%|███████▌  | 30/40 [05:24<01:44, 10.49s/it] 78%|███████▊  | 31/40 [05:33<01:32, 10.26s/it] 80%|████████  | 32/40 [05:43<01:21, 10.19s/it] 82%|████████▎ | 33/40 [06:00<01:25, 12.19s/it] 85%|████████▌ | 34/40 [06:10<01:08, 11.50s/it] 88%|████████▊ | 35/40 [06:20<00:55, 11.10s/it] 90%|█████████ | 36/40 [06:30<00:42, 10.74s/it] 92%|█████████▎| 37/40 [06:40<00:31, 10.42s/it] 95%|█████████▌| 38/40 [06:50<00:20, 10.21s/it] 98%|█████████▊| 39/40 [07:00<00:10, 10.35s/it]100%|██████████| 40/40 [07:11<00:00, 10.35s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:11<00:00, 10.35s/it]100%|██████████| 40/40 [07:11<00:00, 10.78s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 431.1575, 'train_samples_per_second': 6.506, 'train_steps_per_second': 0.093, 'train_loss': 0.7828004837036133, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:13,  6.51s/it]  5%|▌         | 2/40 [00:13<04:14,  6.71s/it]  8%|▊         | 3/40 [00:19<04:02,  6.56s/it] 10%|█         | 4/40 [00:26<03:57,  6.61s/it] 12%|█▎        | 5/40 [00:33<03:52,  6.63s/it] 15%|█▌        | 6/40 [00:39<03:43,  6.57s/it] 18%|█▊        | 7/40 [00:45<03:35,  6.52s/it] 20%|██        | 8/40 [00:52<03:30,  6.58s/it] 22%|██▎       | 9/40 [01:00<03:32,  6.87s/it] 25%|██▌       | 10/40 [01:06<03:23,  6.77s/it] 28%|██▊       | 11/40 [01:13<03:14,  6.69s/it] 30%|███       | 12/40 [01:19<03:03,  6.56s/it] 32%|███▎      | 13/40 [01:26<03:00,  6.68s/it] 35%|███▌      | 14/40 [01:33<02:57,  6.81s/it] 38%|███▊      | 15/40 [01:40<02:52,  6.90s/it] 40%|████      | 16/40 [01:47<02:46,  6.95s/it] 42%|████▎     | 17/40 [01:55<02:45,  7.20s/it] 45%|████▌     | 18/40 [02:02<02:37,  7.17s/it] 48%|████▊     | 19/40 [02:09<02:30,  7.18s/it] 50%|█████     | 20/40 [02:16<02:22,  7.11s/it] 52%|█████▎    | 21/40 [02:23<02:14,  7.09s/it] 55%|█████▌    | 22/40 [02:30<02:04,  6.93s/it] 57%|█████▊    | 23/40 [02:37<01:57,  6.92s/it] 60%|██████    | 24/40 [02:44<01:51,  6.95s/it] 62%|██████▎   | 25/40 [02:52<01:48,  7.26s/it] 65%|██████▌   | 26/40 [02:59<01:39,  7.14s/it] 68%|██████▊   | 27/40 [03:06<01:32,  7.12s/it] 70%|███████   | 28/40 [03:13<01:25,  7.10s/it] 72%|███████▎  | 29/40 [03:20<01:17,  7.04s/it] 75%|███████▌  | 30/40 [03:27<01:11,  7.12s/it] 78%|███████▊  | 31/40 [03:34<01:03,  7.11s/it] 80%|████████  | 32/40 [03:41<00:55,  6.98s/it] 82%|████████▎ | 33/40 [03:48<00:49,  7.12s/it] 85%|████████▌ | 34/40 [03:55<00:42,  7.07s/it] 88%|████████▊ | 35/40 [04:02<00:35,  7.03s/it] 90%|█████████ | 36/40 [04:09<00:27,  6.98s/it] 92%|█████████▎| 37/40 [04:16<00:20,  6.89s/it] 95%|█████████▌| 38/40 [04:23<00:13,  6.90s/it] 98%|█████████▊| 39/40 [04:29<00:06,  6.88s/it]100%|██████████| 40/40 [04:36<00:00,  6.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:36<00:00,  6.82s/it]100%|██████████| 40/40 [04:36<00:00,  6.91s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 276.5556, 'train_samples_per_second': 9.419, 'train_steps_per_second': 0.145, 'train_loss': 0.6987804412841797, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:27,  6.86s/it]  5%|▌         | 2/40 [00:14<04:27,  7.04s/it]  8%|▊         | 3/40 [00:20<04:12,  6.83s/it] 10%|█         | 4/40 [00:27<04:05,  6.83s/it] 12%|█▎        | 5/40 [00:34<03:58,  6.81s/it] 15%|█▌        | 6/40 [00:40<03:47,  6.69s/it] 18%|█▊        | 7/40 [00:47<03:38,  6.61s/it] 20%|██        | 8/40 [00:53<03:30,  6.58s/it] 22%|██▎       | 9/40 [01:01<03:32,  6.85s/it] 25%|██▌       | 10/40 [01:07<03:21,  6.72s/it] 28%|██▊       | 11/40 [01:14<03:13,  6.68s/it] 30%|███       | 12/40 [01:20<03:02,  6.53s/it] 32%|███▎      | 13/40 [01:26<02:57,  6.58s/it] 35%|███▌      | 14/40 [01:33<02:53,  6.67s/it] 38%|███▊      | 15/40 [01:40<02:48,  6.75s/it] 40%|████      | 16/40 [01:47<02:42,  6.77s/it] 42%|████▎     | 17/40 [01:54<02:39,  6.93s/it] 45%|████▌     | 18/40 [02:01<02:31,  6.87s/it] 48%|████▊     | 19/40 [02:08<02:22,  6.79s/it] 50%|█████     | 20/40 [02:15<02:16,  6.82s/it] 52%|█████▎    | 21/40 [02:22<02:09,  6.83s/it] 55%|█████▌    | 22/40 [02:28<02:01,  6.75s/it] 57%|█████▊    | 23/40 [02:35<01:53,  6.70s/it] 60%|██████    | 24/40 [02:41<01:47,  6.71s/it] 62%|██████▎   | 25/40 [02:49<01:45,  7.04s/it] 65%|██████▌   | 26/40 [02:56<01:37,  6.97s/it] 68%|██████▊   | 27/40 [03:03<01:29,  6.90s/it] 70%|███████   | 28/40 [03:09<01:22,  6.85s/it] 72%|███████▎  | 29/40 [03:16<01:14,  6.76s/it] 75%|███████▌  | 30/40 [03:23<01:07,  6.79s/it] 78%|███████▊  | 31/40 [03:30<01:01,  6.79s/it] 80%|████████  | 32/40 [03:36<00:53,  6.71s/it] 82%|████████▎ | 33/40 [03:44<00:48,  6.96s/it] 85%|████████▌ | 34/40 [03:50<00:41,  6.89s/it] 88%|████████▊ | 35/40 [03:57<00:34,  6.83s/it] 90%|█████████ | 36/40 [04:04<00:26,  6.74s/it] 92%|█████████▎| 37/40 [04:10<00:20,  6.69s/it] 95%|█████████▌| 38/40 [04:17<00:13,  6.66s/it] 98%|█████████▊| 39/40 [04:23<00:06,  6.62s/it]100%|██████████| 40/40 [04:30<00:00,  6.59s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:30<00:00,  6.59s/it]100%|██████████| 40/40 [04:30<00:00,  6.76s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 270.3897, 'train_samples_per_second': 9.634, 'train_steps_per_second': 0.148, 'train_loss': 0.7158161163330078, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:50,  8.98s/it]  5%|▌         | 2/40 [00:18<05:58,  9.44s/it]  8%|▊         | 3/40 [00:27<05:42,  9.26s/it] 10%|█         | 4/40 [00:37<05:38,  9.40s/it] 12%|█▎        | 5/40 [00:46<05:29,  9.42s/it] 15%|█▌        | 6/40 [00:55<05:14,  9.25s/it] 18%|█▊        | 7/40 [01:04<05:02,  9.17s/it] 20%|██        | 8/40 [01:14<04:58,  9.34s/it] 22%|██▎       | 9/40 [01:25<05:02,  9.75s/it] 25%|██▌       | 10/40 [01:34<04:49,  9.64s/it] 28%|██▊       | 11/40 [01:44<04:38,  9.60s/it] 30%|███       | 12/40 [01:52<04:23,  9.40s/it] 32%|███▎      | 13/40 [02:02<04:13,  9.38s/it] 35%|███▌      | 14/40 [02:11<04:05,  9.43s/it] 38%|███▊      | 15/40 [02:21<03:57,  9.51s/it] 40%|████      | 16/40 [02:31<03:48,  9.54s/it] 42%|████▎     | 17/40 [02:41<03:48,  9.93s/it] 45%|████▌     | 18/40 [02:51<03:38,  9.92s/it] 48%|████▊     | 19/40 [03:01<03:25,  9.79s/it] 50%|█████     | 20/40 [03:11<03:15,  9.78s/it] 52%|█████▎    | 21/40 [03:20<03:05,  9.78s/it] 55%|█████▌    | 22/40 [03:30<02:52,  9.60s/it] 57%|█████▊    | 23/40 [03:39<02:43,  9.59s/it] 60%|██████    | 24/40 [03:49<02:34,  9.66s/it] 62%|██████▎   | 25/40 [04:00<02:30, 10.05s/it] 65%|██████▌   | 26/40 [04:09<02:16,  9.77s/it] 68%|██████▊   | 27/40 [04:18<02:05,  9.66s/it] 70%|███████   | 28/40 [04:28<01:54,  9.54s/it] 72%|███████▎  | 29/40 [04:37<01:44,  9.48s/it] 75%|███████▌  | 30/40 [04:47<01:35,  9.55s/it] 78%|███████▊  | 31/40 [04:56<01:25,  9.55s/it] 80%|████████  | 32/40 [05:06<01:16,  9.53s/it] 82%|████████▎ | 33/40 [05:16<01:08,  9.80s/it] 85%|████████▌ | 34/40 [05:26<00:58,  9.77s/it] 88%|████████▊ | 35/40 [05:36<00:48,  9.71s/it] 90%|█████████ | 36/40 [05:45<00:38,  9.54s/it] 92%|█████████▎| 37/40 [05:54<00:28,  9.47s/it] 95%|█████████▌| 38/40 [06:03<00:18,  9.43s/it] 98%|█████████▊| 39/40 [06:13<00:09,  9.46s/it]100%|██████████| 40/40 [06:22<00:00,  9.33s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:22<00:00,  9.33s/it]100%|██████████| 40/40 [06:22<00:00,  9.56s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 382.3492, 'train_samples_per_second': 6.813, 'train_steps_per_second': 0.105, 'train_loss': 0.6984512329101562, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_8.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:39, 10.23s/it]  5%|▌         | 2/40 [00:20<06:38, 10.50s/it]  8%|▊         | 3/40 [00:30<06:20, 10.29s/it] 10%|█         | 4/40 [00:41<06:12, 10.36s/it] 12%|█▎        | 5/40 [00:51<06:01, 10.31s/it] 15%|█▌        | 6/40 [01:01<05:43, 10.10s/it] 18%|█▊        | 7/40 [01:11<05:31, 10.03s/it] 20%|██        | 8/40 [01:21<05:26, 10.21s/it] 22%|██▎       | 9/40 [01:33<05:30, 10.66s/it] 25%|██▌       | 10/40 [01:43<05:17, 10.58s/it] 28%|██▊       | 11/40 [01:54<05:07, 10.59s/it] 30%|███       | 12/40 [02:04<04:51, 10.40s/it] 32%|███▎      | 13/40 [02:14<04:39, 10.34s/it] 35%|███▌      | 14/40 [02:25<04:29, 10.36s/it] 38%|███▊      | 15/40 [02:35<04:20, 10.44s/it] 40%|████      | 16/40 [02:46<04:11, 10.48s/it] 42%|████▎     | 17/40 [02:57<04:08, 10.81s/it] 45%|████▌     | 18/40 [03:08<03:56, 10.73s/it] 48%|████▊     | 19/40 [03:18<03:43, 10.62s/it] 50%|█████     | 20/40 [03:29<03:31, 10.55s/it] 52%|█████▎    | 21/40 [03:39<03:21, 10.59s/it] 55%|█████▌    | 22/40 [03:49<03:07, 10.40s/it] 57%|█████▊    | 23/40 [03:59<02:54, 10.29s/it] 60%|██████    | 24/40 [04:10<02:45, 10.33s/it] 62%|██████▎   | 25/40 [04:21<02:40, 10.68s/it] 65%|██████▌   | 26/40 [04:31<02:25, 10.37s/it] 68%|██████▊   | 27/40 [04:41<02:13, 10.26s/it] 70%|███████   | 28/40 [04:51<02:01, 10.13s/it] 72%|███████▎  | 29/40 [05:01<01:50, 10.07s/it] 75%|███████▌  | 30/40 [05:11<01:40, 10.06s/it] 78%|███████▊  | 31/40 [05:21<01:30, 10.08s/it] 80%|████████  | 32/40 [05:31<01:19,  9.98s/it] 82%|████████▎ | 33/40 [05:42<01:13, 10.57s/it] 85%|████████▌ | 34/40 [05:54<01:04, 10.83s/it] 88%|████████▊ | 35/40 [06:05<00:54, 10.86s/it] 90%|█████████ | 36/40 [06:15<00:42, 10.64s/it] 92%|█████████▎| 37/40 [06:25<00:31, 10.49s/it] 95%|█████████▌| 38/40 [06:36<00:20, 10.47s/it] 98%|█████████▊| 39/40 [06:47<00:10, 10.65s/it]100%|██████████| 40/40 [06:57<00:00, 10.68s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:57<00:00, 10.68s/it]100%|██████████| 40/40 [06:57<00:00, 10.45s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_8/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_8/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_8/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 417.8795, 'train_samples_per_second': 6.234, 'train_steps_per_second': 0.096, 'train_loss': 0.7546205520629883, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:07<04:33,  7.02s/it]  5%|▌         | 2/40 [00:14<04:33,  7.19s/it]  8%|▊         | 3/40 [00:20<04:14,  6.89s/it] 10%|█         | 4/40 [00:27<04:04,  6.78s/it] 12%|█▎        | 5/40 [00:34<03:56,  6.77s/it] 15%|█▌        | 6/40 [00:41<03:50,  6.78s/it] 18%|█▊        | 7/40 [00:47<03:39,  6.67s/it] 20%|██        | 8/40 [00:54<03:34,  6.70s/it] 22%|██▎       | 9/40 [01:01<03:36,  6.98s/it] 25%|██▌       | 10/40 [01:08<03:22,  6.76s/it] 28%|██▊       | 11/40 [01:14<03:13,  6.66s/it] 30%|███       | 12/40 [01:20<03:03,  6.56s/it] 32%|███▎      | 13/40 [01:27<02:58,  6.59s/it] 35%|███▌      | 14/40 [01:34<02:54,  6.70s/it] 38%|███▊      | 15/40 [01:41<02:50,  6.84s/it] 40%|████      | 16/40 [01:48<02:44,  6.84s/it] 42%|████▎     | 17/40 [01:55<02:41,  7.00s/it] 45%|████▌     | 18/40 [02:02<02:31,  6.87s/it] 48%|████▊     | 19/40 [02:08<02:22,  6.78s/it] 50%|█████     | 20/40 [02:15<02:15,  6.75s/it] 52%|█████▎    | 21/40 [02:22<02:07,  6.72s/it] 55%|█████▌    | 22/40 [02:28<01:59,  6.64s/it] 57%|█████▊    | 23/40 [02:34<01:50,  6.51s/it] 60%|██████    | 24/40 [02:41<01:46,  6.64s/it] 62%|██████▎   | 25/40 [02:49<01:44,  6.98s/it] 65%|██████▌   | 26/40 [02:56<01:35,  6.83s/it] 68%|██████▊   | 27/40 [03:02<01:27,  6.74s/it] 70%|███████   | 28/40 [03:08<01:19,  6.60s/it] 72%|███████▎  | 29/40 [03:15<01:12,  6.61s/it] 75%|███████▌  | 30/40 [03:22<01:06,  6.60s/it] 78%|███████▊  | 31/40 [03:28<00:59,  6.58s/it] 80%|████████  | 32/40 [03:35<00:52,  6.55s/it] 82%|████████▎ | 33/40 [03:42<00:47,  6.85s/it] 85%|████████▌ | 34/40 [03:49<00:41,  6.86s/it] 88%|████████▊ | 35/40 [03:56<00:34,  6.90s/it] 90%|█████████ | 36/40 [04:03<00:27,  6.80s/it] 92%|█████████▎| 37/40 [04:09<00:20,  6.67s/it] 95%|█████████▌| 38/40 [04:16<00:13,  6.64s/it] 98%|█████████▊| 39/40 [04:23<00:06,  6.71s/it]100%|██████████| 40/40 [04:29<00:00,  6.62s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:29<00:00,  6.62s/it]100%|██████████| 40/40 [04:29<00:00,  6.74s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 269.4385, 'train_samples_per_second': 9.668, 'train_steps_per_second': 0.148, 'train_loss': 0.6985921859741211, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:19,  6.67s/it]  5%|▌         | 2/40 [00:13<04:17,  6.77s/it]  8%|▊         | 3/40 [00:19<04:03,  6.59s/it] 10%|█         | 4/40 [00:26<03:57,  6.60s/it] 12%|█▎        | 5/40 [00:33<03:52,  6.65s/it] 15%|█▌        | 6/40 [00:40<03:48,  6.71s/it] 18%|█▊        | 7/40 [00:46<03:38,  6.63s/it] 20%|██        | 8/40 [00:53<03:33,  6.68s/it] 22%|██▎       | 9/40 [01:00<03:35,  6.95s/it] 25%|██▌       | 10/40 [01:07<03:24,  6.81s/it] 28%|██▊       | 11/40 [01:13<03:15,  6.73s/it] 30%|███       | 12/40 [01:20<03:05,  6.61s/it] 32%|███▎      | 13/40 [01:27<03:00,  6.67s/it] 35%|███▌      | 14/40 [01:33<02:55,  6.74s/it] 38%|███▊      | 15/40 [01:40<02:50,  6.81s/it] 40%|████      | 16/40 [01:47<02:43,  6.81s/it] 42%|████▎     | 17/40 [01:55<02:41,  7.03s/it] 45%|████▌     | 18/40 [02:01<02:32,  6.92s/it] 48%|████▊     | 19/40 [02:08<02:24,  6.88s/it] 50%|█████     | 20/40 [02:15<02:16,  6.81s/it] 52%|█████▎    | 21/40 [02:22<02:08,  6.76s/it] 55%|█████▌    | 22/40 [02:28<02:00,  6.70s/it] 57%|█████▊    | 23/40 [02:35<01:52,  6.63s/it] 60%|██████    | 24/40 [02:41<01:47,  6.71s/it] 62%|██████▎   | 25/40 [02:49<01:44,  6.99s/it] 65%|██████▌   | 26/40 [02:56<01:36,  6.87s/it] 68%|██████▊   | 27/40 [03:02<01:29,  6.85s/it] 70%|███████   | 28/40 [03:09<01:20,  6.70s/it] 72%|███████▎  | 29/40 [03:16<01:13,  6.73s/it] 75%|███████▌  | 30/40 [03:22<01:06,  6.67s/it] 78%|███████▊  | 31/40 [03:29<00:59,  6.64s/it] 80%|████████  | 32/40 [03:35<00:52,  6.61s/it] 82%|████████▎ | 33/40 [03:43<00:47,  6.84s/it] 85%|████████▌ | 34/40 [03:49<00:40,  6.80s/it] 88%|████████▊ | 35/40 [03:56<00:34,  6.80s/it] 90%|█████████ | 36/40 [04:03<00:26,  6.74s/it] 92%|█████████▎| 37/40 [04:09<00:19,  6.61s/it] 95%|█████████▌| 38/40 [04:16<00:13,  6.61s/it] 98%|█████████▊| 39/40 [04:22<00:06,  6.58s/it]100%|██████████| 40/40 [04:28<00:00,  6.45s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:28<00:00,  6.45s/it]100%|██████████| 40/40 [04:28<00:00,  6.72s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 268.8353, 'train_samples_per_second': 9.69, 'train_steps_per_second': 0.149, 'train_loss': 0.7158300399780273, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<05:58,  9.19s/it]  5%|▌         | 2/40 [00:18<06:01,  9.51s/it]  8%|▊         | 3/40 [00:27<05:43,  9.29s/it] 10%|█         | 4/40 [00:37<05:33,  9.26s/it] 12%|█▎        | 5/40 [00:46<05:22,  9.22s/it] 15%|█▌        | 6/40 [00:56<05:20,  9.41s/it] 18%|█▊        | 7/40 [01:05<05:12,  9.48s/it] 20%|██        | 8/40 [01:15<05:07,  9.62s/it] 22%|██▎       | 9/40 [01:26<05:12, 10.07s/it] 25%|██▌       | 10/40 [01:36<04:56,  9.88s/it] 28%|██▊       | 11/40 [01:45<04:42,  9.75s/it] 30%|███       | 12/40 [01:54<04:23,  9.40s/it] 32%|███▎      | 13/40 [02:03<04:14,  9.42s/it] 35%|███▌      | 14/40 [02:13<04:05,  9.43s/it] 38%|███▊      | 15/40 [02:22<03:56,  9.47s/it] 40%|████      | 16/40 [02:32<03:49,  9.56s/it] 42%|████▎     | 17/40 [02:42<03:45,  9.82s/it] 45%|████▌     | 18/40 [02:52<03:32,  9.68s/it] 48%|████▊     | 19/40 [03:01<03:21,  9.61s/it] 50%|█████     | 20/40 [03:10<03:09,  9.49s/it] 52%|█████▎    | 21/40 [03:20<02:59,  9.44s/it] 55%|█████▌    | 22/40 [03:29<02:48,  9.37s/it] 57%|█████▊    | 23/40 [03:38<02:37,  9.29s/it] 60%|██████    | 24/40 [03:48<02:30,  9.42s/it] 62%|██████▎   | 25/40 [03:59<02:28,  9.93s/it] 65%|██████▌   | 26/40 [04:08<02:16,  9.72s/it] 68%|██████▊   | 27/40 [04:18<02:06,  9.73s/it] 70%|███████   | 28/40 [04:27<01:54,  9.51s/it] 72%|███████▎  | 29/40 [04:36<01:44,  9.50s/it] 75%|███████▌  | 30/40 [04:45<01:33,  9.35s/it] 78%|███████▊  | 31/40 [04:54<01:23,  9.29s/it] 80%|████████  | 32/40 [05:04<01:13,  9.22s/it] 82%|████████▎ | 33/40 [05:14<01:07,  9.59s/it] 85%|████████▌ | 34/40 [05:24<00:57,  9.59s/it] 88%|████████▊ | 35/40 [05:33<00:48,  9.62s/it] 90%|█████████ | 36/40 [05:42<00:37,  9.38s/it] 92%|█████████▎| 37/40 [05:51<00:27,  9.18s/it] 95%|█████████▌| 38/40 [06:00<00:18,  9.18s/it] 98%|█████████▊| 39/40 [06:09<00:09,  9.25s/it]100%|██████████| 40/40 [06:18<00:00,  9.10s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:18<00:00,  9.10s/it]100%|██████████| 40/40 [06:18<00:00,  9.47s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 378.6416, 'train_samples_per_second': 6.88, 'train_steps_per_second': 0.106, 'train_loss': 0.6978919982910157, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_2.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:46, 10.42s/it]  5%|▌         | 2/40 [00:21<06:42, 10.59s/it]  8%|▊         | 3/40 [00:30<06:11, 10.05s/it] 10%|█         | 4/40 [00:40<05:58,  9.96s/it] 12%|█▎        | 5/40 [00:50<05:48,  9.96s/it] 15%|█▌        | 6/40 [01:00<05:42, 10.08s/it] 18%|█▊        | 7/40 [01:10<05:29,  9.97s/it] 20%|██        | 8/40 [01:20<05:20, 10.01s/it] 22%|██▎       | 9/40 [01:31<05:21, 10.37s/it] 25%|██▌       | 10/40 [01:41<05:06, 10.21s/it] 28%|██▊       | 11/40 [01:51<04:56, 10.21s/it] 30%|███       | 12/40 [02:01<04:41, 10.07s/it] 32%|███▎      | 13/40 [02:11<04:33, 10.14s/it] 35%|███▌      | 14/40 [02:22<04:25, 10.20s/it] 38%|███▊      | 15/40 [02:32<04:17, 10.30s/it] 40%|████      | 16/40 [02:43<04:08, 10.34s/it] 42%|████▎     | 17/40 [02:54<04:06, 10.70s/it] 45%|████▌     | 18/40 [03:04<03:51, 10.51s/it] 48%|████▊     | 19/40 [03:14<03:39, 10.43s/it] 50%|█████     | 20/40 [03:25<03:27, 10.36s/it] 52%|█████▎    | 21/40 [03:34<03:13, 10.20s/it] 55%|█████▌    | 22/40 [03:44<03:01, 10.09s/it] 57%|█████▊    | 23/40 [03:54<02:50, 10.05s/it] 60%|██████    | 24/40 [04:05<02:43, 10.23s/it] 62%|██████▎   | 25/40 [04:17<02:39, 10.66s/it] 65%|██████▌   | 26/40 [04:26<02:25, 10.41s/it] 68%|██████▊   | 27/40 [04:37<02:14, 10.37s/it] 70%|███████   | 28/40 [04:47<02:04, 10.34s/it] 72%|███████▎  | 29/40 [04:57<01:53, 10.28s/it] 75%|███████▌  | 30/40 [05:07<01:41, 10.17s/it] 78%|███████▊  | 31/40 [05:17<01:32, 10.23s/it] 80%|████████  | 32/40 [05:28<01:21, 10.25s/it] 82%|████████▎ | 33/40 [05:39<01:14, 10.71s/it] 85%|████████▌ | 34/40 [05:50<01:04, 10.75s/it] 88%|████████▊ | 35/40 [06:01<00:54, 10.83s/it] 90%|█████████ | 36/40 [06:11<00:41, 10.50s/it] 92%|█████████▎| 37/40 [06:21<00:30, 10.29s/it] 95%|█████████▌| 38/40 [06:31<00:20, 10.30s/it] 98%|█████████▊| 39/40 [06:42<00:10, 10.39s/it]100%|██████████| 40/40 [06:51<00:00, 10.13s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:51<00:00, 10.13s/it]100%|██████████| 40/40 [06:51<00:00, 10.29s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 411.7323, 'train_samples_per_second': 6.327, 'train_steps_per_second': 0.097, 'train_loss': 0.7546815872192383, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:03,  6.25s/it]  5%|▌         | 2/40 [00:12<03:53,  6.14s/it]  8%|▊         | 3/40 [00:18<03:48,  6.17s/it] 10%|█         | 4/40 [00:24<03:38,  6.06s/it] 12%|█▎        | 5/40 [00:30<03:30,  6.01s/it] 15%|█▌        | 6/40 [00:36<03:22,  5.95s/it] 18%|█▊        | 7/40 [00:42<03:15,  5.91s/it] 20%|██        | 8/40 [00:47<03:07,  5.85s/it] 22%|██▎       | 9/40 [00:58<03:48,  7.37s/it] 25%|██▌       | 10/40 [01:04<03:29,  6.97s/it] 28%|██▊       | 11/40 [01:10<03:11,  6.60s/it] 30%|███       | 12/40 [01:16<02:59,  6.42s/it] 32%|███▎      | 13/40 [01:22<02:50,  6.33s/it] 35%|███▌      | 14/40 [01:29<02:47,  6.45s/it] 38%|███▊      | 15/40 [01:34<02:36,  6.27s/it] 40%|████      | 16/40 [01:41<02:29,  6.22s/it] 42%|████▎     | 17/40 [01:52<02:56,  7.67s/it] 45%|████▌     | 18/40 [01:58<02:38,  7.20s/it] 48%|████▊     | 19/40 [02:04<02:23,  6.82s/it] 50%|█████     | 20/40 [02:10<02:11,  6.60s/it] 52%|█████▎    | 21/40 [02:16<02:01,  6.37s/it] 55%|█████▌    | 22/40 [02:22<01:53,  6.33s/it] 57%|█████▊    | 23/40 [02:28<01:45,  6.20s/it] 60%|██████    | 24/40 [02:34<01:37,  6.09s/it] 62%|██████▎   | 25/40 [02:43<01:48,  7.20s/it] 65%|██████▌   | 26/40 [02:49<01:35,  6.83s/it] 68%|██████▊   | 27/40 [02:55<01:26,  6.62s/it] 70%|███████   | 28/40 [03:01<01:17,  6.42s/it] 72%|███████▎  | 29/40 [03:07<01:09,  6.28s/it] 75%|███████▌  | 30/40 [03:13<01:01,  6.14s/it] 78%|███████▊  | 31/40 [03:19<00:54,  6.07s/it] 80%|████████  | 32/40 [03:25<00:48,  6.04s/it] 82%|████████▎ | 33/40 [03:35<00:51,  7.29s/it] 85%|████████▌ | 34/40 [03:41<00:41,  6.94s/it] 88%|████████▊ | 35/40 [03:47<00:32,  6.56s/it] 90%|█████████ | 36/40 [03:53<00:25,  6.42s/it] 92%|█████████▎| 37/40 [03:59<00:18,  6.23s/it] 95%|█████████▌| 38/40 [04:05<00:12,  6.12s/it] 98%|█████████▊| 39/40 [04:11<00:06,  6.09s/it]100%|██████████| 40/40 [04:17<00:00,  6.09s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:17<00:00,  6.09s/it]100%|██████████| 40/40 [04:17<00:00,  6.44s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 257.4277, 'train_samples_per_second': 10.896, 'train_steps_per_second': 0.155, 'train_loss': 0.7461462020874023, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:53,  5.99s/it]  5%|▌         | 2/40 [00:12<04:01,  6.35s/it]  8%|▊         | 3/40 [00:19<03:58,  6.46s/it] 10%|█         | 4/40 [00:25<03:49,  6.38s/it] 12%|█▎        | 5/40 [00:31<03:42,  6.35s/it] 15%|█▌        | 6/40 [00:37<03:29,  6.15s/it] 18%|█▊        | 7/40 [00:43<03:22,  6.14s/it] 20%|██        | 8/40 [00:49<03:15,  6.11s/it] 22%|██▎       | 9/40 [01:00<03:58,  7.69s/it] 25%|██▌       | 10/40 [01:07<03:37,  7.26s/it] 28%|██▊       | 11/40 [01:13<03:19,  6.88s/it] 30%|███       | 12/40 [01:19<03:07,  6.68s/it] 32%|███▎      | 13/40 [01:25<02:55,  6.50s/it] 35%|███▌      | 14/40 [01:31<02:49,  6.51s/it] 38%|███▊      | 15/40 [01:38<02:40,  6.42s/it] 40%|████      | 16/40 [01:44<02:33,  6.39s/it] 42%|████▎     | 17/40 [01:55<03:00,  7.85s/it] 45%|████▌     | 18/40 [02:01<02:41,  7.36s/it] 48%|████▊     | 19/40 [02:08<02:26,  6.99s/it] 50%|█████     | 20/40 [02:14<02:15,  6.78s/it] 52%|█████▎    | 21/40 [02:20<02:05,  6.59s/it] 55%|█████▌    | 22/40 [02:26<01:57,  6.53s/it] 57%|█████▊    | 23/40 [02:33<01:49,  6.43s/it] 60%|██████    | 24/40 [02:39<01:41,  6.32s/it] 62%|██████▎   | 25/40 [02:49<01:52,  7.52s/it] 65%|██████▌   | 26/40 [02:55<01:39,  7.14s/it] 68%|██████▊   | 27/40 [03:02<01:30,  6.93s/it] 70%|███████   | 28/40 [03:08<01:20,  6.74s/it] 72%|███████▎  | 29/40 [03:14<01:12,  6.59s/it] 75%|███████▌  | 30/40 [03:20<01:04,  6.47s/it] 78%|███████▊  | 31/40 [03:27<00:57,  6.40s/it] 80%|████████  | 32/40 [03:33<00:51,  6.38s/it] 82%|████████▎ | 33/40 [03:44<00:53,  7.70s/it] 85%|████████▌ | 34/40 [03:50<00:43,  7.30s/it] 88%|████████▊ | 35/40 [03:56<00:34,  6.92s/it] 90%|█████████ | 36/40 [04:03<00:27,  6.77s/it] 92%|█████████▎| 37/40 [04:09<00:19,  6.58s/it] 95%|█████████▌| 38/40 [04:15<00:13,  6.62s/it] 98%|█████████▊| 39/40 [04:22<00:06,  6.66s/it]100%|██████████| 40/40 [04:29<00:00,  6.61s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:29<00:00,  6.61s/it]100%|██████████| 40/40 [04:29<00:00,  6.73s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 269.1934, 'train_samples_per_second': 10.42, 'train_steps_per_second': 0.149, 'train_loss': 0.7506973266601562, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:47,  8.90s/it]  5%|▌         | 2/40 [00:17<05:42,  9.01s/it]  8%|▊         | 3/40 [00:27<05:40,  9.20s/it] 10%|█         | 4/40 [00:36<05:26,  9.06s/it] 12%|█▎        | 5/40 [00:45<05:15,  9.01s/it] 15%|█▌        | 6/40 [00:53<05:02,  8.91s/it] 18%|█▊        | 7/40 [01:02<04:50,  8.81s/it] 20%|██        | 8/40 [01:10<04:38,  8.69s/it] 22%|██▎       | 9/40 [01:26<05:34, 10.80s/it] 25%|██▌       | 10/40 [01:35<05:07, 10.23s/it] 28%|██▊       | 11/40 [01:43<04:41,  9.72s/it] 30%|███       | 12/40 [01:52<04:24,  9.44s/it] 32%|███▎      | 13/40 [02:01<04:07,  9.18s/it] 35%|███▌      | 14/40 [02:10<03:56,  9.11s/it] 38%|███▊      | 15/40 [02:18<03:41,  8.87s/it] 40%|████      | 16/40 [02:27<03:31,  8.80s/it] 42%|████▎     | 17/40 [02:42<04:07, 10.77s/it] 45%|████▌     | 18/40 [02:51<03:42, 10.12s/it] 48%|████▊     | 19/40 [02:59<03:21,  9.58s/it] 50%|█████     | 20/40 [03:08<03:06,  9.32s/it] 52%|█████▎    | 21/40 [03:16<02:51,  9.05s/it] 55%|█████▌    | 22/40 [03:25<02:42,  9.03s/it] 57%|█████▊    | 23/40 [03:34<02:30,  8.87s/it] 60%|██████    | 24/40 [03:42<02:20,  8.76s/it] 62%|██████▎   | 25/40 [03:56<02:36, 10.40s/it] 65%|██████▌   | 26/40 [04:05<02:18,  9.87s/it] 68%|██████▊   | 27/40 [04:14<02:04,  9.56s/it] 70%|███████   | 28/40 [04:22<01:50,  9.24s/it] 72%|███████▎  | 29/40 [04:31<01:39,  9.05s/it] 75%|███████▌  | 30/40 [04:39<01:28,  8.87s/it] 78%|███████▊  | 31/40 [04:48<01:20,  8.89s/it] 80%|████████  | 32/40 [04:57<01:10,  8.82s/it] 82%|████████▎ | 33/40 [05:12<01:14, 10.69s/it] 85%|████████▌ | 34/40 [05:21<01:01, 10.17s/it] 88%|████████▊ | 35/40 [05:29<00:48,  9.66s/it] 90%|█████████ | 36/40 [05:38<00:37,  9.45s/it] 92%|█████████▎| 37/40 [05:47<00:27,  9.18s/it] 95%|█████████▌| 38/40 [05:56<00:18,  9.08s/it] 98%|█████████▊| 39/40 [06:04<00:08,  8.94s/it]100%|██████████| 40/40 [06:13<00:00,  8.82s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:13<00:00,  8.82s/it]100%|██████████| 40/40 [06:13<00:00,  9.34s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 373.4241, 'train_samples_per_second': 7.512, 'train_steps_per_second': 0.107, 'train_loss': 0.7571592330932617, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_2.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:00,  9.25s/it]  5%|▌         | 2/40 [00:18<05:58,  9.43s/it]  8%|▊         | 3/40 [00:28<05:58,  9.68s/it] 10%|█         | 4/40 [00:38<05:46,  9.62s/it] 12%|█▎        | 5/40 [00:47<05:36,  9.62s/it] 15%|█▌        | 6/40 [00:57<05:23,  9.52s/it] 18%|█▊        | 7/40 [01:06<05:11,  9.44s/it] 20%|██        | 8/40 [01:15<04:59,  9.36s/it] 22%|██▎       | 9/40 [01:32<06:02, 11.70s/it] 25%|██▌       | 10/40 [01:42<05:31, 11.06s/it] 28%|██▊       | 11/40 [01:51<05:03, 10.48s/it] 30%|███       | 12/40 [02:00<04:45, 10.19s/it] 32%|███▎      | 13/40 [02:10<04:30, 10.03s/it] 35%|███▌      | 14/40 [02:20<04:18,  9.96s/it] 38%|███▊      | 15/40 [02:29<04:05,  9.83s/it] 40%|████      | 16/40 [02:39<03:55,  9.80s/it] 42%|████▎     | 17/40 [02:56<04:34, 11.94s/it] 45%|████▌     | 18/40 [03:06<04:06, 11.21s/it] 48%|████▊     | 19/40 [03:15<03:44, 10.69s/it] 50%|█████     | 20/40 [03:25<03:28, 10.42s/it] 52%|█████▎    | 21/40 [03:34<03:13, 10.17s/it] 55%|█████▌    | 22/40 [03:44<03:01, 10.09s/it] 57%|█████▊    | 23/40 [03:54<02:50, 10.05s/it] 60%|██████    | 24/40 [04:04<02:40, 10.01s/it] 62%|██████▎   | 25/40 [04:21<03:00, 12.05s/it] 65%|██████▌   | 26/40 [04:31<02:41, 11.54s/it] 68%|██████▊   | 27/40 [04:41<02:24, 11.12s/it] 70%|███████   | 28/40 [04:51<02:07, 10.60s/it] 72%|███████▎  | 29/40 [05:00<01:52, 10.26s/it] 75%|███████▌  | 30/40 [05:10<01:39,  9.95s/it] 78%|███████▊  | 31/40 [05:19<01:28,  9.85s/it] 80%|████████  | 32/40 [05:29<01:18,  9.81s/it] 82%|████████▎ | 33/40 [05:46<01:23, 11.89s/it] 85%|████████▌ | 34/40 [05:56<01:07, 11.31s/it] 88%|████████▊ | 35/40 [06:05<00:53, 10.68s/it] 90%|█████████ | 36/40 [06:15<00:41, 10.40s/it] 92%|█████████▎| 37/40 [06:24<00:30, 10.10s/it] 95%|█████████▌| 38/40 [06:33<00:19,  9.89s/it] 98%|█████████▊| 39/40 [06:43<00:09,  9.74s/it]100%|██████████| 40/40 [06:52<00:00,  9.54s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:52<00:00,  9.54s/it]100%|██████████| 40/40 [06:52<00:00, 10.31s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_2/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_2/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_2/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 412.2986, 'train_samples_per_second': 6.803, 'train_steps_per_second': 0.097, 'train_loss': 0.7825010299682618, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:06,  6.32s/it]  5%|▌         | 2/40 [00:12<04:05,  6.46s/it]  8%|▊         | 3/40 [00:19<03:57,  6.41s/it] 10%|█         | 4/40 [00:25<03:48,  6.34s/it] 12%|█▎        | 5/40 [00:31<03:42,  6.35s/it] 15%|█▌        | 6/40 [00:38<03:36,  6.36s/it] 18%|█▊        | 7/40 [00:44<03:29,  6.35s/it] 20%|██        | 8/40 [00:51<03:26,  6.47s/it] 22%|██▎       | 9/40 [00:58<03:27,  6.68s/it] 25%|██▌       | 10/40 [01:04<03:19,  6.64s/it] 28%|██▊       | 11/40 [01:11<03:07,  6.48s/it] 30%|███       | 12/40 [01:17<02:57,  6.35s/it] 32%|███▎      | 13/40 [01:23<02:51,  6.36s/it] 35%|███▌      | 14/40 [01:29<02:44,  6.34s/it] 38%|███▊      | 15/40 [01:36<02:40,  6.44s/it] 40%|████      | 16/40 [01:42<02:34,  6.43s/it] 42%|████▎     | 17/40 [01:50<02:33,  6.66s/it] 45%|████▌     | 18/40 [01:56<02:26,  6.65s/it] 48%|████▊     | 19/40 [02:02<02:16,  6.50s/it] 50%|█████     | 20/40 [02:08<02:07,  6.38s/it] 52%|█████▎    | 21/40 [02:15<02:01,  6.42s/it] 55%|█████▌    | 22/40 [02:21<01:53,  6.33s/it] 57%|█████▊    | 23/40 [02:28<01:48,  6.37s/it] 60%|██████    | 24/40 [02:34<01:42,  6.39s/it] 62%|██████▎   | 25/40 [02:41<01:39,  6.60s/it] 65%|██████▌   | 26/40 [02:47<01:31,  6.55s/it] 68%|██████▊   | 27/40 [02:54<01:24,  6.51s/it] 70%|███████   | 28/40 [03:00<01:16,  6.41s/it] 72%|███████▎  | 29/40 [03:06<01:10,  6.38s/it] 75%|███████▌  | 30/40 [03:12<01:02,  6.29s/it] 78%|███████▊  | 31/40 [03:19<00:57,  6.39s/it] 80%|████████  | 32/40 [03:25<00:50,  6.30s/it] 82%|████████▎ | 33/40 [03:32<00:46,  6.59s/it] 85%|████████▌ | 34/40 [03:39<00:39,  6.52s/it] 88%|████████▊ | 35/40 [03:45<00:32,  6.52s/it] 90%|█████████ | 36/40 [03:51<00:25,  6.40s/it] 92%|█████████▎| 37/40 [03:58<00:19,  6.35s/it] 95%|█████████▌| 38/40 [04:04<00:12,  6.34s/it] 98%|█████████▊| 39/40 [04:10<00:06,  6.31s/it]100%|██████████| 40/40 [04:17<00:00,  6.35s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:17<00:00,  6.35s/it]100%|██████████| 40/40 [04:17<00:00,  6.43s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 257.1806, 'train_samples_per_second': 10.129, 'train_steps_per_second': 0.156, 'train_loss': 0.6991683959960937, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:07,  6.34s/it]  5%|▌         | 2/40 [00:13<04:08,  6.54s/it]  8%|▊         | 3/40 [00:19<03:57,  6.43s/it] 10%|█         | 4/40 [00:25<03:47,  6.33s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.33s/it] 15%|█▌        | 6/40 [00:38<03:35,  6.33s/it] 18%|█▊        | 7/40 [00:44<03:28,  6.31s/it] 20%|██        | 8/40 [00:51<03:26,  6.45s/it] 22%|██▎       | 9/40 [00:58<03:26,  6.68s/it] 25%|██▌       | 10/40 [01:04<03:19,  6.64s/it] 28%|██▊       | 11/40 [01:10<03:07,  6.47s/it] 30%|███       | 12/40 [01:17<02:57,  6.35s/it] 32%|███▎      | 13/40 [01:23<02:51,  6.36s/it] 35%|███▌      | 14/40 [01:29<02:45,  6.36s/it] 38%|███▊      | 15/40 [01:36<02:41,  6.46s/it] 40%|████      | 16/40 [01:42<02:34,  6.45s/it] 42%|████▎     | 17/40 [01:50<02:33,  6.69s/it] 45%|████▌     | 18/40 [01:56<02:26,  6.67s/it] 48%|████▊     | 19/40 [02:02<02:16,  6.48s/it] 50%|█████     | 20/40 [02:08<02:07,  6.35s/it] 52%|█████▎    | 21/40 [02:15<02:01,  6.39s/it] 55%|█████▌    | 22/40 [02:21<01:53,  6.33s/it] 57%|█████▊    | 23/40 [02:28<01:48,  6.39s/it] 60%|██████    | 24/40 [02:34<01:42,  6.43s/it] 62%|██████▎   | 25/40 [02:42<01:41,  6.75s/it] 65%|██████▌   | 26/40 [02:49<01:35,  6.83s/it] 68%|██████▊   | 27/40 [02:56<01:30,  6.93s/it] 70%|███████   | 28/40 [03:02<01:22,  6.86s/it] 72%|███████▎  | 29/40 [03:09<01:15,  6.82s/it] 75%|███████▌  | 30/40 [03:16<01:07,  6.76s/it] 78%|███████▊  | 31/40 [03:23<01:01,  6.86s/it] 80%|████████  | 32/40 [03:30<00:54,  6.77s/it] 82%|████████▎ | 33/40 [03:37<00:49,  7.04s/it] 85%|████████▌ | 34/40 [03:44<00:41,  6.86s/it] 88%|████████▊ | 35/40 [03:50<00:33,  6.76s/it] 90%|█████████ | 36/40 [03:56<00:26,  6.57s/it] 92%|█████████▎| 37/40 [04:03<00:19,  6.48s/it] 95%|█████████▌| 38/40 [04:09<00:12,  6.45s/it] 98%|█████████▊| 39/40 [04:15<00:06,  6.39s/it]100%|██████████| 40/40 [04:21<00:00,  6.30s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:21<00:00,  6.30s/it]100%|██████████| 40/40 [04:21<00:00,  6.54s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 261.7622, 'train_samples_per_second': 9.952, 'train_steps_per_second': 0.153, 'train_loss': 0.7158330917358399, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:50,  8.99s/it]  5%|▌         | 2/40 [00:18<05:47,  9.15s/it]  8%|▊         | 3/40 [00:27<05:34,  9.03s/it] 10%|█         | 4/40 [00:35<05:20,  8.90s/it] 12%|█▎        | 5/40 [00:44<05:12,  8.94s/it] 15%|█▌        | 6/40 [00:53<05:03,  8.94s/it] 18%|█▊        | 7/40 [01:02<04:55,  8.96s/it] 20%|██        | 8/40 [01:12<04:55,  9.23s/it] 22%|██▎       | 9/40 [01:22<04:57,  9.59s/it] 25%|██▌       | 10/40 [01:32<04:46,  9.54s/it] 28%|██▊       | 11/40 [01:41<04:30,  9.32s/it] 30%|███       | 12/40 [01:50<04:17,  9.21s/it] 32%|███▎      | 13/40 [01:59<04:08,  9.19s/it] 35%|███▌      | 14/40 [02:08<03:57,  9.14s/it] 38%|███▊      | 15/40 [02:18<03:53,  9.32s/it] 40%|████      | 16/40 [02:27<03:43,  9.32s/it] 42%|████▎     | 17/40 [02:37<03:42,  9.66s/it] 45%|████▌     | 18/40 [02:47<03:32,  9.64s/it] 48%|████▊     | 19/40 [02:56<03:17,  9.40s/it] 50%|█████     | 20/40 [03:05<03:04,  9.21s/it] 52%|█████▎    | 21/40 [03:14<02:56,  9.29s/it] 55%|█████▌    | 22/40 [03:23<02:45,  9.19s/it] 57%|█████▊    | 23/40 [03:32<02:37,  9.28s/it] 60%|██████    | 24/40 [03:42<02:29,  9.34s/it] 62%|██████▎   | 25/40 [03:52<02:24,  9.66s/it] 65%|██████▌   | 26/40 [04:02<02:13,  9.53s/it] 68%|██████▊   | 27/40 [04:11<02:02,  9.45s/it] 70%|███████   | 28/40 [04:20<01:52,  9.35s/it] 72%|███████▎  | 29/40 [04:29<01:42,  9.31s/it] 75%|███████▌  | 30/40 [04:38<01:31,  9.16s/it] 78%|███████▊  | 31/40 [04:48<01:24,  9.34s/it] 80%|████████  | 32/40 [04:57<01:13,  9.18s/it] 82%|████████▎ | 33/40 [05:07<01:07,  9.60s/it] 85%|████████▌ | 34/40 [05:17<00:57,  9.59s/it] 88%|████████▊ | 35/40 [05:26<00:48,  9.64s/it] 90%|█████████ | 36/40 [05:36<00:37,  9.49s/it] 92%|█████████▎| 37/40 [05:45<00:28,  9.43s/it] 95%|█████████▌| 38/40 [05:54<00:18,  9.44s/it] 98%|█████████▊| 39/40 [06:04<00:09,  9.39s/it]100%|██████████| 40/40 [06:13<00:00,  9.29s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:13<00:00,  9.29s/it]100%|██████████| 40/40 [06:13<00:00,  9.33s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 373.1805, 'train_samples_per_second': 6.981, 'train_steps_per_second': 0.107, 'train_loss': 0.6977348327636719, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_7.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:22,  9.81s/it]  5%|▌         | 2/40 [00:20<06:21, 10.04s/it]  8%|▊         | 3/40 [00:29<06:08,  9.96s/it] 10%|█         | 4/40 [00:39<05:53,  9.83s/it] 12%|█▎        | 5/40 [00:49<05:45,  9.86s/it] 15%|█▌        | 6/40 [00:59<05:35,  9.88s/it] 18%|█▊        | 7/40 [01:09<05:29,  9.98s/it] 20%|██        | 8/40 [01:20<05:29, 10.30s/it] 22%|██▎       | 9/40 [01:32<05:33, 10.76s/it] 25%|██▌       | 10/40 [01:42<05:22, 10.74s/it] 28%|██▊       | 11/40 [01:52<05:04, 10.51s/it] 30%|███       | 12/40 [02:02<04:49, 10.33s/it] 32%|███▎      | 13/40 [02:13<04:39, 10.36s/it] 35%|███▌      | 14/40 [02:23<04:28, 10.34s/it] 38%|███▊      | 15/40 [02:34<04:22, 10.49s/it] 40%|████      | 16/40 [02:45<04:13, 10.56s/it] 42%|████▎     | 17/40 [02:57<04:14, 11.07s/it] 45%|████▌     | 18/40 [03:08<04:03, 11.07s/it] 48%|████▊     | 19/40 [03:18<03:47, 10.85s/it] 50%|█████     | 20/40 [03:29<03:35, 10.76s/it] 52%|█████▎    | 21/40 [03:40<03:24, 10.77s/it] 55%|█████▌    | 22/40 [03:50<03:11, 10.62s/it] 57%|█████▊    | 23/40 [04:01<03:01, 10.65s/it] 60%|██████    | 24/40 [04:11<02:51, 10.69s/it] 62%|██████▎   | 25/40 [04:23<02:45, 11.06s/it] 65%|██████▌   | 26/40 [04:34<02:32, 10.91s/it] 68%|██████▊   | 27/40 [04:44<02:20, 10.79s/it] 70%|███████   | 28/40 [04:55<02:07, 10.63s/it] 72%|███████▎  | 29/40 [05:05<01:56, 10.55s/it] 75%|███████▌  | 30/40 [05:15<01:44, 10.41s/it] 78%|███████▊  | 31/40 [05:26<01:35, 10.56s/it] 80%|████████  | 32/40 [05:36<01:23, 10.39s/it] 82%|████████▎ | 33/40 [05:48<01:15, 10.84s/it] 85%|████████▌ | 34/40 [05:58<01:04, 10.72s/it] 88%|████████▊ | 35/40 [06:09<00:53, 10.71s/it] 90%|█████████ | 36/40 [06:19<00:42, 10.60s/it] 92%|█████████▎| 37/40 [06:30<00:31, 10.51s/it] 95%|█████████▌| 38/40 [06:40<00:20, 10.49s/it] 98%|█████████▊| 39/40 [06:51<00:10, 10.45s/it]100%|██████████| 40/40 [07:01<00:00, 10.34s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:01<00:00, 10.34s/it]100%|██████████| 40/40 [07:01<00:00, 10.53s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_7/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_7/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_7/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 421.1017, 'train_samples_per_second': 6.186, 'train_steps_per_second': 0.095, 'train_loss': 0.755892276763916, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:09,  6.39s/it]  5%|▌         | 2/40 [00:13<04:15,  6.73s/it]  8%|▊         | 3/40 [00:19<04:06,  6.66s/it] 10%|█         | 4/40 [00:26<03:57,  6.60s/it] 12%|█▎        | 5/40 [00:32<03:49,  6.56s/it] 15%|█▌        | 6/40 [00:39<03:44,  6.60s/it] 18%|█▊        | 7/40 [00:46<03:38,  6.62s/it] 20%|██        | 8/40 [00:53<03:36,  6.75s/it] 22%|██▎       | 9/40 [01:00<03:35,  6.94s/it] 25%|██▌       | 10/40 [01:07<03:25,  6.84s/it] 28%|██▊       | 11/40 [01:14<03:17,  6.81s/it] 30%|███       | 12/40 [01:20<03:09,  6.75s/it] 32%|███▎      | 13/40 [01:27<03:00,  6.69s/it] 35%|███▌      | 14/40 [01:33<02:53,  6.67s/it] 38%|███▊      | 15/40 [01:40<02:46,  6.65s/it] 40%|████      | 16/40 [01:47<02:40,  6.71s/it] 42%|████▎     | 17/40 [01:54<02:39,  6.95s/it] 45%|████▌     | 18/40 [02:01<02:33,  6.97s/it] 48%|████▊     | 19/40 [02:08<02:24,  6.88s/it] 50%|█████     | 20/40 [02:15<02:16,  6.82s/it] 52%|█████▎    | 21/40 [02:22<02:10,  6.86s/it] 55%|█████▌    | 22/40 [02:28<02:01,  6.75s/it] 57%|█████▊    | 23/40 [02:35<01:53,  6.68s/it] 60%|██████    | 24/40 [02:42<01:48,  6.76s/it] 62%|██████▎   | 25/40 [02:49<01:45,  7.00s/it] 65%|██████▌   | 26/40 [02:56<01:36,  6.91s/it] 68%|██████▊   | 27/40 [03:03<01:28,  6.84s/it] 70%|███████   | 28/40 [03:09<01:19,  6.66s/it] 72%|███████▎  | 29/40 [03:15<01:13,  6.67s/it] 75%|███████▌  | 30/40 [03:22<01:06,  6.63s/it] 78%|███████▊  | 31/40 [03:29<00:59,  6.62s/it] 80%|████████  | 32/40 [03:35<00:53,  6.64s/it] 82%|████████▎ | 33/40 [03:43<00:48,  6.93s/it] 85%|████████▌ | 34/40 [03:50<00:41,  6.86s/it] 88%|████████▊ | 35/40 [03:56<00:34,  6.83s/it] 90%|█████████ | 36/40 [04:03<00:27,  6.76s/it] 92%|█████████▎| 37/40 [04:10<00:20,  6.72s/it] 95%|█████████▌| 38/40 [04:16<00:13,  6.73s/it] 98%|█████████▊| 39/40 [04:23<00:06,  6.69s/it]100%|██████████| 40/40 [04:29<00:00,  6.55s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:29<00:00,  6.55s/it]100%|██████████| 40/40 [04:29<00:00,  6.74s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 269.6255, 'train_samples_per_second': 9.662, 'train_steps_per_second': 0.148, 'train_loss': 0.6992908477783203, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:08,  6.36s/it]  5%|▌         | 2/40 [00:13<04:13,  6.68s/it]  8%|▊         | 3/40 [00:19<04:04,  6.60s/it] 10%|█         | 4/40 [00:26<03:55,  6.53s/it] 12%|█▎        | 5/40 [00:32<03:46,  6.47s/it] 15%|█▌        | 6/40 [00:39<03:40,  6.49s/it] 18%|█▊        | 7/40 [00:45<03:34,  6.49s/it] 20%|██        | 8/40 [00:52<03:31,  6.62s/it] 22%|██▎       | 9/40 [00:59<03:30,  6.80s/it] 25%|██▌       | 10/40 [01:06<03:21,  6.73s/it] 28%|██▊       | 11/40 [01:13<03:16,  6.79s/it] 30%|███       | 12/40 [01:20<03:10,  6.82s/it] 32%|███▎      | 13/40 [01:26<03:04,  6.85s/it] 35%|███▌      | 14/40 [01:33<02:58,  6.86s/it] 38%|███▊      | 15/40 [01:40<02:52,  6.90s/it] 40%|████      | 16/40 [01:48<02:47,  6.98s/it] 42%|████▎     | 17/40 [01:55<02:44,  7.17s/it] 45%|████▌     | 18/40 [02:02<02:38,  7.21s/it] 48%|████▊     | 19/40 [02:09<02:27,  7.03s/it] 50%|█████     | 20/40 [02:16<02:20,  7.01s/it] 52%|█████▎    | 21/40 [02:23<02:12,  6.96s/it] 55%|█████▌    | 22/40 [02:30<02:03,  6.87s/it] 57%|█████▊    | 23/40 [02:36<01:56,  6.87s/it] 60%|██████    | 24/40 [02:44<01:51,  6.96s/it] 62%|██████▎   | 25/40 [02:51<01:48,  7.20s/it] 65%|██████▌   | 26/40 [02:58<01:40,  7.16s/it] 68%|██████▊   | 27/40 [03:05<01:32,  7.10s/it] 70%|███████   | 28/40 [03:12<01:23,  6.95s/it] 72%|███████▎  | 29/40 [03:19<01:16,  6.97s/it] 75%|███████▌  | 30/40 [03:26<01:09,  6.90s/it] 78%|███████▊  | 31/40 [03:33<01:02,  6.92s/it] 80%|████████  | 32/40 [03:39<00:54,  6.85s/it] 82%|████████▎ | 33/40 [03:47<00:49,  7.07s/it] 85%|████████▌ | 34/40 [03:54<00:41,  7.00s/it] 88%|████████▊ | 35/40 [04:01<00:34,  6.97s/it] 90%|█████████ | 36/40 [04:08<00:27,  6.95s/it] 92%|█████████▎| 37/40 [04:14<00:20,  6.91s/it] 95%|█████████▌| 38/40 [04:21<00:13,  6.91s/it] 98%|█████████▊| 39/40 [04:28<00:06,  6.83s/it]100%|██████████| 40/40 [04:34<00:00,  6.69s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:34<00:00,  6.69s/it]100%|██████████| 40/40 [04:34<00:00,  6.87s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 274.7953, 'train_samples_per_second': 9.48, 'train_steps_per_second': 0.146, 'train_loss': 0.7158027648925781, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:50,  9.00s/it]  5%|▌         | 2/40 [00:18<06:00,  9.49s/it]  8%|▊         | 3/40 [00:28<05:46,  9.37s/it] 10%|█         | 4/40 [00:37<05:32,  9.25s/it] 12%|█▎        | 5/40 [00:46<05:21,  9.18s/it] 15%|█▌        | 6/40 [00:55<05:12,  9.19s/it] 18%|█▊        | 7/40 [01:04<05:02,  9.17s/it] 20%|██        | 8/40 [01:14<05:01,  9.43s/it] 22%|██▎       | 9/40 [01:24<04:59,  9.66s/it] 25%|██▌       | 10/40 [01:34<04:46,  9.56s/it] 28%|██▊       | 11/40 [01:43<04:39,  9.64s/it] 30%|███       | 12/40 [01:53<04:28,  9.60s/it] 32%|███▎      | 13/40 [02:02<04:17,  9.53s/it] 35%|███▌      | 14/40 [02:11<04:05,  9.46s/it] 38%|███▊      | 15/40 [02:21<03:54,  9.40s/it] 40%|████      | 16/40 [02:30<03:47,  9.47s/it] 42%|████▎     | 17/40 [02:40<03:41,  9.62s/it] 45%|████▌     | 18/40 [02:50<03:33,  9.72s/it] 48%|████▊     | 19/40 [02:59<03:20,  9.55s/it] 50%|█████     | 20/40 [03:09<03:09,  9.46s/it] 52%|█████▎    | 21/40 [03:18<02:59,  9.45s/it] 55%|█████▌    | 22/40 [03:27<02:47,  9.33s/it] 57%|█████▊    | 23/40 [03:36<02:37,  9.26s/it] 60%|██████    | 24/40 [03:46<02:31,  9.49s/it] 62%|██████▎   | 25/40 [03:57<02:27,  9.84s/it] 65%|██████▌   | 26/40 [04:07<02:16,  9.76s/it] 68%|██████▊   | 27/40 [04:16<02:06,  9.70s/it] 70%|███████   | 28/40 [04:25<01:52,  9.41s/it] 72%|███████▎  | 29/40 [04:34<01:43,  9.43s/it] 75%|███████▌  | 30/40 [04:44<01:33,  9.38s/it] 78%|███████▊  | 31/40 [04:53<01:24,  9.41s/it] 80%|████████  | 32/40 [05:02<01:15,  9.38s/it] 82%|████████▎ | 33/40 [05:13<01:07,  9.63s/it] 85%|████████▌ | 34/40 [05:22<00:57,  9.66s/it] 88%|████████▊ | 35/40 [05:32<00:48,  9.67s/it] 90%|█████████ | 36/40 [05:41<00:38,  9.61s/it] 92%|█████████▎| 37/40 [05:51<00:28,  9.58s/it] 95%|█████████▌| 38/40 [06:01<00:19,  9.60s/it] 98%|█████████▊| 39/40 [06:10<00:09,  9.53s/it]100%|██████████| 40/40 [06:19<00:00,  9.34s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:19<00:00,  9.34s/it]100%|██████████| 40/40 [06:19<00:00,  9.49s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 379.4064, 'train_samples_per_second': 6.866, 'train_steps_per_second': 0.105, 'train_loss': 0.6989814758300781, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_9.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:30, 10.01s/it]  5%|▌         | 2/40 [00:20<06:42, 10.58s/it]  8%|▊         | 3/40 [00:31<06:32, 10.61s/it] 10%|█         | 4/40 [00:42<06:23, 10.65s/it] 12%|█▎        | 5/40 [00:52<06:11, 10.62s/it] 15%|█▌        | 6/40 [01:03<06:01, 10.64s/it] 18%|█▊        | 7/40 [01:14<05:51, 10.65s/it] 20%|██        | 8/40 [01:25<05:47, 10.86s/it] 22%|██▎       | 9/40 [01:37<05:44, 11.10s/it] 25%|██▌       | 10/40 [01:48<05:30, 11.02s/it] 28%|██▊       | 11/40 [01:59<05:20, 11.04s/it] 30%|███       | 12/40 [02:09<05:05, 10.92s/it] 32%|███▎      | 13/40 [02:20<04:52, 10.82s/it] 35%|███▌      | 14/40 [02:30<04:38, 10.73s/it] 38%|███▊      | 15/40 [02:41<04:26, 10.64s/it] 40%|████      | 16/40 [02:52<04:16, 10.69s/it] 42%|████▎     | 17/40 [03:03<04:12, 10.96s/it] 45%|████▌     | 18/40 [03:14<04:01, 10.99s/it] 48%|████▊     | 19/40 [03:25<03:49, 10.91s/it] 50%|█████     | 20/40 [03:35<03:35, 10.76s/it] 52%|█████▎    | 21/40 [03:46<03:24, 10.75s/it] 55%|█████▌    | 22/40 [03:56<03:10, 10.59s/it] 57%|█████▊    | 23/40 [04:07<02:58, 10.49s/it] 60%|██████    | 24/40 [04:18<02:49, 10.62s/it] 62%|██████▎   | 25/40 [04:30<02:45, 11.04s/it] 65%|██████▌   | 26/40 [04:40<02:32, 10.93s/it] 68%|██████▊   | 27/40 [04:51<02:21, 10.88s/it] 70%|███████   | 28/40 [05:01<02:07, 10.61s/it] 72%|███████▎  | 29/40 [05:12<01:57, 10.64s/it] 75%|███████▌  | 30/40 [05:22<01:45, 10.58s/it] 78%|███████▊  | 31/40 [05:33<01:34, 10.55s/it] 80%|████████  | 32/40 [05:43<01:23, 10.48s/it] 82%|████████▎ | 33/40 [05:54<01:15, 10.75s/it] 85%|████████▌ | 34/40 [06:05<01:04, 10.78s/it] 88%|████████▊ | 35/40 [06:16<00:53, 10.78s/it] 90%|█████████ | 36/40 [06:26<00:42, 10.66s/it] 92%|█████████▎| 37/40 [06:37<00:31, 10.62s/it] 95%|█████████▌| 38/40 [06:48<00:21, 10.64s/it] 98%|█████████▊| 39/40 [06:58<00:10, 10.58s/it]100%|██████████| 40/40 [07:08<00:00, 10.37s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:08<00:00, 10.37s/it]100%|██████████| 40/40 [07:08<00:00, 10.71s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_9/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 428.3613, 'train_samples_per_second': 6.081, 'train_steps_per_second': 0.093, 'train_loss': 0.755370807647705, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:08,  6.38s/it]  5%|▌         | 2/40 [00:12<04:00,  6.33s/it]  8%|▊         | 3/40 [00:18<03:53,  6.30s/it] 10%|█         | 4/40 [00:25<03:46,  6.29s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.33s/it] 15%|█▌        | 6/40 [00:37<03:29,  6.15s/it] 18%|█▊        | 7/40 [00:43<03:17,  5.99s/it] 20%|██        | 8/40 [00:49<03:12,  6.01s/it] 22%|██▎       | 9/40 [00:59<03:49,  7.41s/it] 25%|██▌       | 10/40 [01:05<03:30,  7.01s/it] 28%|██▊       | 11/40 [01:11<03:16,  6.78s/it] 30%|███       | 12/40 [01:18<03:05,  6.64s/it] 32%|███▎      | 13/40 [01:24<02:56,  6.55s/it] 35%|███▌      | 14/40 [01:30<02:47,  6.45s/it] 38%|███▊      | 15/40 [01:37<02:39,  6.37s/it] 40%|████      | 16/40 [01:43<02:32,  6.35s/it] 42%|████▎     | 17/40 [01:54<02:58,  7.74s/it] 45%|████▌     | 18/40 [02:00<02:42,  7.39s/it] 48%|████▊     | 19/40 [02:07<02:27,  7.01s/it] 50%|█████     | 20/40 [02:13<02:15,  6.77s/it] 52%|█████▎    | 21/40 [02:19<02:06,  6.65s/it] 55%|█████▌    | 22/40 [02:25<01:57,  6.56s/it] 57%|█████▊    | 23/40 [02:32<01:49,  6.47s/it] 60%|██████    | 24/40 [02:38<01:41,  6.37s/it] 62%|██████▎   | 25/40 [02:49<01:55,  7.73s/it] 65%|██████▌   | 26/40 [02:55<01:41,  7.26s/it] 68%|██████▊   | 27/40 [03:01<01:30,  6.97s/it] 70%|███████   | 28/40 [03:08<01:21,  6.79s/it] 72%|███████▎  | 29/40 [03:14<01:13,  6.65s/it] 75%|███████▌  | 30/40 [03:20<01:05,  6.55s/it] 78%|███████▊  | 31/40 [03:26<00:57,  6.44s/it] 80%|████████  | 32/40 [03:33<00:51,  6.46s/it] 82%|████████▎ | 33/40 [03:44<00:54,  7.76s/it] 85%|████████▌ | 34/40 [03:51<00:44,  7.49s/it] 88%|████████▊ | 35/40 [03:57<00:35,  7.19s/it] 90%|█████████ | 36/40 [04:04<00:27,  6.99s/it] 92%|█████████▎| 37/40 [04:10<00:20,  6.82s/it] 95%|█████████▌| 38/40 [04:16<00:13,  6.72s/it] 98%|█████████▊| 39/40 [04:23<00:06,  6.72s/it]100%|██████████| 40/40 [04:30<00:00,  6.65s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:30<00:00,  6.65s/it]100%|██████████| 40/40 [04:30<00:00,  6.75s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 270.1862, 'train_samples_per_second': 10.382, 'train_steps_per_second': 0.148, 'train_loss': 0.7463563919067383, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:22,  6.74s/it]  5%|▌         | 2/40 [00:13<04:13,  6.67s/it]  8%|▊         | 3/40 [00:19<04:01,  6.53s/it] 10%|█         | 4/40 [00:26<03:55,  6.54s/it] 12%|█▎        | 5/40 [00:32<03:48,  6.52s/it] 15%|█▌        | 6/40 [00:39<03:39,  6.44s/it] 18%|█▊        | 7/40 [00:45<03:30,  6.39s/it] 20%|██        | 8/40 [00:52<03:28,  6.52s/it] 22%|██▎       | 9/40 [01:03<04:07,  7.98s/it] 25%|██▌       | 10/40 [01:09<03:44,  7.47s/it] 28%|██▊       | 11/40 [01:16<03:28,  7.18s/it] 30%|███       | 12/40 [01:22<03:13,  6.91s/it] 32%|███▎      | 13/40 [01:28<03:02,  6.77s/it] 35%|███▌      | 14/40 [01:35<02:52,  6.64s/it] 38%|███▊      | 15/40 [01:41<02:42,  6.50s/it] 40%|████      | 16/40 [01:47<02:34,  6.46s/it] 42%|████▎     | 17/40 [01:58<02:59,  7.82s/it] 45%|████▌     | 18/40 [02:05<02:41,  7.34s/it] 48%|████▊     | 19/40 [02:11<02:26,  6.98s/it] 50%|█████     | 20/40 [02:17<02:15,  6.79s/it] 52%|█████▎    | 21/40 [02:23<02:05,  6.61s/it] 55%|█████▌    | 22/40 [02:29<01:57,  6.50s/it] 57%|█████▊    | 23/40 [02:36<01:48,  6.38s/it] 60%|██████    | 24/40 [02:42<01:40,  6.29s/it] 62%|██████▎   | 25/40 [02:53<01:55,  7.69s/it] 65%|██████▌   | 26/40 [02:59<01:41,  7.22s/it] 68%|██████▊   | 27/40 [03:05<01:29,  6.89s/it] 70%|███████   | 28/40 [03:11<01:20,  6.72s/it] 72%|███████▎  | 29/40 [03:18<01:13,  6.69s/it] 75%|███████▌  | 30/40 [03:24<01:05,  6.59s/it] 78%|███████▊  | 31/40 [03:30<00:58,  6.50s/it] 80%|████████  | 32/40 [03:37<00:52,  6.52s/it] 82%|████████▎ | 33/40 [03:48<00:54,  7.81s/it] 85%|████████▌ | 34/40 [03:54<00:44,  7.43s/it] 88%|████████▊ | 35/40 [04:00<00:35,  7.05s/it] 90%|█████████ | 36/40 [04:07<00:27,  6.84s/it] 92%|█████████▎| 37/40 [04:13<00:19,  6.62s/it] 95%|█████████▌| 38/40 [04:19<00:13,  6.54s/it] 98%|█████████▊| 39/40 [04:26<00:06,  6.50s/it]100%|██████████| 40/40 [04:32<00:00,  6.40s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:32<00:00,  6.40s/it]100%|██████████| 40/40 [04:32<00:00,  6.81s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 272.3628, 'train_samples_per_second': 10.299, 'train_steps_per_second': 0.147, 'train_loss': 0.7508327484130859, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:42,  8.77s/it]  5%|▌         | 2/40 [00:17<05:31,  8.73s/it]  8%|▊         | 3/40 [00:26<05:22,  8.71s/it] 10%|█         | 4/40 [00:35<05:15,  8.77s/it] 12%|█▎        | 5/40 [00:43<05:07,  8.78s/it] 15%|█▌        | 6/40 [00:52<04:53,  8.63s/it] 18%|█▊        | 7/40 [01:00<04:39,  8.48s/it] 20%|██        | 8/40 [01:09<04:33,  8.55s/it] 22%|██▎       | 9/40 [01:24<05:28, 10.60s/it] 25%|██▌       | 10/40 [01:32<04:57,  9.93s/it] 28%|██▊       | 11/40 [01:41<04:35,  9.50s/it] 30%|███       | 12/40 [01:49<04:19,  9.28s/it] 32%|███▎      | 13/40 [01:58<04:06,  9.13s/it] 35%|███▌      | 14/40 [02:07<03:54,  9.03s/it] 38%|███▊      | 15/40 [02:15<03:41,  8.85s/it] 40%|████      | 16/40 [02:24<03:30,  8.78s/it] 42%|████▎     | 17/40 [02:39<04:05, 10.67s/it] 45%|████▌     | 18/40 [02:48<03:40, 10.02s/it] 48%|████▊     | 19/40 [02:56<03:20,  9.54s/it] 50%|█████     | 20/40 [03:05<03:06,  9.32s/it] 52%|█████▎    | 21/40 [03:13<02:52,  9.09s/it] 55%|█████▌    | 22/40 [03:22<02:42,  9.00s/it] 57%|█████▊    | 23/40 [03:31<02:30,  8.85s/it] 60%|██████    | 24/40 [03:39<02:20,  8.76s/it] 62%|██████▎   | 25/40 [03:54<02:39, 10.66s/it] 65%|██████▌   | 26/40 [04:03<02:19,  9.96s/it] 68%|██████▊   | 27/40 [04:11<02:03,  9.51s/it] 70%|███████   | 28/40 [04:20<01:51,  9.26s/it] 72%|███████▎  | 29/40 [04:29<01:40,  9.13s/it] 75%|███████▌  | 30/40 [04:37<01:29,  8.99s/it] 78%|███████▊  | 31/40 [04:46<01:19,  8.83s/it] 80%|████████  | 32/40 [04:55<01:11,  8.91s/it] 82%|████████▎ | 33/40 [05:10<01:16, 10.88s/it] 85%|████████▌ | 34/40 [05:20<01:02, 10.45s/it] 88%|████████▊ | 35/40 [05:29<00:49,  9.96s/it] 90%|█████████ | 36/40 [05:38<00:38,  9.67s/it] 92%|█████████▎| 37/40 [05:47<00:28,  9.49s/it] 95%|█████████▌| 38/40 [05:56<00:18,  9.33s/it] 98%|█████████▊| 39/40 [06:05<00:09,  9.24s/it]100%|██████████| 40/40 [06:13<00:00,  9.06s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:13<00:00,  9.06s/it]100%|██████████| 40/40 [06:13<00:00,  9.34s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 373.7339, 'train_samples_per_second': 7.505, 'train_steps_per_second': 0.107, 'train_loss': 0.7575536727905273, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_6.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:10<06:37, 10.20s/it]  5%|▌         | 2/40 [00:20<06:19, 10.00s/it]  8%|▊         | 3/40 [00:29<06:08,  9.96s/it] 10%|█         | 4/40 [00:39<05:59,  9.98s/it] 12%|█▎        | 5/40 [00:50<05:51, 10.06s/it] 15%|█▌        | 6/40 [00:59<05:35,  9.87s/it] 18%|█▊        | 7/40 [01:08<05:18,  9.66s/it] 20%|██        | 8/40 [01:19<05:14,  9.81s/it] 22%|██▎       | 9/40 [01:36<06:14, 12.09s/it] 25%|██▌       | 10/40 [01:45<05:36, 11.23s/it] 28%|██▊       | 11/40 [01:54<05:10, 10.69s/it] 30%|███       | 12/40 [02:04<04:50, 10.39s/it] 32%|███▎      | 13/40 [02:14<04:34, 10.18s/it] 35%|███▌      | 14/40 [02:23<04:18,  9.94s/it] 38%|███▊      | 15/40 [02:32<04:02,  9.68s/it] 40%|████      | 16/40 [02:42<03:50,  9.62s/it] 42%|████▎     | 17/40 [02:58<04:29, 11.71s/it] 45%|████▌     | 18/40 [03:08<04:02, 11.00s/it] 48%|████▊     | 19/40 [03:17<03:40, 10.51s/it] 50%|█████     | 20/40 [03:27<03:25, 10.25s/it] 52%|█████▎    | 21/40 [03:36<03:11, 10.07s/it] 55%|█████▌    | 22/40 [03:46<02:59, 10.00s/it] 57%|█████▊    | 23/40 [03:56<02:48,  9.89s/it] 60%|██████    | 24/40 [04:05<02:35,  9.70s/it] 62%|██████▎   | 25/40 [04:22<02:57, 11.84s/it] 65%|██████▌   | 26/40 [04:31<02:35, 11.14s/it] 68%|██████▊   | 27/40 [04:41<02:17, 10.55s/it] 70%|███████   | 28/40 [04:50<02:02, 10.19s/it] 72%|███████▎  | 29/40 [04:59<01:49,  9.97s/it] 75%|███████▌  | 30/40 [05:09<01:39,  9.97s/it] 78%|███████▊  | 31/40 [05:18<01:27,  9.70s/it] 80%|████████  | 32/40 [05:28<01:17,  9.71s/it] 82%|████████▎ | 33/40 [05:44<01:21, 11.60s/it] 85%|████████▌ | 34/40 [05:54<01:06, 11.08s/it] 88%|████████▊ | 35/40 [06:03<00:52, 10.47s/it] 90%|█████████ | 36/40 [06:12<00:40, 10.13s/it] 92%|█████████▎| 37/40 [06:22<00:29,  9.84s/it] 95%|█████████▌| 38/40 [06:31<00:19,  9.78s/it] 98%|█████████▊| 39/40 [06:41<00:09,  9.73s/it]100%|██████████| 40/40 [06:50<00:00,  9.49s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:50<00:00,  9.49s/it]100%|██████████| 40/40 [06:50<00:00, 10.26s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_6/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_6/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_6/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 410.2827, 'train_samples_per_second': 6.837, 'train_steps_per_second': 0.097, 'train_loss': 0.7827349662780761, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:45,  5.79s/it]  5%|▌         | 2/40 [00:11<03:44,  5.92s/it]  8%|▊         | 3/40 [00:17<03:40,  5.97s/it] 10%|█         | 4/40 [00:23<03:36,  6.03s/it] 12%|█▎        | 5/40 [00:30<03:32,  6.08s/it] 15%|█▌        | 6/40 [00:36<03:26,  6.09s/it] 18%|█▊        | 7/40 [00:42<03:23,  6.16s/it] 20%|██        | 8/40 [00:49<03:20,  6.26s/it] 22%|██▎       | 9/40 [00:59<03:52,  7.51s/it] 25%|██▌       | 10/40 [01:05<03:31,  7.06s/it] 28%|██▊       | 11/40 [01:11<03:14,  6.71s/it] 30%|███       | 12/40 [01:17<03:03,  6.56s/it] 32%|███▎      | 13/40 [01:23<02:52,  6.40s/it] 35%|███▌      | 14/40 [01:29<02:43,  6.30s/it] 38%|███▊      | 15/40 [01:35<02:33,  6.12s/it] 40%|████      | 16/40 [01:41<02:29,  6.21s/it] 42%|████▎     | 17/40 [01:52<02:52,  7.48s/it] 45%|████▌     | 18/40 [01:58<02:34,  7.00s/it] 48%|████▊     | 19/40 [02:04<02:21,  6.73s/it] 50%|█████     | 20/40 [02:10<02:14,  6.72s/it] 52%|█████▎    | 21/40 [02:17<02:05,  6.62s/it] 55%|█████▌    | 22/40 [02:23<01:58,  6.59s/it] 57%|█████▊    | 23/40 [02:29<01:50,  6.48s/it] 60%|██████    | 24/40 [02:36<01:44,  6.52s/it] 62%|██████▎   | 25/40 [02:47<01:59,  7.95s/it] 65%|██████▌   | 26/40 [02:53<01:43,  7.40s/it] 68%|██████▊   | 27/40 [03:00<01:31,  7.04s/it] 70%|███████   | 28/40 [03:06<01:21,  6.77s/it] 72%|███████▎  | 29/40 [03:12<01:13,  6.70s/it] 75%|███████▌  | 30/40 [03:19<01:06,  6.60s/it] 78%|███████▊  | 31/40 [03:25<00:58,  6.53s/it] 80%|████████  | 32/40 [03:32<00:52,  6.52s/it] 82%|████████▎ | 33/40 [03:42<00:54,  7.74s/it] 85%|████████▌ | 34/40 [03:49<00:44,  7.36s/it] 88%|████████▊ | 35/40 [03:55<00:35,  7.00s/it] 90%|█████████ | 36/40 [04:01<00:26,  6.72s/it] 92%|█████████▎| 37/40 [04:07<00:19,  6.53s/it] 95%|█████████▌| 38/40 [04:13<00:12,  6.47s/it] 98%|█████████▊| 39/40 [04:20<00:06,  6.44s/it]100%|██████████| 40/40 [04:26<00:00,  6.32s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:26<00:00,  6.32s/it]100%|██████████| 40/40 [04:26<00:00,  6.65s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 266.1657, 'train_samples_per_second': 10.539, 'train_steps_per_second': 0.15, 'train_loss': 0.7461502075195312, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:54,  6.01s/it]  5%|▌         | 2/40 [00:12<03:49,  6.03s/it]  8%|▊         | 3/40 [00:18<03:45,  6.10s/it] 10%|█         | 4/40 [00:24<03:38,  6.07s/it] 12%|█▎        | 5/40 [00:30<03:35,  6.15s/it] 15%|█▌        | 6/40 [00:36<03:25,  6.03s/it] 18%|█▊        | 7/40 [00:42<03:19,  6.05s/it] 20%|██        | 8/40 [00:48<03:14,  6.08s/it] 22%|██▎       | 9/40 [00:58<03:47,  7.33s/it] 25%|██▌       | 10/40 [01:04<03:27,  6.91s/it] 28%|██▊       | 11/40 [01:10<03:10,  6.57s/it] 30%|███       | 12/40 [01:16<03:00,  6.46s/it] 32%|███▎      | 13/40 [01:22<02:50,  6.30s/it] 35%|███▌      | 14/40 [01:28<02:41,  6.23s/it] 38%|███▊      | 15/40 [01:34<02:31,  6.06s/it] 40%|████      | 16/40 [01:41<02:30,  6.26s/it] 42%|████▎     | 17/40 [01:51<02:55,  7.64s/it] 45%|████▌     | 18/40 [01:57<02:36,  7.13s/it] 48%|████▊     | 19/40 [02:03<02:22,  6.76s/it] 50%|█████     | 20/40 [02:09<02:11,  6.55s/it] 52%|█████▎    | 21/40 [02:15<02:00,  6.33s/it] 55%|█████▌    | 22/40 [02:21<01:53,  6.28s/it] 57%|█████▊    | 23/40 [02:27<01:44,  6.17s/it] 60%|██████    | 24/40 [02:33<01:38,  6.16s/it] 62%|██████▎   | 25/40 [02:44<01:51,  7.46s/it] 65%|██████▌   | 26/40 [02:50<01:38,  7.02s/it] 68%|██████▊   | 27/40 [02:56<01:27,  6.73s/it] 70%|███████   | 28/40 [03:02<01:17,  6.49s/it] 72%|███████▎  | 29/40 [03:08<01:09,  6.32s/it] 75%|███████▌  | 30/40 [03:14<01:02,  6.20s/it] 78%|███████▊  | 31/40 [03:19<00:54,  6.07s/it] 80%|████████  | 32/40 [03:25<00:48,  6.07s/it] 82%|████████▎ | 33/40 [03:36<00:51,  7.30s/it] 85%|████████▌ | 34/40 [03:42<00:41,  6.96s/it] 88%|████████▊ | 35/40 [03:48<00:33,  6.64s/it] 90%|█████████ | 36/40 [03:53<00:25,  6.35s/it] 92%|█████████▎| 37/40 [03:59<00:18,  6.17s/it] 95%|█████████▌| 38/40 [04:05<00:12,  6.12s/it] 98%|█████████▊| 39/40 [04:11<00:06,  6.14s/it]100%|██████████| 40/40 [04:17<00:00,  6.03s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:17<00:00,  6.03s/it]100%|██████████| 40/40 [04:17<00:00,  6.44s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 257.5849, 'train_samples_per_second': 10.89, 'train_steps_per_second': 0.155, 'train_loss': 0.7505878448486328, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:26,  8.38s/it]  5%|▌         | 2/40 [00:16<05:17,  8.35s/it]  8%|▊         | 3/40 [00:25<05:13,  8.46s/it] 10%|█         | 4/40 [00:33<05:03,  8.44s/it] 12%|█▎        | 5/40 [00:42<04:59,  8.55s/it] 15%|█▌        | 6/40 [00:50<04:43,  8.33s/it] 18%|█▊        | 7/40 [00:58<04:36,  8.39s/it] 20%|██        | 8/40 [01:07<04:29,  8.42s/it] 22%|██▎       | 9/40 [01:21<05:15, 10.17s/it] 25%|██▌       | 10/40 [01:29<04:50,  9.68s/it] 28%|██▊       | 11/40 [01:38<04:28,  9.26s/it] 30%|███       | 12/40 [01:47<04:15,  9.11s/it] 32%|███▎      | 13/40 [01:55<03:59,  8.86s/it] 35%|███▌      | 14/40 [02:03<03:47,  8.75s/it] 38%|███▊      | 15/40 [02:11<03:32,  8.48s/it] 40%|████      | 16/40 [02:20<03:27,  8.66s/it] 42%|████▎     | 17/40 [02:35<04:00, 10.44s/it] 45%|████▌     | 18/40 [02:43<03:35,  9.79s/it] 48%|████▊     | 19/40 [02:51<03:16,  9.34s/it] 50%|█████     | 20/40 [03:00<03:01,  9.05s/it] 52%|█████▎    | 21/40 [03:08<02:46,  8.76s/it] 55%|█████▌    | 22/40 [03:17<02:37,  8.74s/it] 57%|█████▊    | 23/40 [03:25<02:26,  8.61s/it] 60%|██████    | 24/40 [03:34<02:17,  8.62s/it] 62%|██████▎   | 25/40 [03:49<02:39, 10.65s/it] 65%|██████▌   | 26/40 [03:57<02:19,  9.99s/it] 68%|██████▊   | 27/40 [04:06<02:05,  9.66s/it] 70%|███████   | 28/40 [04:15<01:52,  9.38s/it] 72%|███████▎  | 29/40 [04:24<01:42,  9.35s/it] 75%|███████▌  | 30/40 [04:33<01:31,  9.17s/it] 78%|███████▊  | 31/40 [04:41<01:19,  8.87s/it] 80%|████████  | 32/40 [04:50<01:10,  8.82s/it] 82%|████████▎ | 33/40 [05:04<01:13, 10.51s/it] 85%|████████▌ | 34/40 [05:13<01:00, 10.04s/it] 88%|████████▊ | 35/40 [05:22<00:47,  9.59s/it] 90%|█████████ | 36/40 [05:30<00:36,  9.20s/it] 92%|█████████▎| 37/40 [05:39<00:26,  8.96s/it] 95%|█████████▌| 38/40 [05:47<00:17,  8.92s/it] 98%|█████████▊| 39/40 [05:56<00:08,  8.95s/it]100%|██████████| 40/40 [06:05<00:00,  8.75s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:05<00:00,  8.75s/it]100%|██████████| 40/40 [06:05<00:00,  9.13s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 365.1329, 'train_samples_per_second': 7.682, 'train_steps_per_second': 0.11, 'train_loss': 0.7573277473449707, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_1.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:14,  9.60s/it]  5%|▌         | 2/40 [00:19<06:04,  9.59s/it]  8%|▊         | 3/40 [00:28<05:57,  9.66s/it] 10%|█         | 4/40 [00:38<05:47,  9.64s/it] 12%|█▎        | 5/40 [00:48<05:40,  9.73s/it] 15%|█▌        | 6/40 [00:57<05:24,  9.53s/it] 18%|█▊        | 7/40 [01:07<05:16,  9.58s/it] 20%|██        | 8/40 [01:16<05:07,  9.61s/it] 22%|██▎       | 9/40 [01:33<06:01, 11.65s/it] 25%|██▌       | 10/40 [01:42<05:29, 10.99s/it] 28%|██▊       | 11/40 [01:51<05:04, 10.51s/it] 30%|███       | 12/40 [02:01<04:48, 10.30s/it] 32%|███▎      | 13/40 [02:11<04:31, 10.06s/it] 35%|███▌      | 14/40 [02:20<04:18,  9.94s/it] 38%|███▊      | 15/40 [02:29<04:01,  9.66s/it] 40%|████      | 16/40 [02:40<03:57,  9.88s/it] 42%|████▎     | 17/40 [02:57<04:37, 12.07s/it] 45%|████▌     | 18/40 [03:06<04:08, 11.27s/it] 48%|████▊     | 19/40 [03:16<03:45, 10.74s/it] 50%|█████     | 20/40 [03:26<03:29, 10.48s/it] 52%|█████▎    | 21/40 [03:35<03:12, 10.11s/it] 55%|█████▌    | 22/40 [03:45<02:59,  9.96s/it] 57%|█████▊    | 23/40 [03:54<02:45,  9.72s/it] 60%|██████    | 24/40 [04:04<02:35,  9.70s/it] 62%|██████▎   | 25/40 [04:20<02:56, 11.78s/it] 65%|██████▌   | 26/40 [04:29<02:33, 11.00s/it] 68%|██████▊   | 27/40 [04:39<02:17, 10.54s/it] 70%|███████   | 28/40 [04:48<02:01, 10.16s/it] 72%|███████▎  | 29/40 [04:57<01:49,  9.93s/it] 75%|███████▌  | 30/40 [05:07<01:37,  9.74s/it] 78%|███████▊  | 31/40 [05:16<01:25,  9.53s/it] 80%|████████  | 32/40 [05:25<01:16,  9.56s/it] 82%|████████▎ | 33/40 [05:41<01:20, 11.49s/it] 85%|████████▌ | 34/40 [05:51<01:05, 10.97s/it] 88%|████████▊ | 35/40 [06:00<00:52, 10.47s/it] 90%|█████████ | 36/40 [06:10<00:40, 10.11s/it] 92%|█████████▎| 37/40 [06:19<00:29,  9.85s/it] 95%|█████████▌| 38/40 [06:29<00:19,  9.78s/it] 98%|█████████▊| 39/40 [06:38<00:09,  9.79s/it]100%|██████████| 40/40 [06:48<00:00,  9.58s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:48<00:00,  9.58s/it]100%|██████████| 40/40 [06:48<00:00, 10.20s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_1/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_1/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_1/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 408.0246, 'train_samples_per_second': 6.875, 'train_steps_per_second': 0.098, 'train_loss': 0.7829066276550293, 'epoch': 4.91}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:53,  6.00s/it]  5%|▌         | 2/40 [00:12<03:59,  6.29s/it]  8%|▊         | 3/40 [00:18<03:53,  6.32s/it] 10%|█         | 4/40 [00:24<03:44,  6.24s/it] 12%|█▎        | 5/40 [00:31<03:42,  6.36s/it] 15%|█▌        | 6/40 [00:37<03:35,  6.35s/it] 18%|█▊        | 7/40 [00:44<03:29,  6.35s/it] 20%|██        | 8/40 [00:50<03:25,  6.42s/it] 22%|██▎       | 9/40 [00:58<03:29,  6.76s/it] 25%|██▌       | 10/40 [01:05<03:25,  6.84s/it] 28%|██▊       | 11/40 [01:12<03:17,  6.82s/it] 30%|███       | 12/40 [01:18<03:10,  6.80s/it] 32%|███▎      | 13/40 [01:25<03:03,  6.78s/it] 35%|███▌      | 14/40 [01:32<02:54,  6.71s/it] 38%|███▊      | 15/40 [01:38<02:47,  6.69s/it] 40%|████      | 16/40 [01:45<02:41,  6.75s/it] 42%|████▎     | 17/40 [01:53<02:39,  6.95s/it] 45%|████▌     | 18/40 [01:59<02:29,  6.79s/it] 48%|████▊     | 19/40 [02:06<02:21,  6.73s/it] 50%|█████     | 20/40 [02:12<02:13,  6.68s/it] 52%|█████▎    | 21/40 [02:19<02:05,  6.62s/it] 55%|█████▌    | 22/40 [02:25<01:57,  6.53s/it] 57%|█████▊    | 23/40 [02:31<01:50,  6.52s/it] 60%|██████    | 24/40 [02:38<01:44,  6.52s/it] 62%|██████▎   | 25/40 [02:45<01:40,  6.73s/it] 65%|██████▌   | 26/40 [02:52<01:32,  6.64s/it] 68%|██████▊   | 27/40 [02:58<01:26,  6.69s/it] 70%|███████   | 28/40 [03:05<01:18,  6.51s/it] 72%|███████▎  | 29/40 [03:11<01:11,  6.50s/it] 75%|███████▌  | 30/40 [03:17<01:04,  6.47s/it] 78%|███████▊  | 31/40 [03:24<00:58,  6.47s/it] 80%|████████  | 32/40 [03:30<00:51,  6.45s/it] 82%|████████▎ | 33/40 [03:38<00:47,  6.79s/it] 85%|████████▌ | 34/40 [03:44<00:39,  6.64s/it] 88%|████████▊ | 35/40 [03:51<00:33,  6.73s/it] 90%|█████████ | 36/40 [03:57<00:26,  6.60s/it] 92%|█████████▎| 37/40 [04:04<00:19,  6.57s/it] 95%|█████████▌| 38/40 [04:11<00:13,  6.60s/it] 98%|█████████▊| 39/40 [04:17<00:06,  6.62s/it]100%|██████████| 40/40 [04:23<00:00,  6.51s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:23<00:00,  6.51s/it]100%|██████████| 40/40 [04:23<00:00,  6.60s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 263.9458, 'train_samples_per_second': 9.869, 'train_steps_per_second': 0.152, 'train_loss': 0.6989629745483399, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:10,  6.43s/it]  5%|▌         | 2/40 [00:13<04:12,  6.64s/it]  8%|▊         | 3/40 [00:19<04:06,  6.67s/it] 10%|█         | 4/40 [00:26<03:57,  6.60s/it] 12%|█▎        | 5/40 [00:33<03:53,  6.68s/it] 15%|█▌        | 6/40 [00:39<03:46,  6.66s/it] 18%|█▊        | 7/40 [00:46<03:39,  6.65s/it] 20%|██        | 8/40 [00:53<03:34,  6.69s/it] 22%|██▎       | 9/40 [01:00<03:36,  7.00s/it] 25%|██▌       | 10/40 [01:07<03:25,  6.86s/it] 28%|██▊       | 11/40 [01:14<03:17,  6.82s/it] 30%|███       | 12/40 [01:20<03:09,  6.78s/it] 32%|███▎      | 13/40 [01:27<03:01,  6.74s/it] 35%|███▌      | 14/40 [01:34<02:53,  6.67s/it] 38%|███▊      | 15/40 [01:40<02:47,  6.71s/it] 40%|████      | 16/40 [01:47<02:40,  6.70s/it] 42%|████▎     | 17/40 [01:54<02:38,  6.91s/it] 45%|████▌     | 18/40 [02:01<02:29,  6.81s/it] 48%|████▊     | 19/40 [02:08<02:22,  6.79s/it] 50%|█████     | 20/40 [02:15<02:15,  6.79s/it] 52%|█████▎    | 21/40 [02:21<02:08,  6.78s/it] 55%|█████▌    | 22/40 [02:28<02:01,  6.73s/it] 57%|█████▊    | 23/40 [02:35<01:54,  6.74s/it] 60%|██████    | 24/40 [02:42<01:48,  6.77s/it] 62%|██████▎   | 25/40 [02:49<01:44,  6.97s/it] 65%|██████▌   | 26/40 [02:56<01:36,  6.91s/it] 68%|██████▊   | 27/40 [03:02<01:29,  6.87s/it] 70%|███████   | 28/40 [03:09<01:21,  6.78s/it] 72%|███████▎  | 29/40 [03:16<01:14,  6.79s/it] 75%|███████▌  | 30/40 [03:23<01:07,  6.77s/it] 78%|███████▊  | 31/40 [03:29<01:00,  6.77s/it] 80%|████████  | 32/40 [03:36<00:54,  6.77s/it] 82%|████████▎ | 33/40 [03:44<00:49,  7.10s/it] 85%|████████▌ | 34/40 [03:51<00:42,  7.04s/it] 88%|████████▊ | 35/40 [03:58<00:35,  7.07s/it] 90%|█████████ | 36/40 [04:05<00:27,  6.94s/it] 92%|█████████▎| 37/40 [04:12<00:20,  6.92s/it] 95%|█████████▌| 38/40 [04:18<00:13,  6.89s/it] 98%|█████████▊| 39/40 [04:25<00:06,  6.82s/it]100%|██████████| 40/40 [04:31<00:00,  6.67s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:31<00:00,  6.67s/it]100%|██████████| 40/40 [04:31<00:00,  6.80s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 271.8825, 'train_samples_per_second': 9.581, 'train_steps_per_second': 0.147, 'train_loss': 0.7157644271850586, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:48,  8.95s/it]  5%|▌         | 2/40 [00:18<05:56,  9.39s/it]  8%|▊         | 3/40 [00:28<05:47,  9.40s/it] 10%|█         | 4/40 [00:37<05:35,  9.33s/it] 12%|█▎        | 5/40 [00:46<05:31,  9.46s/it] 15%|█▌        | 6/40 [00:56<05:21,  9.47s/it] 18%|█▊        | 7/40 [01:05<05:11,  9.45s/it] 20%|██        | 8/40 [01:15<05:07,  9.61s/it] 22%|██▎       | 9/40 [01:26<05:11, 10.06s/it] 25%|██▌       | 10/40 [01:36<04:56,  9.87s/it] 28%|██▊       | 11/40 [01:45<04:43,  9.76s/it] 30%|███       | 12/40 [01:55<04:31,  9.71s/it] 32%|███▎      | 13/40 [02:04<04:19,  9.62s/it] 35%|███▌      | 14/40 [02:13<04:05,  9.46s/it] 38%|███▊      | 15/40 [02:23<03:58,  9.53s/it] 40%|████      | 16/40 [02:33<03:50,  9.61s/it] 42%|████▎     | 17/40 [02:44<03:48,  9.94s/it] 45%|████▌     | 18/40 [02:53<03:36,  9.84s/it] 48%|████▊     | 19/40 [03:03<03:27,  9.88s/it] 50%|█████     | 20/40 [03:13<03:17,  9.87s/it] 52%|█████▎    | 21/40 [03:23<03:06,  9.80s/it] 55%|█████▌    | 22/40 [03:32<02:54,  9.69s/it] 57%|█████▊    | 23/40 [03:42<02:44,  9.69s/it] 60%|██████    | 24/40 [03:52<02:36,  9.79s/it] 62%|██████▎   | 25/40 [04:03<02:30, 10.06s/it] 65%|██████▌   | 26/40 [04:12<02:17,  9.85s/it] 68%|██████▊   | 27/40 [04:22<02:07,  9.79s/it] 70%|███████   | 28/40 [04:31<01:54,  9.57s/it] 72%|███████▎  | 29/40 [04:40<01:45,  9.61s/it] 75%|███████▌  | 30/40 [04:50<01:36,  9.69s/it] 78%|███████▊  | 31/40 [05:00<01:27,  9.69s/it] 80%|████████  | 32/40 [05:10<01:17,  9.70s/it] 82%|████████▎ | 33/40 [05:21<01:11, 10.20s/it] 85%|████████▌ | 34/40 [05:30<00:59,  9.98s/it] 88%|████████▊ | 35/40 [05:41<00:50, 10.03s/it] 90%|█████████ | 36/40 [05:50<00:39,  9.85s/it] 92%|█████████▎| 37/40 [05:59<00:29,  9.73s/it] 95%|█████████▌| 38/40 [06:09<00:19,  9.71s/it] 98%|█████████▊| 39/40 [06:19<00:09,  9.64s/it]100%|██████████| 40/40 [06:28<00:00,  9.42s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:28<00:00,  9.42s/it]100%|██████████| 40/40 [06:28<00:00,  9.70s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 388.0034, 'train_samples_per_second': 6.714, 'train_steps_per_second': 0.103, 'train_loss': 0.6985019683837891, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_10.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:29, 10.00s/it]  5%|▌         | 2/40 [00:20<06:35, 10.40s/it]  8%|▊         | 3/40 [00:31<06:26, 10.44s/it] 10%|█         | 4/40 [00:41<06:12, 10.36s/it] 12%|█▎        | 5/40 [00:52<06:07, 10.51s/it] 15%|█▌        | 6/40 [01:02<05:57, 10.52s/it] 18%|█▊        | 7/40 [01:13<05:50, 10.61s/it] 20%|██        | 8/40 [01:24<05:43, 10.73s/it] 22%|██▎       | 9/40 [01:36<05:46, 11.19s/it] 25%|██▌       | 10/40 [01:47<05:27, 10.93s/it] 28%|██▊       | 11/40 [01:58<05:17, 10.95s/it] 30%|███       | 12/40 [02:08<05:03, 10.83s/it] 32%|███▎      | 13/40 [02:19<04:50, 10.78s/it] 35%|███▌      | 14/40 [02:29<04:36, 10.63s/it] 38%|███▊      | 15/40 [02:40<04:24, 10.58s/it] 40%|████      | 16/40 [02:50<04:14, 10.62s/it] 42%|████▎     | 17/40 [03:02<04:15, 11.09s/it] 45%|████▌     | 18/40 [03:13<04:00, 10.94s/it] 48%|████▊     | 19/40 [03:24<03:48, 10.87s/it] 50%|█████     | 20/40 [03:34<03:35, 10.77s/it] 52%|█████▎    | 21/40 [03:45<03:23, 10.72s/it] 55%|█████▌    | 22/40 [03:55<03:11, 10.64s/it] 57%|█████▊    | 23/40 [04:06<02:59, 10.58s/it] 60%|██████    | 24/40 [04:17<02:50, 10.68s/it] 62%|██████▎   | 25/40 [04:28<02:44, 10.99s/it] 65%|██████▌   | 26/40 [04:39<02:31, 10.82s/it] 68%|██████▊   | 27/40 [04:49<02:19, 10.77s/it] 70%|███████   | 28/40 [05:00<02:07, 10.59s/it] 72%|███████▎  | 29/40 [05:10<01:56, 10.62s/it] 75%|███████▌  | 30/40 [05:21<01:45, 10.57s/it] 78%|███████▊  | 31/40 [05:31<01:35, 10.58s/it] 80%|████████  | 32/40 [05:42<01:24, 10.60s/it] 82%|████████▎ | 33/40 [05:55<01:18, 11.18s/it] 85%|████████▌ | 34/40 [06:05<01:06, 11.04s/it] 88%|████████▊ | 35/40 [06:17<00:55, 11.15s/it] 90%|█████████ | 36/40 [06:27<00:43, 10.96s/it] 92%|█████████▎| 37/40 [06:38<00:32, 10.87s/it] 95%|█████████▌| 38/40 [06:49<00:21, 10.92s/it] 98%|█████████▊| 39/40 [07:00<00:10, 10.87s/it]100%|██████████| 40/40 [07:10<00:00, 10.75s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:10<00:00, 10.75s/it]100%|██████████| 40/40 [07:10<00:00, 10.77s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_10/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_10/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_10/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 430.6113, 'train_samples_per_second': 6.05, 'train_steps_per_second': 0.093, 'train_loss': 0.7552803039550782, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:17,  6.60s/it]  5%|▌         | 2/40 [00:13<04:25,  6.99s/it]  8%|▊         | 3/40 [00:20<04:14,  6.87s/it] 10%|█         | 4/40 [00:27<04:06,  6.86s/it] 12%|█▎        | 5/40 [00:34<04:01,  6.91s/it] 15%|█▌        | 6/40 [00:41<03:55,  6.94s/it] 18%|█▊        | 7/40 [00:48<03:48,  6.92s/it] 20%|██        | 8/40 [00:55<03:43,  6.97s/it] 22%|██▎       | 9/40 [01:03<03:45,  7.27s/it] 25%|██▌       | 10/40 [01:09<03:30,  7.01s/it] 28%|██▊       | 11/40 [01:16<03:22,  6.99s/it] 30%|███       | 12/40 [01:23<03:12,  6.86s/it] 32%|███▎      | 13/40 [01:29<03:02,  6.77s/it] 35%|███▌      | 14/40 [01:36<02:54,  6.71s/it] 38%|███▊      | 15/40 [01:43<02:47,  6.70s/it] 40%|████      | 16/40 [01:49<02:41,  6.74s/it] 42%|████▎     | 17/40 [01:57<02:40,  6.97s/it] 45%|████▌     | 18/40 [02:04<02:33,  6.99s/it] 48%|████▊     | 19/40 [02:11<02:24,  6.88s/it] 50%|█████     | 20/40 [02:17<02:12,  6.61s/it] 52%|█████▎    | 21/40 [02:23<02:04,  6.57s/it] 55%|█████▌    | 22/40 [02:29<01:57,  6.54s/it] 57%|█████▊    | 23/40 [02:36<01:51,  6.57s/it] 60%|██████    | 24/40 [02:43<01:46,  6.66s/it] 62%|██████▎   | 25/40 [02:51<01:44,  6.97s/it] 65%|██████▌   | 26/40 [02:57<01:36,  6.92s/it] 68%|██████▊   | 27/40 [03:04<01:30,  6.93s/it] 70%|███████   | 28/40 [03:11<01:21,  6.79s/it] 72%|███████▎  | 29/40 [03:18<01:14,  6.77s/it] 75%|███████▌  | 30/40 [03:25<01:08,  6.84s/it] 78%|███████▊  | 31/40 [03:32<01:01,  6.86s/it] 80%|████████  | 32/40 [03:38<00:54,  6.85s/it] 82%|████████▎ | 33/40 [03:47<00:51,  7.30s/it] 85%|████████▌ | 34/40 [03:54<00:43,  7.31s/it] 88%|████████▊ | 35/40 [04:01<00:36,  7.21s/it] 90%|█████████ | 36/40 [04:08<00:28,  7.03s/it] 92%|█████████▎| 37/40 [04:14<00:20,  6.95s/it] 95%|█████████▌| 38/40 [04:21<00:13,  6.83s/it] 98%|█████████▊| 39/40 [04:28<00:06,  6.84s/it]100%|██████████| 40/40 [04:35<00:00,  6.83s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:35<00:00,  6.83s/it]100%|██████████| 40/40 [04:35<00:00,  6.88s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 275.1106, 'train_samples_per_second': 9.469, 'train_steps_per_second': 0.145, 'train_loss': 0.6991260528564454, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:00,  6.17s/it]  5%|▌         | 2/40 [00:12<04:08,  6.54s/it]  8%|▊         | 3/40 [00:19<04:01,  6.53s/it] 10%|█         | 4/40 [00:26<03:55,  6.55s/it] 12%|█▎        | 5/40 [00:32<03:51,  6.62s/it] 15%|█▌        | 6/40 [00:39<03:45,  6.64s/it] 18%|█▊        | 7/40 [00:46<03:38,  6.62s/it] 20%|██        | 8/40 [00:52<03:33,  6.69s/it] 22%|██▎       | 9/40 [01:00<03:36,  7.00s/it] 25%|██▌       | 10/40 [01:07<03:25,  6.84s/it] 28%|██▊       | 11/40 [01:13<03:18,  6.86s/it] 30%|███       | 12/40 [01:20<03:10,  6.79s/it] 32%|███▎      | 13/40 [01:27<03:03,  6.80s/it] 35%|███▌      | 14/40 [01:34<02:55,  6.75s/it] 38%|███▊      | 15/40 [01:40<02:50,  6.80s/it] 40%|████      | 16/40 [01:47<02:43,  6.83s/it] 42%|████▎     | 17/40 [01:55<02:42,  7.05s/it] 45%|████▌     | 18/40 [02:02<02:35,  7.07s/it] 48%|████▊     | 19/40 [02:09<02:26,  6.98s/it] 50%|█████     | 20/40 [02:15<02:16,  6.84s/it] 52%|█████▎    | 21/40 [02:22<02:10,  6.86s/it] 55%|█████▌    | 22/40 [02:29<02:02,  6.83s/it] 57%|█████▊    | 23/40 [02:36<01:55,  6.79s/it] 60%|██████    | 24/40 [02:43<01:49,  6.87s/it] 62%|██████▎   | 25/40 [02:51<01:47,  7.14s/it] 65%|██████▌   | 26/40 [02:57<01:39,  7.08s/it] 68%|██████▊   | 27/40 [03:04<01:31,  7.05s/it] 70%|███████   | 28/40 [03:11<01:22,  6.86s/it] 72%|███████▎  | 29/40 [03:18<01:14,  6.82s/it] 75%|███████▌  | 30/40 [03:24<01:08,  6.83s/it] 78%|███████▊  | 31/40 [03:31<01:01,  6.87s/it] 80%|████████  | 32/40 [03:38<00:53,  6.71s/it] 82%|████████▎ | 33/40 [03:45<00:48,  6.99s/it] 85%|████████▌ | 34/40 [03:52<00:41,  6.86s/it] 88%|████████▊ | 35/40 [03:59<00:33,  6.77s/it] 90%|█████████ | 36/40 [04:05<00:26,  6.60s/it] 92%|█████████▎| 37/40 [04:11<00:19,  6.64s/it] 95%|█████████▌| 38/40 [04:18<00:13,  6.73s/it] 98%|█████████▊| 39/40 [04:25<00:06,  6.77s/it]100%|██████████| 40/40 [04:32<00:00,  6.75s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:32<00:00,  6.75s/it]100%|██████████| 40/40 [04:32<00:00,  6.81s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 272.4516, 'train_samples_per_second': 9.561, 'train_steps_per_second': 0.147, 'train_loss': 0.7157188415527344, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:35,  8.60s/it]  5%|▌         | 2/40 [00:18<05:57,  9.40s/it]  8%|▊         | 3/40 [00:27<05:46,  9.36s/it] 10%|█         | 4/40 [00:37<05:36,  9.36s/it] 12%|█▎        | 5/40 [00:46<05:28,  9.39s/it] 15%|█▌        | 6/40 [00:56<05:21,  9.44s/it] 18%|█▊        | 7/40 [01:05<05:08,  9.35s/it] 20%|██        | 8/40 [01:14<04:59,  9.37s/it] 22%|██▎       | 9/40 [01:25<05:04,  9.83s/it] 25%|██▌       | 10/40 [01:34<04:44,  9.48s/it] 28%|██▊       | 11/40 [01:43<04:33,  9.45s/it] 30%|███       | 12/40 [01:53<04:23,  9.41s/it] 32%|███▎      | 13/40 [02:02<04:15,  9.46s/it] 35%|███▌      | 14/40 [02:11<04:04,  9.41s/it] 38%|███▊      | 15/40 [02:21<03:57,  9.48s/it] 40%|████      | 16/40 [02:31<03:48,  9.54s/it] 42%|████▎     | 17/40 [02:41<03:46,  9.85s/it] 45%|████▌     | 18/40 [02:51<03:38,  9.91s/it] 48%|████▊     | 19/40 [03:01<03:27,  9.88s/it] 50%|█████     | 20/40 [03:10<03:10,  9.53s/it] 52%|█████▎    | 21/40 [03:19<03:00,  9.52s/it] 55%|█████▌    | 22/40 [03:29<02:50,  9.48s/it] 57%|█████▊    | 23/40 [03:38<02:40,  9.46s/it] 60%|██████    | 24/40 [03:48<02:34,  9.63s/it] 62%|██████▎   | 25/40 [03:59<02:30, 10.06s/it] 65%|██████▌   | 26/40 [04:09<02:19,  9.94s/it] 68%|██████▊   | 27/40 [04:19<02:09,  9.96s/it] 70%|███████   | 28/40 [04:28<01:56,  9.71s/it] 72%|███████▎  | 29/40 [04:37<01:45,  9.58s/it] 75%|███████▌  | 30/40 [04:47<01:35,  9.51s/it] 78%|███████▊  | 31/40 [04:56<01:25,  9.48s/it] 80%|████████  | 32/40 [05:05<01:15,  9.39s/it] 82%|████████▎ | 33/40 [05:16<01:09,  9.88s/it] 85%|████████▌ | 34/40 [05:26<00:59,  9.84s/it] 88%|████████▊ | 35/40 [05:36<00:49,  9.80s/it] 90%|█████████ | 36/40 [05:45<00:38,  9.60s/it] 92%|█████████▎| 37/40 [05:54<00:28,  9.54s/it] 95%|█████████▌| 38/40 [06:03<00:18,  9.40s/it] 98%|█████████▊| 39/40 [06:13<00:09,  9.47s/it]100%|██████████| 40/40 [06:23<00:00,  9.55s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:23<00:00,  9.55s/it]100%|██████████| 40/40 [06:23<00:00,  9.58s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 383.2482, 'train_samples_per_second': 6.797, 'train_steps_per_second': 0.104, 'train_loss': 0.6983644485473632, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_5.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:26,  9.90s/it]  5%|▌         | 2/40 [00:20<06:40, 10.54s/it]  8%|▊         | 3/40 [00:31<06:30, 10.56s/it] 10%|█         | 4/40 [00:41<06:15, 10.44s/it] 12%|█▎        | 5/40 [00:52<06:05, 10.45s/it] 15%|█▌        | 6/40 [01:02<05:55, 10.47s/it] 18%|█▊        | 7/40 [01:13<05:44, 10.43s/it] 20%|██        | 8/40 [01:23<05:36, 10.50s/it] 22%|██▎       | 9/40 [01:35<05:39, 10.96s/it] 25%|██▌       | 10/40 [01:45<05:21, 10.71s/it] 28%|██▊       | 11/40 [01:56<05:10, 10.70s/it] 30%|███       | 12/40 [02:06<04:56, 10.60s/it] 32%|███▎      | 13/40 [02:17<04:45, 10.57s/it] 35%|███▌      | 14/40 [02:27<04:32, 10.49s/it] 38%|███▊      | 15/40 [02:38<04:24, 10.60s/it] 40%|████      | 16/40 [02:49<04:15, 10.64s/it] 42%|████▎     | 17/40 [03:00<04:11, 10.95s/it] 45%|████▌     | 18/40 [03:12<04:01, 11.00s/it] 48%|████▊     | 19/40 [03:22<03:49, 10.92s/it] 50%|█████     | 20/40 [03:32<03:31, 10.57s/it] 52%|█████▎    | 21/40 [03:43<03:21, 10.60s/it] 55%|█████▌    | 22/40 [03:53<03:09, 10.51s/it] 57%|█████▊    | 23/40 [04:03<02:57, 10.45s/it] 60%|██████    | 24/40 [04:14<02:50, 10.63s/it] 62%|██████▎   | 25/40 [04:27<02:48, 11.27s/it] 65%|██████▌   | 26/40 [04:38<02:36, 11.21s/it] 68%|██████▊   | 27/40 [04:50<02:26, 11.25s/it] 70%|███████   | 28/40 [05:00<02:12, 11.02s/it] 72%|███████▎  | 29/40 [05:11<01:59, 10.87s/it] 75%|███████▌  | 30/40 [05:21<01:47, 10.78s/it] 78%|███████▊  | 31/40 [05:32<01:36, 10.74s/it] 80%|████████  | 32/40 [05:42<01:24, 10.55s/it] 82%|████████▎ | 33/40 [05:54<01:17, 11.02s/it] 85%|████████▌ | 34/40 [06:05<01:05, 10.91s/it] 88%|████████▊ | 35/40 [06:15<00:54, 10.87s/it] 90%|█████████ | 36/40 [06:26<00:43, 10.76s/it] 92%|█████████▎| 37/40 [06:36<00:31, 10.66s/it] 95%|█████████▌| 38/40 [06:47<00:21, 10.53s/it] 98%|█████████▊| 39/40 [06:57<00:10, 10.54s/it]100%|██████████| 40/40 [07:08<00:00, 10.56s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [07:08<00:00, 10.56s/it]100%|██████████| 40/40 [07:08<00:00, 10.71s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_5/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_5/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_5/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'train_runtime': 428.28, 'train_samples_per_second': 6.082, 'train_steps_per_second': 0.093, 'train_loss': 0.7550976753234864, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:06<29:05,  6.15s/it]  1%|          | 2/285 [00:12<28:43,  6.09s/it]  1%|          | 3/285 [00:18<29:15,  6.22s/it]  1%|▏         | 4/285 [00:25<29:32,  6.31s/it]  2%|▏         | 5/285 [00:31<30:08,  6.46s/it]  2%|▏         | 6/285 [00:38<30:15,  6.51s/it]  2%|▏         | 7/285 [00:44<29:34,  6.38s/it]  3%|▎         | 8/285 [00:50<29:18,  6.35s/it]  3%|▎         | 9/285 [00:56<28:42,  6.24s/it]  4%|▎         | 10/285 [01:03<28:51,  6.30s/it]  4%|▍         | 11/285 [01:09<28:37,  6.27s/it]  4%|▍         | 12/285 [01:15<28:32,  6.27s/it]  5%|▍         | 13/285 [01:22<28:36,  6.31s/it]  5%|▍         | 14/285 [01:28<29:02,  6.43s/it]  5%|▌         | 15/285 [01:35<28:54,  6.43s/it]  6%|▌         | 16/285 [01:41<28:43,  6.41s/it]  6%|▌         | 17/285 [01:48<28:46,  6.44s/it]  6%|▋         | 18/285 [01:54<28:40,  6.44s/it]  7%|▋         | 19/285 [02:00<28:21,  6.40s/it]  7%|▋         | 20/285 [02:07<28:36,  6.48s/it]  7%|▋         | 21/285 [02:14<28:37,  6.51s/it]  8%|▊         | 22/285 [02:20<28:03,  6.40s/it]  8%|▊         | 23/285 [02:26<28:28,  6.52s/it]  8%|▊         | 24/285 [02:33<28:10,  6.48s/it]  9%|▉         | 25/285 [02:39<28:06,  6.49s/it]  9%|▉         | 26/285 [02:46<28:02,  6.50s/it]  9%|▉         | 27/285 [02:52<27:25,  6.38s/it] 10%|▉         | 28/285 [02:58<27:12,  6.35s/it] 10%|█         | 29/285 [03:04<26:45,  6.27s/it] 11%|█         | 30/285 [03:11<26:47,  6.30s/it] 11%|█         | 31/285 [03:17<26:43,  6.31s/it] 11%|█         | 32/285 [03:23<26:15,  6.23s/it] 12%|█▏        | 33/285 [03:29<26:02,  6.20s/it] 12%|█▏        | 34/285 [03:35<25:39,  6.14s/it] 12%|█▏        | 35/285 [03:41<25:16,  6.07s/it] 13%|█▎        | 36/285 [03:47<25:18,  6.10s/it] 13%|█▎        | 37/285 [03:54<25:20,  6.13s/it] 13%|█▎        | 38/285 [04:00<25:19,  6.15s/it] 14%|█▎        | 39/285 [04:06<25:22,  6.19s/it] 14%|█▍        | 40/285 [04:12<25:29,  6.24s/it] 14%|█▍        | 41/285 [04:19<25:31,  6.28s/it] 15%|█▍        | 42/285 [04:25<25:16,  6.24s/it] 15%|█▌        | 43/285 [04:31<25:02,  6.21s/it] 15%|█▌        | 44/285 [04:37<24:55,  6.21s/it] 16%|█▌        | 45/285 [04:43<24:49,  6.21s/it] 16%|█▌        | 46/285 [04:49<24:28,  6.14s/it] 16%|█▋        | 47/285 [04:55<24:16,  6.12s/it] 17%|█▋        | 48/285 [05:02<24:16,  6.14s/it] 17%|█▋        | 49/285 [05:08<24:26,  6.21s/it] 18%|█▊        | 50/285 [05:14<24:34,  6.28s/it] 18%|█▊        | 51/285 [05:21<24:44,  6.34s/it] 18%|█▊        | 52/285 [05:27<24:27,  6.30s/it] 19%|█▊        | 53/285 [05:33<24:17,  6.28s/it] 19%|█▉        | 54/285 [05:40<24:20,  6.32s/it] 19%|█▉        | 55/285 [05:46<24:04,  6.28s/it] 20%|█▉        | 56/285 [05:53<24:22,  6.39s/it] 20%|██        | 57/285 [05:59<24:07,  6.35s/it] 20%|██        | 58/285 [06:06<24:55,  6.59s/it] 21%|██        | 59/285 [06:12<24:14,  6.44s/it] 21%|██        | 60/285 [06:18<23:40,  6.31s/it] 21%|██▏       | 61/285 [06:24<23:17,  6.24s/it] 22%|██▏       | 62/285 [06:30<22:58,  6.18s/it] 22%|██▏       | 63/285 [06:37<23:01,  6.22s/it] 22%|██▏       | 64/285 [06:43<22:50,  6.20s/it] 23%|██▎       | 65/285 [06:49<22:44,  6.20s/it] 23%|██▎       | 66/285 [06:55<22:50,  6.26s/it] 24%|██▎       | 67/285 [07:01<22:17,  6.14s/it] 24%|██▍       | 68/285 [07:08<22:23,  6.19s/it] 24%|██▍       | 69/285 [07:14<22:38,  6.29s/it] 25%|██▍       | 70/285 [07:20<22:27,  6.27s/it] 25%|██▍       | 71/285 [07:27<22:32,  6.32s/it] 25%|██▌       | 72/285 [07:33<22:50,  6.44s/it] 26%|██▌       | 73/285 [07:40<22:56,  6.49s/it] 26%|██▌       | 74/285 [07:46<22:38,  6.44s/it] 26%|██▋       | 75/285 [07:53<22:44,  6.50s/it] 27%|██▋       | 76/285 [08:00<22:44,  6.53s/it] 27%|██▋       | 77/285 [08:07<23:11,  6.69s/it] 27%|██▋       | 78/285 [08:13<22:55,  6.64s/it] 28%|██▊       | 79/285 [08:20<22:59,  6.70s/it] 28%|██▊       | 80/285 [08:27<22:42,  6.65s/it] 28%|██▊       | 81/285 [08:33<22:18,  6.56s/it] 29%|██▉       | 82/285 [08:40<22:26,  6.63s/it] 29%|██▉       | 83/285 [08:46<22:20,  6.64s/it] 29%|██▉       | 84/285 [08:53<22:21,  6.68s/it] 30%|██▉       | 85/285 [09:00<22:25,  6.73s/it] 30%|███       | 86/285 [09:07<22:14,  6.70s/it] 31%|███       | 87/285 [09:13<21:45,  6.60s/it] 31%|███       | 88/285 [09:19<21:27,  6.53s/it] 31%|███       | 89/285 [09:26<21:17,  6.52s/it] 32%|███▏      | 90/285 [09:32<21:04,  6.49s/it] 32%|███▏      | 91/285 [09:38<20:43,  6.41s/it] 32%|███▏      | 92/285 [09:45<20:48,  6.47s/it] 33%|███▎      | 93/285 [09:51<20:37,  6.44s/it] 33%|███▎      | 94/285 [09:58<20:38,  6.48s/it] 33%|███▎      | 95/285 [10:04<20:14,  6.39s/it] 34%|███▎      | 96/285 [10:10<19:42,  6.26s/it] 34%|███▍      | 97/285 [10:16<19:31,  6.23s/it] 34%|███▍      | 98/285 [10:22<19:18,  6.19s/it] 35%|███▍      | 99/285 [10:28<18:47,  6.06s/it] 35%|███▌      | 100/285 [10:34<18:44,  6.08s/it] 35%|███▌      | 101/285 [10:40<18:39,  6.08s/it] 36%|███▌      | 102/285 [10:46<18:26,  6.05s/it] 36%|███▌      | 103/285 [10:53<18:26,  6.08s/it] 36%|███▋      | 104/285 [10:59<18:22,  6.09s/it] 37%|███▋      | 105/285 [11:04<17:56,  5.98s/it] 37%|███▋      | 106/285 [11:10<17:49,  5.98s/it] 38%|███▊      | 107/285 [11:16<17:43,  5.97s/it] 38%|███▊      | 108/285 [11:22<17:32,  5.95s/it] 38%|███▊      | 109/285 [11:28<17:45,  6.05s/it] 39%|███▊      | 110/285 [11:35<17:42,  6.07s/it] 39%|███▉      | 111/285 [11:41<17:35,  6.07s/it] 39%|███▉      | 112/285 [11:47<17:40,  6.13s/it] 40%|███▉      | 113/285 [11:53<17:42,  6.18s/it] 40%|████      | 114/285 [11:59<17:14,  6.05s/it] 40%|████      | 115/285 [12:06<17:39,  6.23s/it] 41%|████      | 116/285 [12:12<17:28,  6.20s/it] 41%|████      | 117/285 [12:18<17:06,  6.11s/it] 41%|████▏     | 118/285 [12:23<16:46,  6.03s/it] 42%|████▏     | 119/285 [12:29<16:19,  5.90s/it] 42%|████▏     | 120/285 [12:35<16:18,  5.93s/it] 42%|████▏     | 121/285 [12:41<16:16,  5.96s/it] 43%|████▎     | 122/285 [12:47<16:13,  5.97s/it] 43%|████▎     | 123/285 [12:53<16:17,  6.03s/it] 44%|████▎     | 124/285 [12:59<16:08,  6.02s/it] 44%|████▍     | 125/285 [13:05<16:05,  6.03s/it] 44%|████▍     | 126/285 [13:11<15:52,  5.99s/it] 45%|████▍     | 127/285 [13:17<15:44,  5.98s/it] 45%|████▍     | 128/285 [13:23<15:22,  5.88s/it] 45%|████▌     | 129/285 [13:29<15:22,  5.91s/it] 46%|████▌     | 130/285 [13:35<15:20,  5.94s/it] 46%|████▌     | 131/285 [13:41<15:19,  5.97s/it] 46%|████▋     | 132/285 [13:47<15:13,  5.97s/it] 47%|████▋     | 133/285 [13:53<15:07,  5.97s/it] 47%|████▋     | 134/285 [13:59<15:16,  6.07s/it] 47%|████▋     | 135/285 [14:05<15:06,  6.05s/it] 48%|████▊     | 136/285 [14:11<14:38,  5.90s/it] 48%|████▊     | 137/285 [14:17<14:41,  5.96s/it] 48%|████▊     | 138/285 [14:23<14:44,  6.01s/it] 49%|████▉     | 139/285 [14:29<14:51,  6.11s/it] 49%|████▉     | 140/285 [14:35<14:51,  6.15s/it] 49%|████▉     | 141/285 [14:42<14:45,  6.15s/it] 50%|████▉     | 142/285 [14:47<14:24,  6.05s/it] 50%|█████     | 143/285 [14:53<14:06,  5.96s/it] 51%|█████     | 144/285 [14:59<13:54,  5.92s/it] 51%|█████     | 145/285 [15:05<14:04,  6.03s/it] 51%|█████     | 146/285 [15:11<13:47,  5.95s/it] 52%|█████▏    | 147/285 [15:17<13:33,  5.90s/it] 52%|█████▏    | 148/285 [15:23<13:40,  5.99s/it] 52%|█████▏    | 149/285 [15:29<13:45,  6.07s/it] 53%|█████▎    | 150/285 [15:36<13:46,  6.12s/it] 53%|█████▎    | 151/285 [15:42<13:42,  6.14s/it] 53%|█████▎    | 152/285 [15:48<13:29,  6.09s/it] 54%|█████▎    | 153/285 [15:54<13:29,  6.13s/it] 54%|█████▍    | 154/285 [16:00<13:25,  6.15s/it] 54%|█████▍    | 155/285 [16:06<13:11,  6.09s/it] 55%|█████▍    | 156/285 [16:12<13:18,  6.19s/it] 55%|█████▌    | 157/285 [16:18<13:04,  6.13s/it] 55%|█████▌    | 158/285 [16:24<12:47,  6.05s/it] 56%|█████▌    | 159/285 [16:30<12:38,  6.02s/it] 56%|█████▌    | 160/285 [16:36<12:17,  5.90s/it] 56%|█████▋    | 161/285 [16:42<12:22,  5.99s/it] 57%|█████▋    | 162/285 [16:48<12:21,  6.03s/it] 57%|█████▋    | 163/285 [16:55<12:27,  6.12s/it] 58%|█████▊    | 164/285 [17:01<12:27,  6.18s/it] 58%|█████▊    | 165/285 [17:07<12:32,  6.27s/it] 58%|█████▊    | 166/285 [17:14<12:23,  6.24s/it] 59%|█████▊    | 167/285 [17:20<12:10,  6.19s/it] 59%|█████▉    | 168/285 [17:26<11:55,  6.11s/it] 59%|█████▉    | 169/285 [17:31<11:43,  6.06s/it] 60%|█████▉    | 170/285 [17:37<11:30,  6.01s/it] 60%|██████    | 171/285 [17:43<11:24,  6.01s/it] 60%|██████    | 172/285 [17:49<11:21,  6.03s/it] 61%|██████    | 173/285 [17:55<11:04,  5.94s/it] 61%|██████    | 174/285 [18:01<10:49,  5.85s/it] 61%|██████▏   | 175/285 [18:07<10:54,  5.95s/it] 62%|██████▏   | 176/285 [18:14<11:10,  6.15s/it] 62%|██████▏   | 177/285 [18:20<11:12,  6.22s/it] 62%|██████▏   | 178/285 [18:27<11:26,  6.41s/it] 63%|██████▎   | 179/285 [18:33<11:20,  6.42s/it] 63%|██████▎   | 180/285 [18:39<11:06,  6.35s/it] 64%|██████▎   | 181/285 [18:46<11:04,  6.39s/it] 64%|██████▍   | 182/285 [18:52<10:55,  6.36s/it] 64%|██████▍   | 183/285 [18:58<10:39,  6.27s/it] 65%|██████▍   | 184/285 [19:05<10:35,  6.29s/it] 65%|██████▍   | 185/285 [19:10<10:10,  6.11s/it] 65%|██████▌   | 186/285 [19:16<10:02,  6.08s/it] 66%|██████▌   | 187/285 [19:22<09:41,  5.94s/it] 66%|██████▌   | 188/285 [19:28<09:33,  5.91s/it] 66%|██████▋   | 189/285 [19:34<09:21,  5.85s/it] 67%|██████▋   | 190/285 [19:39<09:14,  5.83s/it] 67%|██████▋   | 191/285 [19:45<09:12,  5.88s/it] 67%|██████▋   | 192/285 [19:52<09:18,  6.00s/it] 68%|██████▊   | 193/285 [19:58<09:19,  6.09s/it] 68%|██████▊   | 194/285 [20:04<09:22,  6.18s/it] 68%|██████▊   | 195/285 [20:11<09:31,  6.35s/it] 69%|██████▉   | 196/285 [20:17<09:09,  6.17s/it] 69%|██████▉   | 197/285 [20:22<08:46,  5.98s/it] 69%|██████▉   | 198/285 [20:28<08:33,  5.90s/it] 70%|██████▉   | 199/285 [20:34<08:25,  5.88s/it] 70%|███████   | 200/285 [20:40<08:28,  5.98s/it]                                                  70%|███████   | 200/285 [20:40<08:28,  5.98s/it] 71%|███████   | 201/285 [20:46<08:17,  5.92s/it] 71%|███████   | 202/285 [20:52<08:10,  5.91s/it] 71%|███████   | 203/285 [20:57<07:57,  5.82s/it] 72%|███████▏  | 204/285 [21:04<07:59,  5.92s/it] 72%|███████▏  | 205/285 [21:09<07:49,  5.87s/it] 72%|███████▏  | 206/285 [21:15<07:46,  5.90s/it] 73%|███████▎  | 207/285 [21:22<07:53,  6.07s/it] 73%|███████▎  | 208/285 [21:28<07:51,  6.12s/it] 73%|███████▎  | 209/285 [21:35<07:55,  6.26s/it] 74%|███████▎  | 210/285 [21:41<07:45,  6.21s/it] 74%|███████▍  | 211/285 [21:47<07:36,  6.18s/it] 74%|███████▍  | 212/285 [21:53<07:32,  6.20s/it] 75%|███████▍  | 213/285 [21:59<07:21,  6.14s/it] 75%|███████▌  | 214/285 [22:05<07:05,  6.00s/it] 75%|███████▌  | 215/285 [22:10<06:52,  5.90s/it] 76%|███████▌  | 216/285 [22:17<06:54,  6.00s/it] 76%|███████▌  | 217/285 [22:22<06:43,  5.94s/it] 76%|███████▋  | 218/285 [22:28<06:40,  5.97s/it] 77%|███████▋  | 219/285 [22:34<06:29,  5.90s/it] 77%|███████▋  | 220/285 [22:40<06:23,  5.89s/it] 78%|███████▊  | 221/285 [22:46<06:13,  5.84s/it] 78%|███████▊  | 222/285 [22:52<06:07,  5.83s/it] 78%|███████▊  | 223/285 [22:57<06:02,  5.85s/it] 79%|███████▊  | 224/285 [23:03<05:57,  5.86s/it] 79%|███████▉  | 225/285 [23:09<05:49,  5.82s/it] 79%|███████▉  | 226/285 [23:15<05:45,  5.85s/it] 80%|███████▉  | 227/285 [23:21<05:39,  5.85s/it] 80%|████████  | 228/285 [23:27<05:37,  5.92s/it] 80%|████████  | 229/285 [23:33<05:43,  6.13s/it] 81%|████████  | 230/285 [23:39<05:33,  6.07s/it] 81%|████████  | 231/285 [23:45<05:23,  5.98s/it] 81%|████████▏ | 232/285 [23:51<05:18,  6.00s/it] 82%|████████▏ | 233/285 [23:57<05:08,  5.93s/it] 82%|████████▏ | 234/285 [24:03<05:02,  5.93s/it] 82%|████████▏ | 235/285 [24:09<04:54,  5.90s/it] 83%|████████▎ | 236/285 [24:14<04:43,  5.78s/it] 83%|████████▎ | 237/285 [24:20<04:36,  5.77s/it] 84%|████████▎ | 238/285 [24:26<04:31,  5.77s/it] 84%|████████▍ | 239/285 [24:32<04:30,  5.88s/it] 84%|████████▍ | 240/285 [24:38<04:28,  5.96s/it] 85%|████████▍ | 241/285 [24:44<04:19,  5.89s/it] 85%|████████▍ | 242/285 [24:50<04:13,  5.89s/it] 85%|████████▌ | 243/285 [24:56<04:08,  5.93s/it] 86%|████████▌ | 244/285 [25:01<04:00,  5.87s/it] 86%|████████▌ | 245/285 [25:07<03:55,  5.88s/it] 86%|████████▋ | 246/285 [25:13<03:46,  5.80s/it] 87%|████████▋ | 247/285 [25:19<03:46,  5.95s/it] 87%|████████▋ | 248/285 [25:25<03:38,  5.90s/it] 87%|████████▋ | 249/285 [25:31<03:29,  5.83s/it] 88%|████████▊ | 250/285 [25:37<03:29,  5.98s/it] 88%|████████▊ | 251/285 [25:43<03:22,  5.96s/it] 88%|████████▊ | 252/285 [25:49<03:17,  5.97s/it] 89%|████████▉ | 253/285 [25:55<03:10,  5.95s/it] 89%|████████▉ | 254/285 [26:01<03:04,  5.97s/it] 89%|████████▉ | 255/285 [26:07<03:00,  6.02s/it] 90%|████████▉ | 256/285 [26:13<02:52,  5.96s/it] 90%|█████████ | 257/285 [26:19<02:50,  6.10s/it] 91%|█████████ | 258/285 [26:25<02:43,  6.06s/it] 91%|█████████ | 259/285 [26:31<02:36,  6.03s/it] 91%|█████████ | 260/285 [26:37<02:29,  5.98s/it] 92%|█████████▏| 261/285 [26:43<02:22,  5.95s/it] 92%|█████████▏| 262/285 [26:49<02:17,  6.00s/it] 92%|█████████▏| 263/285 [26:55<02:12,  6.03s/it] 93%|█████████▎| 264/285 [27:01<02:06,  6.02s/it] 93%|█████████▎| 265/285 [27:07<02:00,  6.01s/it] 93%|█████████▎| 266/285 [27:13<01:52,  5.94s/it] 94%|█████████▎| 267/285 [27:19<01:49,  6.10s/it] 94%|█████████▍| 268/285 [27:26<01:44,  6.15s/it] 94%|█████████▍| 269/285 [27:32<01:37,  6.11s/it] 95%|█████████▍| 270/285 [27:38<01:32,  6.20s/it] 95%|█████████▌| 271/285 [27:44<01:26,  6.21s/it] 95%|█████████▌| 272/285 [27:50<01:20,  6.17s/it] 96%|█████████▌| 273/285 [27:57<01:14,  6.20s/it] 96%|█████████▌| 274/285 [28:03<01:09,  6.27s/it] 96%|█████████▋| 275/285 [28:09<01:03,  6.31s/it] 97%|█████████▋| 276/285 [28:16<00:56,  6.30s/it] 97%|█████████▋| 277/285 [28:22<00:50,  6.29s/it] 98%|█████████▊| 278/285 [28:28<00:43,  6.19s/it] 98%|█████████▊| 279/285 [28:34<00:36,  6.13s/it] 98%|█████████▊| 280/285 [28:40<00:30,  6.20s/it] 99%|█████████▊| 281/285 [28:46<00:24,  6.20s/it] 99%|█████████▉| 282/285 [28:53<00:18,  6.30s/it] 99%|█████████▉| 283/285 [29:00<00:12,  6.44s/it]100%|█████████▉| 284/285 [29:06<00:06,  6.34s/it]100%|██████████| 285/285 [29:12<00:00,  6.35s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [29:12<00:00,  6.35s/it]100%|██████████| 285/285 [29:12<00:00,  6.15s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 419933186
{'loss': 0.3517, 'learning_rate': 9.800000000000001e-06, 'epoch': 3.51}
{'train_runtime': 1752.7647, 'train_samples_per_second': 10.424, 'train_steps_per_second': 0.163, 'train_loss': 0.2808660908749229, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:06<30:25,  6.43s/it]  1%|          | 2/285 [00:12<29:16,  6.21s/it]  1%|          | 3/285 [00:18<29:14,  6.22s/it]  1%|▏         | 4/285 [00:24<29:02,  6.20s/it]  2%|▏         | 5/285 [00:31<29:01,  6.22s/it]  2%|▏         | 6/285 [00:37<28:42,  6.17s/it]  2%|▏         | 7/285 [00:43<28:00,  6.05s/it]  3%|▎         | 8/285 [00:49<27:52,  6.04s/it]  3%|▎         | 9/285 [00:54<27:14,  5.92s/it]  4%|▎         | 10/285 [01:00<27:19,  5.96s/it]  4%|▍         | 11/285 [01:06<26:54,  5.89s/it]  4%|▍         | 12/285 [01:12<26:44,  5.88s/it]  5%|▍         | 13/285 [01:18<27:17,  6.02s/it]  5%|▍         | 14/285 [01:25<27:40,  6.13s/it]  5%|▌         | 15/285 [01:31<27:35,  6.13s/it]  6%|▌         | 16/285 [01:37<27:20,  6.10s/it]  6%|▌         | 17/285 [01:43<27:29,  6.15s/it]  6%|▋         | 18/285 [01:49<27:36,  6.20s/it]  7%|▋         | 19/285 [01:55<27:22,  6.18s/it]  7%|▋         | 20/285 [02:01<27:03,  6.12s/it]  7%|▋         | 21/285 [02:08<27:04,  6.15s/it]  8%|▊         | 22/285 [02:14<26:35,  6.07s/it]  8%|▊         | 23/285 [02:20<26:31,  6.07s/it]  8%|▊         | 24/285 [02:26<26:23,  6.07s/it]  9%|▉         | 25/285 [02:32<26:12,  6.05s/it]  9%|▉         | 26/285 [02:37<25:43,  5.96s/it]  9%|▉         | 27/285 [02:43<25:20,  5.90s/it] 10%|▉         | 28/285 [02:49<25:26,  5.94s/it] 10%|█         | 29/285 [02:55<25:06,  5.88s/it] 11%|█         | 30/285 [03:01<25:13,  5.94s/it] 11%|█         | 31/285 [03:07<25:13,  5.96s/it] 11%|█         | 32/285 [03:13<24:54,  5.91s/it] 12%|█▏        | 33/285 [03:19<24:57,  5.94s/it] 12%|█▏        | 34/285 [03:25<25:19,  6.05s/it] 12%|█▏        | 35/285 [03:31<24:52,  5.97s/it] 13%|█▎        | 36/285 [03:37<24:36,  5.93s/it] 13%|█▎        | 37/285 [03:43<24:30,  5.93s/it] 13%|█▎        | 38/285 [03:49<24:23,  5.92s/it] 14%|█▎        | 39/285 [03:55<24:26,  5.96s/it] 14%|█▍        | 40/285 [04:00<24:06,  5.90s/it] 14%|█▍        | 41/285 [04:06<23:55,  5.88s/it] 15%|█▍        | 42/285 [04:12<23:36,  5.83s/it] 15%|█▌        | 43/285 [04:18<23:19,  5.78s/it] 15%|█▌        | 44/285 [04:24<23:39,  5.89s/it] 16%|█▌        | 45/285 [04:30<23:48,  5.95s/it] 16%|█▌        | 46/285 [04:35<23:12,  5.83s/it] 16%|█▋        | 47/285 [04:41<22:51,  5.76s/it] 17%|█▋        | 48/285 [04:47<22:55,  5.80s/it] 17%|█▋        | 49/285 [04:53<23:10,  5.89s/it] 18%|█▊        | 50/285 [04:59<23:40,  6.05s/it] 18%|█▊        | 51/285 [05:06<24:12,  6.21s/it] 18%|█▊        | 52/285 [05:12<24:08,  6.22s/it] 19%|█▊        | 53/285 [05:18<24:04,  6.22s/it] 19%|█▉        | 54/285 [05:25<24:14,  6.29s/it] 19%|█▉        | 55/285 [05:31<24:11,  6.31s/it] 20%|█▉        | 56/285 [05:38<24:31,  6.43s/it] 20%|██        | 57/285 [05:44<24:09,  6.36s/it] 20%|██        | 58/285 [05:51<24:49,  6.56s/it] 21%|██        | 59/285 [05:57<24:04,  6.39s/it] 21%|██        | 60/285 [06:03<23:30,  6.27s/it] 21%|██▏       | 61/285 [06:09<22:54,  6.14s/it] 22%|██▏       | 62/285 [06:15<22:29,  6.05s/it] 22%|██▏       | 63/285 [06:21<22:27,  6.07s/it] 22%|██▏       | 64/285 [06:27<22:19,  6.06s/it] 23%|██▎       | 65/285 [06:33<22:28,  6.13s/it] 23%|██▎       | 66/285 [06:40<22:44,  6.23s/it] 24%|██▎       | 67/285 [06:46<22:12,  6.11s/it] 24%|██▍       | 68/285 [06:52<22:11,  6.14s/it] 24%|██▍       | 69/285 [06:58<22:11,  6.17s/it] 25%|██▍       | 70/285 [07:04<21:51,  6.10s/it] 25%|██▍       | 71/285 [07:10<21:40,  6.08s/it] 25%|██▌       | 72/285 [07:16<21:28,  6.05s/it] 26%|██▌       | 73/285 [07:22<21:04,  5.96s/it] 26%|██▌       | 74/285 [07:27<20:31,  5.84s/it] 26%|██▋       | 75/285 [07:33<20:31,  5.86s/it] 27%|██▋       | 76/285 [07:40<20:56,  6.01s/it] 27%|██▋       | 77/285 [07:46<21:14,  6.13s/it] 27%|██▋       | 78/285 [07:52<20:45,  6.02s/it] 28%|██▊       | 79/285 [07:58<20:44,  6.04s/it] 28%|██▊       | 80/285 [08:03<20:14,  5.93s/it] 28%|██▊       | 81/285 [08:09<19:56,  5.87s/it] 29%|██▉       | 82/285 [08:15<20:11,  5.97s/it] 29%|██▉       | 83/285 [08:21<20:03,  5.96s/it] 29%|██▉       | 84/285 [08:27<19:53,  5.94s/it] 30%|██▉       | 85/285 [08:34<20:18,  6.09s/it] 30%|███       | 86/285 [08:40<20:06,  6.06s/it] 31%|███       | 87/285 [08:46<19:45,  5.99s/it] 31%|███       | 88/285 [08:51<19:28,  5.93s/it] 31%|███       | 89/285 [08:57<19:23,  5.94s/it] 32%|███▏      | 90/285 [09:03<19:16,  5.93s/it] 32%|███▏      | 91/285 [09:09<19:02,  5.89s/it] 32%|███▏      | 92/285 [09:15<19:09,  5.95s/it] 33%|███▎      | 93/285 [09:21<19:01,  5.95s/it] 33%|███▎      | 94/285 [09:27<19:06,  6.00s/it] 33%|███▎      | 95/285 [09:33<18:58,  5.99s/it] 34%|███▎      | 96/285 [09:39<18:44,  5.95s/it] 34%|███▍      | 97/285 [09:45<18:34,  5.93s/it] 34%|███▍      | 98/285 [09:51<18:32,  5.95s/it] 35%|███▍      | 99/285 [09:57<18:26,  5.95s/it] 35%|███▌      | 100/285 [10:03<18:43,  6.07s/it] 35%|███▌      | 101/285 [10:10<19:18,  6.29s/it] 36%|███▌      | 102/285 [10:17<19:47,  6.49s/it] 36%|███▌      | 103/285 [10:23<19:35,  6.46s/it] 36%|███▋      | 104/285 [10:30<19:23,  6.43s/it] 37%|███▋      | 105/285 [10:36<19:10,  6.39s/it] 37%|███▋      | 106/285 [10:42<19:00,  6.37s/it] 38%|███▊      | 107/285 [10:49<18:58,  6.40s/it] 38%|███▊      | 108/285 [10:55<18:40,  6.33s/it] 38%|███▊      | 109/285 [11:01<18:41,  6.37s/it] 39%|███▊      | 110/285 [11:08<18:25,  6.32s/it] 39%|███▉      | 111/285 [11:14<18:15,  6.30s/it] 39%|███▉      | 112/285 [11:20<18:04,  6.27s/it] 40%|███▉      | 113/285 [11:26<17:56,  6.26s/it] 40%|████      | 114/285 [11:32<17:19,  6.08s/it] 40%|████      | 115/285 [11:39<17:41,  6.24s/it] 41%|████      | 116/285 [11:45<17:26,  6.19s/it] 41%|████      | 117/285 [11:50<17:04,  6.10s/it] 41%|████▏     | 118/285 [11:56<16:49,  6.05s/it] 42%|████▏     | 119/285 [12:02<16:24,  5.93s/it] 42%|████▏     | 120/285 [12:08<16:20,  5.94s/it] 42%|████▏     | 121/285 [12:14<16:21,  5.98s/it] 43%|████▎     | 122/285 [12:20<16:17,  6.00s/it] 43%|████▎     | 123/285 [12:26<16:20,  6.05s/it] 44%|████▎     | 124/285 [12:32<16:12,  6.04s/it] 44%|████▍     | 125/285 [12:38<16:03,  6.02s/it] 44%|████▍     | 126/285 [12:44<15:47,  5.96s/it] 45%|████▍     | 127/285 [12:50<15:33,  5.91s/it] 45%|████▍     | 128/285 [12:55<15:10,  5.80s/it] 45%|████▌     | 129/285 [13:01<15:09,  5.83s/it] 46%|████▌     | 130/285 [13:07<15:08,  5.86s/it] 46%|████▌     | 131/285 [13:13<15:09,  5.91s/it] 46%|████▋     | 132/285 [13:19<15:02,  5.90s/it] 47%|████▋     | 133/285 [13:25<14:52,  5.87s/it] 47%|████▋     | 134/285 [13:31<14:59,  5.96s/it] 47%|████▋     | 135/285 [13:37<14:50,  5.93s/it] 48%|████▊     | 136/285 [13:43<14:25,  5.81s/it] 48%|████▊     | 137/285 [13:48<14:20,  5.81s/it] 48%|████▊     | 138/285 [13:54<14:10,  5.79s/it] 49%|████▉     | 139/285 [14:00<14:10,  5.83s/it] 49%|████▉     | 140/285 [14:06<14:27,  5.98s/it] 49%|████▉     | 141/285 [14:13<14:35,  6.08s/it] 50%|████▉     | 142/285 [14:19<14:24,  6.04s/it] 50%|█████     | 143/285 [14:24<14:01,  5.93s/it] 51%|█████     | 144/285 [14:30<13:45,  5.85s/it] 51%|█████     | 145/285 [14:36<13:52,  5.94s/it] 51%|█████     | 146/285 [14:42<13:36,  5.87s/it] 52%|█████▏    | 147/285 [14:48<13:22,  5.82s/it] 52%|█████▏    | 148/285 [14:54<13:25,  5.88s/it] 52%|█████▏    | 149/285 [15:00<13:30,  5.96s/it] 53%|█████▎    | 150/285 [15:06<13:27,  5.98s/it] 53%|█████▎    | 151/285 [15:12<13:27,  6.02s/it] 53%|█████▎    | 152/285 [15:18<13:15,  5.98s/it] 54%|█████▎    | 153/285 [15:24<13:28,  6.12s/it] 54%|█████▍    | 154/285 [15:30<13:18,  6.09s/it] 54%|█████▍    | 155/285 [15:36<13:10,  6.08s/it] 55%|█████▍    | 156/285 [15:43<13:18,  6.19s/it] 55%|█████▌    | 157/285 [15:49<13:06,  6.14s/it] 55%|█████▌    | 158/285 [15:55<13:01,  6.15s/it] 56%|█████▌    | 159/285 [16:01<12:57,  6.17s/it] 56%|█████▌    | 160/285 [16:07<12:47,  6.14s/it] 56%|█████▋    | 161/285 [16:14<12:57,  6.27s/it] 57%|█████▋    | 162/285 [16:20<12:57,  6.32s/it] 57%|█████▋    | 163/285 [16:27<13:03,  6.42s/it] 58%|█████▊    | 164/285 [16:33<13:02,  6.46s/it] 58%|█████▊    | 165/285 [16:40<13:08,  6.57s/it] 58%|█████▊    | 166/285 [16:46<12:41,  6.40s/it] 59%|█████▊    | 167/285 [16:52<12:26,  6.32s/it] 59%|█████▉    | 168/285 [16:59<12:11,  6.25s/it] 59%|█████▉    | 169/285 [17:05<12:03,  6.24s/it] 60%|█████▉    | 170/285 [17:11<11:54,  6.21s/it] 60%|██████    | 171/285 [17:17<11:41,  6.15s/it] 60%|██████    | 172/285 [17:23<11:42,  6.22s/it] 61%|██████    | 173/285 [17:29<11:29,  6.16s/it] 61%|██████    | 174/285 [17:35<11:18,  6.11s/it] 61%|██████▏   | 175/285 [17:41<11:11,  6.10s/it] 62%|██████▏   | 176/285 [17:48<11:11,  6.16s/it] 62%|██████▏   | 177/285 [17:54<11:10,  6.21s/it] 62%|██████▏   | 178/285 [18:00<11:07,  6.24s/it] 63%|██████▎   | 179/285 [18:07<11:01,  6.24s/it] 63%|██████▎   | 180/285 [18:13<10:56,  6.25s/it] 64%|██████▎   | 181/285 [18:19<10:57,  6.32s/it] 64%|██████▍   | 182/285 [18:26<10:53,  6.35s/it] 64%|██████▍   | 183/285 [18:32<10:43,  6.31s/it] 65%|██████▍   | 184/285 [18:38<10:41,  6.35s/it] 65%|██████▍   | 185/285 [18:44<10:27,  6.28s/it] 65%|██████▌   | 186/285 [18:51<10:28,  6.35s/it] 66%|██████▌   | 187/285 [18:57<10:19,  6.32s/it] 66%|██████▌   | 188/285 [19:04<10:11,  6.31s/it] 66%|██████▋   | 189/285 [19:10<10:03,  6.29s/it] 67%|██████▋   | 190/285 [19:16<09:55,  6.27s/it] 67%|██████▋   | 191/285 [19:22<09:53,  6.31s/it] 67%|██████▋   | 192/285 [19:29<09:56,  6.42s/it] 68%|██████▊   | 193/285 [19:36<09:53,  6.45s/it] 68%|██████▊   | 194/285 [19:42<09:47,  6.45s/it] 68%|██████▊   | 195/285 [19:49<09:46,  6.51s/it] 69%|██████▉   | 196/285 [19:55<09:28,  6.39s/it] 69%|██████▉   | 197/285 [20:01<09:12,  6.28s/it] 69%|██████▉   | 198/285 [20:07<09:00,  6.21s/it] 70%|██████▉   | 199/285 [20:13<08:52,  6.19s/it] 70%|███████   | 200/285 [20:19<08:45,  6.18s/it]                                                  70%|███████   | 200/285 [20:19<08:45,  6.18s/it] 71%|███████   | 201/285 [20:25<08:40,  6.19s/it] 71%|███████   | 202/285 [20:32<08:37,  6.24s/it] 71%|███████   | 203/285 [20:38<08:27,  6.18s/it] 72%|███████▏  | 204/285 [20:45<08:34,  6.35s/it] 72%|███████▏  | 205/285 [20:51<08:36,  6.45s/it] 72%|███████▏  | 206/285 [20:58<08:41,  6.60s/it] 73%|███████▎  | 207/285 [21:05<08:50,  6.81s/it] 73%|███████▎  | 208/285 [21:12<08:43,  6.79s/it] 73%|███████▎  | 209/285 [21:19<08:39,  6.84s/it] 74%|███████▎  | 210/285 [21:26<08:29,  6.79s/it] 74%|███████▍  | 211/285 [21:33<08:21,  6.78s/it] 74%|███████▍  | 212/285 [21:39<08:11,  6.73s/it] 75%|███████▍  | 213/285 [21:45<07:51,  6.56s/it] 75%|███████▌  | 214/285 [21:51<07:36,  6.42s/it] 75%|███████▌  | 215/285 [21:58<07:24,  6.35s/it] 76%|███████▌  | 216/285 [22:04<07:21,  6.41s/it] 76%|███████▌  | 217/285 [22:10<07:06,  6.27s/it] 76%|███████▋  | 218/285 [22:16<07:00,  6.27s/it] 77%|███████▋  | 219/285 [22:23<06:52,  6.24s/it] 77%|███████▋  | 220/285 [22:29<06:46,  6.25s/it] 78%|███████▊  | 221/285 [22:35<06:37,  6.21s/it] 78%|███████▊  | 222/285 [22:41<06:27,  6.16s/it] 78%|███████▊  | 223/285 [22:47<06:25,  6.21s/it] 79%|███████▊  | 224/285 [22:54<06:19,  6.23s/it] 79%|███████▉  | 225/285 [23:00<06:14,  6.23s/it] 79%|███████▉  | 226/285 [23:06<06:06,  6.22s/it] 80%|███████▉  | 227/285 [23:12<05:59,  6.20s/it] 80%|████████  | 228/285 [23:19<05:58,  6.30s/it] 80%|████████  | 229/285 [23:26<06:00,  6.44s/it] 81%|████████  | 230/285 [23:32<05:46,  6.31s/it] 81%|████████  | 231/285 [23:37<05:32,  6.16s/it] 81%|████████▏ | 232/285 [23:43<05:25,  6.14s/it] 82%|████████▏ | 233/285 [23:49<05:17,  6.10s/it] 82%|████████▏ | 234/285 [23:56<05:10,  6.10s/it] 82%|████████▏ | 235/285 [24:01<05:01,  6.02s/it] 83%|████████▎ | 236/285 [24:07<04:48,  5.88s/it] 83%|████████▎ | 237/285 [24:13<04:39,  5.83s/it] 84%|████████▎ | 238/285 [24:18<04:32,  5.79s/it] 84%|████████▍ | 239/285 [24:24<04:30,  5.89s/it] 84%|████████▍ | 240/285 [24:31<04:28,  5.97s/it] 85%|████████▍ | 241/285 [24:36<04:19,  5.91s/it] 85%|████████▍ | 242/285 [24:42<04:13,  5.89s/it] 85%|████████▌ | 243/285 [24:48<04:09,  5.94s/it] 86%|████████▌ | 244/285 [24:54<04:00,  5.87s/it] 86%|████████▌ | 245/285 [25:00<03:54,  5.87s/it] 86%|████████▋ | 246/285 [25:05<03:46,  5.80s/it] 87%|████████▋ | 247/285 [25:12<03:45,  5.95s/it] 87%|████████▋ | 248/285 [25:18<03:39,  5.94s/it] 87%|████████▋ | 249/285 [25:23<03:32,  5.90s/it] 88%|████████▊ | 250/285 [25:30<03:30,  6.02s/it] 88%|████████▊ | 251/285 [25:36<03:24,  6.00s/it] 88%|████████▊ | 252/285 [25:42<03:18,  6.00s/it] 89%|████████▉ | 253/285 [25:48<03:10,  5.94s/it] 89%|████████▉ | 254/285 [25:54<03:04,  5.95s/it] 89%|████████▉ | 255/285 [26:00<02:59,  5.97s/it] 90%|████████▉ | 256/285 [26:05<02:50,  5.89s/it] 90%|█████████ | 257/285 [26:11<02:47,  6.00s/it] 91%|█████████ | 258/285 [26:17<02:40,  5.96s/it] 91%|█████████ | 259/285 [26:23<02:33,  5.91s/it] 91%|█████████ | 260/285 [26:29<02:30,  6.04s/it] 92%|█████████▏| 261/285 [26:35<02:23,  6.00s/it] 92%|█████████▏| 262/285 [26:41<02:18,  6.01s/it] 92%|█████████▏| 263/285 [26:48<02:14,  6.11s/it] 93%|█████████▎| 264/285 [26:54<02:08,  6.11s/it] 93%|█████████▎| 265/285 [27:00<02:01,  6.07s/it] 93%|█████████▎| 266/285 [27:05<01:52,  5.92s/it] 94%|█████████▎| 267/285 [27:11<01:46,  5.94s/it] 94%|█████████▍| 268/285 [27:17<01:40,  5.92s/it] 94%|█████████▍| 269/285 [27:23<01:35,  5.96s/it] 95%|█████████▍| 270/285 [27:30<01:30,  6.02s/it] 95%|█████████▌| 271/285 [27:36<01:24,  6.02s/it] 95%|█████████▌| 272/285 [27:41<01:17,  5.97s/it] 96%|█████████▌| 273/285 [27:47<01:10,  5.91s/it] 96%|█████████▌| 274/285 [27:53<01:05,  5.98s/it] 96%|█████████▋| 275/285 [28:00<01:00,  6.07s/it] 97%|█████████▋| 276/285 [28:05<00:54,  6.01s/it] 97%|█████████▋| 277/285 [28:11<00:46,  5.87s/it] 98%|█████████▊| 278/285 [28:17<00:40,  5.85s/it] 98%|█████████▊| 279/285 [28:23<00:35,  5.94s/it] 98%|█████████▊| 280/285 [28:29<00:30,  6.02s/it] 99%|█████████▊| 281/285 [28:35<00:23,  5.94s/it] 99%|█████████▉| 282/285 [28:41<00:17,  5.89s/it] 99%|█████████▉| 283/285 [28:47<00:11,  5.96s/it]100%|█████████▉| 284/285 [28:53<00:06,  6.07s/it]100%|██████████| 285/285 [28:59<00:00,  6.07s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [28:59<00:00,  6.07s/it]100%|██████████| 285/285 [28:59<00:00,  6.10s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652359063
{'loss': 0.474, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 1739.6865, 'train_samples_per_second': 10.502, 'train_steps_per_second': 0.164, 'train_loss': 0.3631122957196152, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:07<37:18,  7.88s/it]  1%|          | 2/285 [00:15<36:46,  7.80s/it]  1%|          | 3/285 [00:23<37:38,  8.01s/it]  1%|▏         | 4/285 [00:32<37:57,  8.10s/it]  2%|▏         | 5/285 [00:40<38:33,  8.26s/it]  2%|▏         | 6/285 [00:49<38:38,  8.31s/it]  2%|▏         | 7/285 [00:57<38:06,  8.23s/it]  3%|▎         | 8/285 [01:05<38:16,  8.29s/it]  3%|▎         | 9/285 [01:13<37:37,  8.18s/it]  4%|▎         | 10/285 [01:22<38:04,  8.31s/it]  4%|▍         | 11/285 [01:30<37:49,  8.28s/it]  4%|▍         | 12/285 [01:38<37:57,  8.34s/it]  5%|▍         | 13/285 [01:47<38:08,  8.41s/it]  5%|▍         | 14/285 [01:56<38:59,  8.63s/it]  5%|▌         | 15/285 [02:05<38:58,  8.66s/it]  6%|▌         | 16/285 [02:13<38:28,  8.58s/it]  6%|▌         | 17/285 [02:22<38:21,  8.59s/it]  6%|▋         | 18/285 [02:30<38:25,  8.63s/it]  7%|▋         | 19/285 [02:39<38:15,  8.63s/it]  7%|▋         | 20/285 [02:48<38:18,  8.67s/it]  7%|▋         | 21/285 [02:57<38:35,  8.77s/it]  8%|▊         | 22/285 [03:05<38:03,  8.68s/it]  8%|▊         | 23/285 [03:14<38:15,  8.76s/it]  8%|▊         | 24/285 [03:23<38:25,  8.83s/it]  9%|▉         | 25/285 [03:32<38:14,  8.83s/it]  9%|▉         | 26/285 [03:40<37:07,  8.60s/it]  9%|▉         | 27/285 [03:48<36:23,  8.46s/it] 10%|▉         | 28/285 [03:57<36:45,  8.58s/it] 10%|█         | 29/285 [04:06<36:22,  8.53s/it] 11%|█         | 30/285 [04:14<36:31,  8.59s/it] 11%|█         | 31/285 [04:23<36:32,  8.63s/it] 11%|█         | 32/285 [04:32<36:10,  8.58s/it] 12%|█▏        | 33/285 [04:40<35:47,  8.52s/it] 12%|█▏        | 34/285 [04:49<35:48,  8.56s/it] 12%|█▏        | 35/285 [04:57<34:57,  8.39s/it] 13%|█▎        | 36/285 [05:05<34:38,  8.35s/it] 13%|█▎        | 37/285 [05:13<34:38,  8.38s/it] 13%|█▎        | 38/285 [05:21<34:19,  8.34s/it] 14%|█▎        | 39/285 [05:30<34:13,  8.35s/it] 14%|█▍        | 40/285 [05:38<33:55,  8.31s/it] 14%|█▍        | 41/285 [05:46<33:41,  8.28s/it] 15%|█▍        | 42/285 [05:54<33:15,  8.21s/it] 15%|█▌        | 43/285 [06:02<32:44,  8.12s/it] 15%|█▌        | 44/285 [06:11<32:51,  8.18s/it] 16%|█▌        | 45/285 [06:19<33:32,  8.39s/it] 16%|█▌        | 46/285 [06:28<33:15,  8.35s/it] 16%|█▋        | 47/285 [06:36<32:49,  8.28s/it] 17%|█▋        | 48/285 [06:44<32:59,  8.35s/it] 17%|█▋        | 49/285 [06:53<33:16,  8.46s/it] 18%|█▊        | 50/285 [07:02<33:38,  8.59s/it] 18%|█▊        | 51/285 [07:11<34:05,  8.74s/it] 18%|█▊        | 52/285 [07:20<33:42,  8.68s/it] 19%|█▊        | 53/285 [07:28<33:24,  8.64s/it] 19%|█▉        | 54/285 [07:37<33:34,  8.72s/it] 19%|█▉        | 55/285 [07:46<33:12,  8.66s/it] 20%|█▉        | 56/285 [07:55<33:45,  8.85s/it] 20%|██        | 57/285 [08:03<33:20,  8.78s/it] 20%|██        | 58/285 [08:13<34:39,  9.16s/it] 21%|██        | 59/285 [08:22<33:18,  8.84s/it] 21%|██        | 60/285 [08:30<32:53,  8.77s/it] 21%|██▏       | 61/285 [08:38<31:49,  8.52s/it] 22%|██▏       | 62/285 [08:46<30:56,  8.32s/it] 22%|██▏       | 63/285 [08:55<31:00,  8.38s/it] 22%|██▏       | 64/285 [09:03<30:30,  8.28s/it] 23%|██▎       | 65/285 [09:11<30:27,  8.31s/it] 23%|██▎       | 66/285 [09:20<31:03,  8.51s/it] 24%|██▎       | 67/285 [09:27<29:54,  8.23s/it] 24%|██▍       | 68/285 [09:36<29:52,  8.26s/it] 24%|██▍       | 69/285 [09:45<30:28,  8.47s/it] 25%|██▍       | 70/285 [09:53<30:08,  8.41s/it] 25%|██▍       | 71/285 [10:02<30:05,  8.44s/it] 25%|██▌       | 72/285 [10:10<29:57,  8.44s/it] 26%|██▌       | 73/285 [10:18<29:18,  8.30s/it] 26%|██▌       | 74/285 [10:26<28:25,  8.08s/it] 26%|██▋       | 75/285 [10:34<28:23,  8.11s/it] 27%|██▋       | 76/285 [10:42<28:27,  8.17s/it] 27%|██▋       | 77/285 [10:51<29:13,  8.43s/it] 27%|██▋       | 78/285 [10:59<28:32,  8.27s/it] 28%|██▊       | 79/285 [11:07<28:35,  8.33s/it] 28%|██▊       | 80/285 [11:15<27:47,  8.13s/it] 28%|██▊       | 81/285 [11:23<27:12,  8.00s/it] 29%|██▉       | 82/285 [11:31<27:43,  8.20s/it] 29%|██▉       | 83/285 [11:40<27:33,  8.19s/it] 29%|██▉       | 84/285 [11:48<27:26,  8.19s/it] 30%|██▉       | 85/285 [11:57<28:01,  8.41s/it] 30%|███       | 86/285 [12:05<27:38,  8.34s/it] 31%|███       | 87/285 [12:13<26:58,  8.17s/it] 31%|███       | 88/285 [12:21<26:41,  8.13s/it] 31%|███       | 89/285 [12:29<26:36,  8.15s/it] 32%|███▏      | 90/285 [12:37<26:23,  8.12s/it] 32%|███▏      | 91/285 [12:45<26:02,  8.05s/it] 32%|███▏      | 92/285 [12:53<26:12,  8.15s/it] 33%|███▎      | 93/285 [13:01<26:05,  8.15s/it] 33%|███▎      | 94/285 [13:10<26:21,  8.28s/it] 33%|███▎      | 95/285 [13:18<26:11,  8.27s/it] 34%|███▎      | 96/285 [13:26<25:52,  8.21s/it] 34%|███▍      | 97/285 [13:35<26:30,  8.46s/it] 34%|███▍      | 98/285 [13:45<27:14,  8.74s/it] 35%|███▍      | 99/285 [13:53<26:36,  8.59s/it] 35%|███▌      | 100/285 [14:01<26:22,  8.55s/it] 35%|███▌      | 101/285 [14:10<26:03,  8.50s/it] 36%|███▌      | 102/285 [14:18<26:00,  8.53s/it] 36%|███▌      | 103/285 [14:27<25:31,  8.42s/it] 36%|███▋      | 104/285 [14:35<25:16,  8.38s/it] 37%|███▋      | 105/285 [14:43<24:41,  8.23s/it] 37%|███▋      | 106/285 [14:51<24:49,  8.32s/it] 38%|███▊      | 107/285 [15:00<24:39,  8.31s/it] 38%|███▊      | 108/285 [15:08<24:27,  8.29s/it] 38%|███▊      | 109/285 [15:17<24:53,  8.49s/it] 39%|███▊      | 110/285 [15:25<24:46,  8.50s/it] 39%|███▉      | 111/285 [15:34<24:39,  8.50s/it] 39%|███▉      | 112/285 [15:43<24:50,  8.61s/it] 40%|███▉      | 113/285 [15:51<24:53,  8.69s/it] 40%|████      | 114/285 [15:59<24:09,  8.48s/it] 40%|████      | 115/285 [16:09<24:40,  8.71s/it] 41%|████      | 116/285 [16:17<24:27,  8.68s/it] 41%|████      | 117/285 [16:26<23:53,  8.53s/it] 41%|████▏     | 118/285 [16:34<23:23,  8.40s/it] 42%|████▏     | 119/285 [16:41<22:43,  8.22s/it] 42%|████▏     | 120/285 [16:50<22:37,  8.22s/it] 42%|████▏     | 121/285 [16:58<22:35,  8.26s/it] 43%|████▎     | 122/285 [17:06<22:31,  8.29s/it] 43%|████▎     | 123/285 [17:15<22:39,  8.39s/it] 44%|████▎     | 124/285 [17:23<22:19,  8.32s/it] 44%|████▍     | 125/285 [17:32<22:15,  8.35s/it] 44%|████▍     | 126/285 [17:40<22:10,  8.37s/it] 45%|████▍     | 127/285 [17:48<21:52,  8.31s/it] 45%|████▍     | 128/285 [17:56<21:15,  8.12s/it] 45%|████▌     | 129/285 [18:04<21:09,  8.14s/it] 46%|████▌     | 130/285 [18:12<21:06,  8.17s/it] 46%|████▌     | 131/285 [18:21<21:14,  8.27s/it] 46%|████▋     | 132/285 [18:29<21:15,  8.34s/it] 47%|████▋     | 133/285 [18:37<20:58,  8.28s/it] 47%|████▋     | 134/285 [18:46<21:14,  8.44s/it] 47%|████▋     | 135/285 [18:55<21:00,  8.40s/it] 48%|████▊     | 136/285 [19:02<20:17,  8.17s/it] 48%|████▊     | 137/285 [19:10<20:06,  8.15s/it] 48%|████▊     | 138/285 [19:18<19:48,  8.08s/it] 49%|████▉     | 139/285 [19:26<19:43,  8.11s/it] 49%|████▉     | 140/285 [19:35<19:55,  8.24s/it] 49%|████▉     | 141/285 [19:43<19:57,  8.32s/it] 50%|████▉     | 142/285 [19:51<19:31,  8.20s/it] 50%|█████     | 143/285 [19:59<19:09,  8.10s/it] 51%|█████     | 144/285 [20:07<18:51,  8.02s/it] 51%|█████     | 145/285 [20:16<19:15,  8.25s/it] 51%|█████     | 146/285 [20:24<18:53,  8.15s/it] 52%|█████▏    | 147/285 [20:32<18:38,  8.11s/it] 52%|█████▏    | 148/285 [20:40<18:42,  8.19s/it] 52%|█████▏    | 149/285 [20:49<18:44,  8.27s/it] 53%|█████▎    | 150/285 [20:57<18:31,  8.23s/it] 53%|█████▎    | 151/285 [21:05<18:40,  8.36s/it] 53%|█████▎    | 152/285 [21:14<18:32,  8.36s/it] 54%|█████▎    | 153/285 [21:22<18:38,  8.47s/it] 54%|█████▍    | 154/285 [21:31<18:23,  8.42s/it] 54%|█████▍    | 155/285 [21:39<18:02,  8.33s/it] 55%|█████▍    | 156/285 [21:48<18:05,  8.41s/it] 55%|█████▌    | 157/285 [21:55<17:36,  8.25s/it] 55%|█████▌    | 158/285 [22:03<17:20,  8.19s/it] 56%|█████▌    | 159/285 [22:12<17:14,  8.21s/it] 56%|█████▌    | 160/285 [22:19<16:47,  8.06s/it] 56%|█████▋    | 161/285 [22:28<16:59,  8.22s/it] 57%|█████▋    | 162/285 [22:36<17:01,  8.30s/it] 57%|█████▋    | 163/285 [22:45<17:05,  8.41s/it] 58%|█████▊    | 164/285 [22:54<17:03,  8.46s/it] 58%|█████▊    | 165/285 [23:03<17:13,  8.61s/it] 58%|█████▊    | 166/285 [23:11<16:37,  8.38s/it] 59%|█████▊    | 167/285 [23:19<16:18,  8.30s/it] 59%|█████▉    | 168/285 [23:27<16:00,  8.21s/it] 59%|█████▉    | 169/285 [23:35<15:51,  8.20s/it] 60%|█████▉    | 170/285 [23:43<15:40,  8.18s/it] 60%|██████    | 171/285 [23:51<15:35,  8.20s/it] 60%|██████    | 172/285 [24:00<15:44,  8.36s/it] 61%|██████    | 173/285 [24:08<15:43,  8.43s/it] 61%|██████    | 174/285 [24:17<15:44,  8.51s/it] 61%|██████▏   | 175/285 [24:26<15:43,  8.57s/it] 62%|██████▏   | 176/285 [24:35<15:52,  8.74s/it] 62%|██████▏   | 177/285 [24:44<15:51,  8.81s/it] 62%|██████▏   | 178/285 [24:53<15:38,  8.77s/it] 63%|██████▎   | 179/285 [25:01<15:21,  8.69s/it] 63%|██████▎   | 180/285 [25:10<15:01,  8.58s/it] 64%|██████▎   | 181/285 [25:18<14:58,  8.64s/it] 64%|██████▍   | 182/285 [25:27<14:54,  8.68s/it] 64%|██████▍   | 183/285 [25:35<14:33,  8.56s/it] 65%|██████▍   | 184/285 [25:44<14:38,  8.70s/it] 65%|██████▍   | 185/285 [25:53<14:20,  8.61s/it] 65%|██████▌   | 186/285 [26:02<14:25,  8.74s/it] 66%|██████▌   | 187/285 [26:10<14:01,  8.59s/it] 66%|██████▌   | 188/285 [26:19<13:52,  8.58s/it] 66%|██████▋   | 189/285 [26:27<13:35,  8.49s/it] 67%|██████▋   | 190/285 [26:35<13:25,  8.48s/it] 67%|██████▋   | 191/285 [26:44<13:21,  8.53s/it] 67%|██████▋   | 192/285 [26:53<13:29,  8.71s/it] 68%|██████▊   | 193/285 [27:02<13:24,  8.75s/it] 68%|██████▊   | 194/285 [27:11<13:12,  8.71s/it] 68%|██████▊   | 195/285 [27:20<13:12,  8.81s/it] 69%|██████▉   | 196/285 [27:28<12:54,  8.70s/it] 69%|██████▉   | 197/285 [27:36<12:32,  8.55s/it] 69%|██████▉   | 198/285 [27:45<12:19,  8.50s/it] 70%|██████▉   | 199/285 [27:53<12:06,  8.45s/it] 70%|███████   | 200/285 [28:01<11:53,  8.40s/it]                                                  70%|███████   | 200/285 [28:01<11:53,  8.40s/it] 71%|███████   | 201/285 [28:09<11:39,  8.33s/it] 71%|███████   | 202/285 [28:18<11:34,  8.37s/it] 71%|███████   | 203/285 [28:26<11:15,  8.24s/it] 72%|███████▏  | 204/285 [28:35<11:19,  8.39s/it] 72%|███████▏  | 205/285 [28:43<11:00,  8.26s/it] 72%|███████▏  | 206/285 [28:51<11:01,  8.38s/it] 73%|███████▎  | 207/285 [29:00<11:13,  8.64s/it] 73%|███████▎  | 208/285 [29:09<11:02,  8.60s/it] 73%|███████▎  | 209/285 [29:18<11:03,  8.73s/it] 74%|███████▎  | 210/285 [29:26<10:45,  8.61s/it] 74%|███████▍  | 211/285 [29:35<10:31,  8.53s/it] 74%|███████▍  | 212/285 [29:43<10:26,  8.58s/it] 75%|███████▍  | 213/285 [29:52<10:14,  8.53s/it] 75%|███████▌  | 214/285 [30:00<09:59,  8.44s/it] 75%|███████▌  | 215/285 [30:08<09:45,  8.36s/it] 76%|███████▌  | 216/285 [30:17<09:50,  8.56s/it] 76%|███████▌  | 217/285 [30:25<09:29,  8.38s/it] 76%|███████▋  | 218/285 [30:34<09:21,  8.38s/it] 77%|███████▋  | 219/285 [30:42<09:12,  8.37s/it] 77%|███████▋  | 220/285 [30:50<09:06,  8.41s/it] 78%|███████▊  | 221/285 [30:59<08:55,  8.37s/it] 78%|███████▊  | 222/285 [31:07<08:45,  8.34s/it] 78%|███████▊  | 223/285 [31:15<08:40,  8.39s/it] 79%|███████▊  | 224/285 [31:24<08:32,  8.40s/it] 79%|███████▉  | 225/285 [31:32<08:20,  8.35s/it] 79%|███████▉  | 226/285 [31:41<08:15,  8.40s/it] 80%|███████▉  | 227/285 [31:49<08:05,  8.36s/it] 80%|████████  | 228/285 [31:58<08:03,  8.48s/it] 80%|████████  | 229/285 [32:07<08:12,  8.79s/it] 81%|████████  | 230/285 [32:16<08:00,  8.73s/it] 81%|████████  | 231/285 [32:24<07:44,  8.60s/it] 81%|████████▏ | 232/285 [32:33<07:37,  8.63s/it] 82%|████████▏ | 233/285 [32:41<07:25,  8.56s/it] 82%|████████▏ | 234/285 [32:50<07:17,  8.58s/it] 82%|████████▏ | 235/285 [32:58<07:06,  8.54s/it] 83%|████████▎ | 236/285 [33:06<06:51,  8.39s/it] 83%|████████▎ | 237/285 [33:15<06:45,  8.44s/it] 84%|████████▎ | 238/285 [33:23<06:37,  8.46s/it] 84%|████████▍ | 239/285 [33:32<06:38,  8.66s/it] 84%|████████▍ | 240/285 [33:42<06:35,  8.78s/it] 85%|████████▍ | 241/285 [33:50<06:20,  8.65s/it] 85%|████████▍ | 242/285 [33:59<06:12,  8.66s/it] 85%|████████▌ | 243/285 [34:07<06:06,  8.73s/it] 86%|████████▌ | 244/285 [34:16<05:52,  8.61s/it] 86%|████████▌ | 245/285 [34:24<05:42,  8.57s/it] 86%|████████▋ | 246/285 [34:32<05:29,  8.44s/it] 87%|████████▋ | 247/285 [34:42<05:30,  8.70s/it] 87%|████████▋ | 248/285 [34:50<05:21,  8.68s/it] 87%|████████▋ | 249/285 [34:59<05:12,  8.68s/it] 88%|████████▊ | 250/285 [35:09<05:13,  8.96s/it] 88%|████████▊ | 251/285 [35:18<05:06,  9.01s/it] 88%|████████▊ | 252/285 [35:27<04:59,  9.08s/it] 89%|████████▉ | 253/285 [35:36<04:48,  9.01s/it] 89%|████████▉ | 254/285 [35:45<04:37,  8.95s/it] 89%|████████▉ | 255/285 [35:53<04:26,  8.90s/it] 90%|████████▉ | 256/285 [36:02<04:12,  8.71s/it] 90%|█████████ | 257/285 [36:11<04:05,  8.78s/it] 91%|█████████ | 258/285 [36:19<03:56,  8.75s/it] 91%|█████████ | 259/285 [36:28<03:45,  8.69s/it] 91%|█████████ | 260/285 [36:36<03:33,  8.54s/it] 92%|█████████▏| 261/285 [36:44<03:21,  8.38s/it] 92%|█████████▏| 262/285 [36:53<03:14,  8.45s/it] 92%|█████████▏| 263/285 [37:01<03:06,  8.47s/it] 93%|█████████▎| 264/285 [37:09<02:56,  8.42s/it] 93%|█████████▎| 265/285 [37:18<02:47,  8.36s/it] 93%|█████████▎| 266/285 [37:25<02:35,  8.19s/it] 94%|█████████▎| 267/285 [37:34<02:28,  8.25s/it] 94%|█████████▍| 268/285 [37:42<02:20,  8.25s/it] 94%|█████████▍| 269/285 [37:50<02:11,  8.20s/it] 95%|█████████▍| 270/285 [37:59<02:04,  8.32s/it] 95%|█████████▌| 271/285 [38:07<01:55,  8.24s/it] 95%|█████████▌| 272/285 [38:15<01:45,  8.12s/it] 96%|█████████▌| 273/285 [38:23<01:37,  8.11s/it] 96%|█████████▌| 274/285 [38:32<01:31,  8.30s/it] 96%|█████████▋| 275/285 [38:40<01:24,  8.49s/it] 97%|█████████▋| 276/285 [38:49<01:15,  8.44s/it] 97%|█████████▋| 277/285 [38:56<01:05,  8.22s/it] 98%|█████████▊| 278/285 [39:05<00:57,  8.23s/it] 98%|█████████▊| 279/285 [39:13<00:49,  8.20s/it] 98%|█████████▊| 280/285 [39:22<00:42,  8.41s/it] 99%|█████████▊| 281/285 [39:30<00:33,  8.39s/it] 99%|█████████▉| 282/285 [39:38<00:25,  8.35s/it] 99%|█████████▉| 283/285 [39:47<00:17,  8.53s/it]100%|█████████▉| 284/285 [39:55<00:08,  8.34s/it]100%|██████████| 285/285 [40:04<00:00,  8.41s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [40:04<00:00,  8.41s/it]100%|██████████| 285/285 [40:04<00:00,  8.44s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 3654
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 285
  Number of trainable parameters = 652356503
{'loss': 0.257, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 2404.2879, 'train_samples_per_second': 7.599, 'train_steps_per_second': 0.119, 'train_loss': 0.18460323475954826, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_imbalanced_train.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/285 [00:00<?, ?it/s]  0%|          | 1/285 [00:09<43:40,  9.23s/it]  1%|          | 2/285 [00:18<44:18,  9.39s/it]  1%|          | 3/285 [00:28<45:15,  9.63s/it]  1%|▏         | 4/285 [00:38<44:56,  9.59s/it]  2%|▏         | 5/285 [00:48<45:12,  9.69s/it]  2%|▏         | 6/285 [00:57<44:54,  9.66s/it]  2%|▏         | 7/285 [01:06<44:00,  9.50s/it]  3%|▎         | 8/285 [01:16<43:57,  9.52s/it]  3%|▎         | 9/285 [01:25<43:10,  9.39s/it]  4%|▎         | 10/285 [01:35<43:24,  9.47s/it]  4%|▍         | 11/285 [01:44<42:58,  9.41s/it]  4%|▍         | 12/285 [01:53<42:53,  9.43s/it]  5%|▍         | 13/285 [02:03<42:48,  9.44s/it]  5%|▍         | 14/285 [02:13<43:28,  9.63s/it]  5%|▌         | 15/285 [02:22<43:14,  9.61s/it]  6%|▌         | 16/285 [02:32<42:35,  9.50s/it]  6%|▌         | 17/285 [02:41<42:12,  9.45s/it]  6%|▋         | 18/285 [02:51<42:06,  9.46s/it]  7%|▋         | 19/285 [03:00<41:42,  9.41s/it]  7%|▋         | 20/285 [03:09<41:39,  9.43s/it]  7%|▋         | 21/285 [03:19<41:56,  9.53s/it]  8%|▊         | 22/285 [03:28<41:18,  9.43s/it]  8%|▊         | 23/285 [03:38<41:20,  9.47s/it]  8%|▊         | 24/285 [03:47<41:21,  9.51s/it]  9%|▉         | 25/285 [03:57<41:18,  9.53s/it]  9%|▉         | 26/285 [04:06<40:40,  9.42s/it]  9%|▉         | 27/285 [04:15<40:08,  9.34s/it] 10%|▉         | 28/285 [04:25<40:31,  9.46s/it] 10%|█         | 29/285 [04:34<40:03,  9.39s/it] 11%|█         | 30/285 [04:44<40:12,  9.46s/it] 11%|█         | 31/285 [04:53<40:12,  9.50s/it] 11%|█         | 32/285 [05:03<39:48,  9.44s/it] 12%|█▏        | 33/285 [05:12<39:33,  9.42s/it] 12%|█▏        | 34/285 [05:22<39:37,  9.47s/it] 12%|█▏        | 35/285 [05:32<39:48,  9.55s/it] 13%|█▎        | 36/285 [05:41<40:04,  9.66s/it] 13%|█▎        | 37/285 [05:52<40:32,  9.81s/it] 13%|█▎        | 38/285 [06:01<40:27,  9.83s/it] 14%|█▎        | 39/285 [06:11<40:14,  9.82s/it] 14%|█▍        | 40/285 [06:21<39:26,  9.66s/it] 14%|█▍        | 41/285 [06:30<38:51,  9.56s/it] 15%|█▍        | 42/285 [06:39<38:16,  9.45s/it] 15%|█▌        | 43/285 [06:48<37:43,  9.35s/it] 15%|█▌        | 44/285 [06:57<37:26,  9.32s/it] 16%|█▌        | 45/285 [07:07<37:13,  9.31s/it] 16%|█▌        | 46/285 [07:16<36:32,  9.17s/it] 16%|█▋        | 47/285 [07:24<36:02,  9.09s/it] 17%|█▋        | 48/285 [07:34<36:14,  9.18s/it] 17%|█▋        | 49/285 [07:43<36:33,  9.29s/it] 18%|█▊        | 50/285 [07:53<36:51,  9.41s/it] 18%|█▊        | 51/285 [08:03<37:13,  9.54s/it] 18%|█▊        | 52/285 [08:12<36:42,  9.45s/it] 19%|█▊        | 53/285 [08:21<36:23,  9.41s/it] 19%|█▉        | 54/285 [08:31<36:25,  9.46s/it] 19%|█▉        | 55/285 [08:40<35:59,  9.39s/it] 20%|█▉        | 56/285 [08:50<36:25,  9.54s/it] 20%|██        | 57/285 [09:00<36:01,  9.48s/it] 20%|██        | 58/285 [09:10<37:12,  9.83s/it] 21%|██        | 59/285 [09:19<35:58,  9.55s/it] 21%|██        | 60/285 [09:28<35:12,  9.39s/it] 21%|██▏       | 61/285 [09:37<34:23,  9.21s/it] 22%|██▏       | 62/285 [09:46<33:46,  9.09s/it] 22%|██▏       | 63/285 [09:55<33:56,  9.17s/it] 22%|██▏       | 64/285 [10:04<33:36,  9.12s/it] 23%|██▎       | 65/285 [10:13<33:35,  9.16s/it] 23%|██▎       | 66/285 [10:23<33:59,  9.31s/it] 24%|██▎       | 67/285 [10:32<33:02,  9.09s/it] 24%|██▍       | 68/285 [10:41<33:06,  9.15s/it] 24%|██▍       | 69/285 [10:51<33:39,  9.35s/it] 25%|██▍       | 70/285 [11:00<33:25,  9.33s/it] 25%|██▍       | 71/285 [11:09<33:22,  9.36s/it] 25%|██▌       | 72/285 [11:19<33:15,  9.37s/it] 26%|██▌       | 73/285 [11:28<32:43,  9.26s/it] 26%|██▌       | 74/285 [11:36<31:51,  9.06s/it] 26%|██▋       | 75/285 [11:46<31:50,  9.10s/it] 27%|██▋       | 76/285 [11:55<31:51,  9.15s/it] 27%|██▋       | 77/285 [12:05<32:45,  9.45s/it] 27%|██▋       | 78/285 [12:14<32:20,  9.38s/it] 28%|██▊       | 79/285 [12:24<32:30,  9.47s/it] 28%|██▊       | 80/285 [12:33<31:53,  9.34s/it] 28%|██▊       | 81/285 [12:42<31:14,  9.19s/it] 29%|██▉       | 82/285 [12:51<31:30,  9.31s/it] 29%|██▉       | 83/285 [13:00<31:10,  9.26s/it] 29%|██▉       | 84/285 [13:10<30:50,  9.21s/it] 30%|██▉       | 85/285 [13:19<31:25,  9.43s/it] 30%|███       | 86/285 [13:29<30:59,  9.35s/it] 31%|███       | 87/285 [13:37<30:20,  9.20s/it] 31%|███       | 88/285 [13:46<30:00,  9.14s/it] 31%|███       | 89/285 [13:56<29:56,  9.17s/it] 32%|███▏      | 90/285 [14:05<29:42,  9.14s/it] 32%|███▏      | 91/285 [14:14<29:19,  9.07s/it] 32%|███▏      | 92/285 [14:23<29:41,  9.23s/it] 33%|███▎      | 93/285 [14:32<29:29,  9.22s/it] 33%|███▎      | 94/285 [14:42<29:44,  9.35s/it] 33%|███▎      | 95/285 [14:51<29:31,  9.32s/it] 34%|███▎      | 96/285 [15:01<29:11,  9.27s/it] 34%|███▍      | 97/285 [15:09<28:44,  9.18s/it] 34%|███▍      | 98/285 [15:19<28:43,  9.22s/it] 35%|███▍      | 99/285 [15:28<28:06,  9.07s/it] 35%|███▌      | 100/285 [15:37<28:20,  9.19s/it] 35%|███▌      | 101/285 [15:46<28:25,  9.27s/it] 36%|███▌      | 102/285 [15:56<28:10,  9.24s/it] 36%|███▌      | 103/285 [16:05<27:46,  9.16s/it] 36%|███▋      | 104/285 [16:14<27:31,  9.12s/it] 37%|███▋      | 105/285 [16:23<27:48,  9.27s/it] 37%|███▋      | 106/285 [16:33<28:21,  9.51s/it] 38%|███▊      | 107/285 [16:43<28:32,  9.62s/it] 38%|███▊      | 108/285 [16:53<28:30,  9.66s/it] 38%|███▊      | 109/285 [17:03<28:51,  9.84s/it] 39%|███▊      | 110/285 [17:13<28:43,  9.85s/it] 39%|███▉      | 111/285 [17:23<28:18,  9.76s/it] 39%|███▉      | 112/285 [17:32<28:12,  9.78s/it] 40%|███▉      | 113/285 [17:42<28:08,  9.82s/it] 40%|████      | 114/285 [17:52<27:25,  9.63s/it] 40%|████      | 115/285 [18:02<28:08,  9.93s/it] 41%|████      | 116/285 [18:12<27:55,  9.91s/it] 41%|████      | 117/285 [18:21<27:18,  9.75s/it] 41%|████▏     | 118/285 [18:31<26:53,  9.66s/it] 42%|████▏     | 119/285 [18:40<26:10,  9.46s/it] 42%|████▏     | 120/285 [18:49<26:01,  9.46s/it] 42%|████▏     | 121/285 [18:59<25:57,  9.50s/it] 43%|████▎     | 122/285 [19:09<25:57,  9.55s/it] 43%|████▎     | 123/285 [19:19<26:05,  9.66s/it] 44%|████▎     | 124/285 [19:28<25:46,  9.60s/it] 44%|████▍     | 125/285 [19:38<25:40,  9.63s/it] 44%|████▍     | 126/285 [19:47<25:13,  9.52s/it] 45%|████▍     | 127/285 [19:56<25:03,  9.52s/it] 45%|████▍     | 128/285 [20:05<24:17,  9.28s/it] 45%|████▌     | 129/285 [20:15<24:12,  9.31s/it] 46%|████▌     | 130/285 [20:24<24:16,  9.40s/it] 46%|████▌     | 131/285 [20:34<24:24,  9.51s/it] 46%|████▋     | 132/285 [20:44<24:19,  9.54s/it] 47%|████▋     | 133/285 [20:53<24:16,  9.59s/it] 47%|████▋     | 134/285 [21:04<24:39,  9.80s/it] 47%|████▋     | 135/285 [21:13<24:21,  9.74s/it] 48%|████▊     | 136/285 [21:22<23:45,  9.57s/it] 48%|████▊     | 137/285 [21:32<23:43,  9.62s/it] 48%|████▊     | 138/285 [21:41<23:18,  9.51s/it] 49%|████▉     | 139/285 [21:51<23:15,  9.56s/it] 49%|████▉     | 140/285 [22:01<23:19,  9.65s/it] 49%|████▉     | 141/285 [22:11<23:15,  9.69s/it] 50%|████▉     | 142/285 [22:20<22:52,  9.60s/it] 50%|█████     | 143/285 [22:29<22:36,  9.55s/it] 51%|█████     | 144/285 [22:39<22:39,  9.64s/it] 51%|█████     | 145/285 [22:50<22:57,  9.84s/it] 51%|█████     | 146/285 [22:59<22:35,  9.75s/it] 52%|█████▏    | 147/285 [23:09<22:08,  9.63s/it] 52%|█████▏    | 148/285 [23:18<22:07,  9.69s/it] 52%|█████▏    | 149/285 [23:28<21:52,  9.65s/it] 53%|█████▎    | 150/285 [23:37<21:38,  9.62s/it] 53%|█████▎    | 151/285 [23:47<21:34,  9.66s/it] 53%|█████▎    | 152/285 [23:57<21:25,  9.66s/it] 54%|█████▎    | 153/285 [24:07<21:32,  9.79s/it] 54%|█████▍    | 154/285 [24:17<21:17,  9.75s/it] 54%|█████▍    | 155/285 [24:26<21:09,  9.76s/it] 55%|█████▍    | 156/285 [24:36<21:06,  9.82s/it] 55%|█████▌    | 157/285 [24:46<20:35,  9.65s/it] 55%|█████▌    | 158/285 [24:55<20:20,  9.61s/it] 56%|█████▌    | 159/285 [25:05<20:21,  9.70s/it] 56%|█████▌    | 160/285 [25:14<19:56,  9.57s/it] 56%|█████▋    | 161/285 [25:24<19:57,  9.66s/it] 57%|█████▋    | 162/285 [25:34<19:53,  9.70s/it] 57%|█████▋    | 163/285 [25:44<19:56,  9.81s/it] 58%|█████▊    | 164/285 [25:54<19:50,  9.84s/it] 58%|█████▊    | 165/285 [26:04<19:57,  9.98s/it] 58%|█████▊    | 166/285 [26:14<19:28,  9.82s/it] 59%|█████▊    | 167/285 [26:23<19:09,  9.74s/it] 59%|█████▉    | 168/285 [26:33<18:46,  9.63s/it] 59%|█████▉    | 169/285 [26:42<18:31,  9.59s/it] 60%|█████▉    | 170/285 [26:52<18:22,  9.58s/it] 60%|██████    | 171/285 [27:01<18:12,  9.58s/it] 60%|██████    | 172/285 [27:11<18:17,  9.72s/it] 61%|██████    | 173/285 [27:21<18:17,  9.80s/it] 61%|██████    | 174/285 [27:31<18:14,  9.86s/it] 61%|██████▏   | 175/285 [27:41<18:14,  9.95s/it] 62%|██████▏   | 176/285 [27:52<18:18, 10.08s/it] 62%|██████▏   | 177/285 [28:02<18:05, 10.05s/it] 62%|██████▏   | 178/285 [28:12<17:46,  9.96s/it] 63%|██████▎   | 179/285 [28:22<17:36,  9.97s/it] 63%|██████▎   | 180/285 [28:31<17:16,  9.87s/it] 64%|██████▎   | 181/285 [28:41<17:07,  9.88s/it] 64%|██████▍   | 182/285 [28:51<17:07,  9.97s/it] 64%|██████▍   | 183/285 [29:01<16:43,  9.84s/it] 65%|██████▍   | 184/285 [29:11<16:38,  9.88s/it] 65%|██████▍   | 185/285 [29:21<16:27,  9.87s/it] 65%|██████▌   | 186/285 [29:31<16:28,  9.99s/it] 66%|██████▌   | 187/285 [29:40<15:58,  9.78s/it] 66%|██████▌   | 188/285 [29:50<15:43,  9.73s/it] 66%|██████▋   | 189/285 [29:59<15:28,  9.67s/it] 67%|██████▋   | 190/285 [30:09<15:18,  9.67s/it] 67%|██████▋   | 191/285 [30:19<15:07,  9.65s/it] 67%|██████▋   | 192/285 [30:29<15:12,  9.81s/it] 68%|██████▊   | 193/285 [30:39<15:07,  9.86s/it] 68%|██████▊   | 194/285 [30:49<14:58,  9.87s/it] 68%|██████▊   | 195/285 [30:59<14:57,  9.98s/it] 69%|██████▉   | 196/285 [31:08<14:30,  9.78s/it] 69%|██████▉   | 197/285 [31:17<14:00,  9.55s/it] 69%|██████▉   | 198/285 [31:26<13:40,  9.44s/it] 70%|██████▉   | 199/285 [31:36<13:24,  9.35s/it] 70%|███████   | 200/285 [31:45<13:13,  9.33s/it]                                                  70%|███████   | 200/285 [31:45<13:13,  9.33s/it] 71%|███████   | 201/285 [31:54<13:03,  9.33s/it] 71%|███████   | 202/285 [32:04<13:00,  9.40s/it] 71%|███████   | 203/285 [32:13<12:45,  9.34s/it] 72%|███████▏  | 204/285 [32:23<12:53,  9.55s/it] 72%|███████▏  | 205/285 [32:32<12:36,  9.46s/it] 72%|███████▏  | 206/285 [32:42<12:33,  9.54s/it] 73%|███████▎  | 207/285 [32:52<12:41,  9.77s/it] 73%|███████▎  | 208/285 [33:02<12:26,  9.69s/it] 73%|███████▎  | 209/285 [33:12<12:23,  9.78s/it] 74%|███████▎  | 210/285 [33:21<12:01,  9.63s/it] 74%|███████▍  | 211/285 [33:30<11:44,  9.52s/it] 74%|███████▍  | 212/285 [33:40<11:35,  9.53s/it] 75%|███████▍  | 213/285 [33:49<11:19,  9.44s/it] 75%|███████▌  | 214/285 [33:58<11:03,  9.35s/it] 75%|███████▌  | 215/285 [34:07<10:50,  9.29s/it] 76%|███████▌  | 216/285 [34:17<10:54,  9.48s/it] 76%|███████▌  | 217/285 [34:26<10:32,  9.30s/it] 76%|███████▋  | 218/285 [34:36<10:25,  9.33s/it] 77%|███████▋  | 219/285 [34:45<10:15,  9.33s/it] 77%|███████▋  | 220/285 [34:54<10:11,  9.40s/it] 78%|███████▊  | 221/285 [35:04<10:01,  9.40s/it] 78%|███████▊  | 222/285 [35:13<09:51,  9.38s/it] 78%|███████▊  | 223/285 [35:23<09:45,  9.44s/it] 79%|███████▊  | 224/285 [35:32<09:41,  9.54s/it] 79%|███████▉  | 225/285 [35:42<09:32,  9.53s/it] 79%|███████▉  | 226/285 [35:52<09:24,  9.56s/it] 80%|███████▉  | 227/285 [36:01<09:10,  9.49s/it] 80%|████████  | 228/285 [36:11<09:11,  9.67s/it] 80%|████████  | 229/285 [36:22<09:19,  9.99s/it] 81%|████████  | 230/285 [36:32<09:04,  9.91s/it] 81%|████████  | 231/285 [36:41<08:46,  9.75s/it] 81%|████████▏ | 232/285 [36:51<08:35,  9.73s/it] 82%|████████▏ | 233/285 [37:00<08:19,  9.61s/it] 82%|████████▏ | 234/285 [37:09<08:07,  9.57s/it] 82%|████████▏ | 235/285 [37:19<07:54,  9.48s/it] 83%|████████▎ | 236/285 [37:27<07:33,  9.25s/it] 83%|████████▎ | 237/285 [37:37<07:23,  9.23s/it] 84%|████████▎ | 238/285 [37:46<07:12,  9.20s/it] 84%|████████▍ | 239/285 [37:56<07:12,  9.39s/it] 84%|████████▍ | 240/285 [38:06<07:11,  9.59s/it] 85%|████████▍ | 241/285 [38:16<07:07,  9.71s/it] 85%|████████▍ | 242/285 [38:26<07:05,  9.89s/it] 85%|████████▌ | 243/285 [38:36<07:02, 10.07s/it] 86%|████████▌ | 244/285 [38:46<06:49,  9.99s/it] 86%|████████▌ | 245/285 [38:56<06:42, 10.06s/it] 86%|████████▋ | 246/285 [39:06<06:22,  9.82s/it] 87%|████████▋ | 247/285 [39:16<06:18,  9.95s/it] 87%|████████▋ | 248/285 [39:25<06:02,  9.80s/it] 87%|████████▋ | 249/285 [39:35<05:47,  9.65s/it] 88%|████████▊ | 250/285 [39:45<05:42,  9.78s/it] 88%|████████▊ | 251/285 [39:54<05:27,  9.62s/it] 88%|████████▊ | 252/285 [40:04<05:16,  9.60s/it] 89%|████████▉ | 253/285 [40:13<05:03,  9.49s/it] 89%|████████▉ | 254/285 [40:22<04:55,  9.53s/it] 89%|████████▉ | 255/285 [40:32<04:49,  9.65s/it] 90%|████████▉ | 256/285 [40:42<04:37,  9.56s/it] 90%|█████████ | 257/285 [40:52<04:31,  9.68s/it] 91%|█████████ | 258/285 [41:01<04:18,  9.58s/it] 91%|█████████ | 259/285 [41:10<04:06,  9.47s/it] 91%|█████████ | 260/285 [41:19<03:54,  9.38s/it] 92%|█████████▏| 261/285 [41:29<03:44,  9.35s/it] 92%|█████████▏| 262/285 [41:39<03:38,  9.50s/it] 92%|█████████▏| 263/285 [41:48<03:28,  9.50s/it] 93%|█████████▎| 264/285 [41:57<03:17,  9.41s/it] 93%|█████████▎| 265/285 [42:06<03:06,  9.33s/it] 93%|█████████▎| 266/285 [42:15<02:54,  9.17s/it] 94%|█████████▎| 267/285 [42:25<02:46,  9.25s/it] 94%|█████████▍| 268/285 [42:34<02:37,  9.24s/it] 94%|█████████▍| 269/285 [42:43<02:27,  9.20s/it] 95%|█████████▍| 270/285 [42:53<02:19,  9.32s/it] 95%|█████████▌| 271/285 [43:02<02:09,  9.27s/it] 95%|█████████▌| 272/285 [43:11<01:59,  9.16s/it] 96%|█████████▌| 273/285 [43:20<01:49,  9.16s/it] 96%|█████████▌| 274/285 [43:29<01:42,  9.34s/it] 96%|█████████▋| 275/285 [43:39<01:35,  9.51s/it] 97%|█████████▋| 276/285 [43:49<01:25,  9.45s/it] 97%|█████████▋| 277/285 [43:57<01:13,  9.24s/it] 98%|█████████▊| 278/285 [44:07<01:04,  9.21s/it] 98%|█████████▊| 279/285 [44:16<00:55,  9.28s/it] 98%|█████████▊| 280/285 [44:26<00:47,  9.54s/it] 99%|█████████▊| 281/285 [44:36<00:37,  9.48s/it] 99%|█████████▉| 282/285 [44:45<00:28,  9.41s/it] 99%|█████████▉| 283/285 [44:55<00:19,  9.54s/it]100%|█████████▉| 284/285 [45:03<00:09,  9.32s/it]100%|██████████| 285/285 [45:13<00:00,  9.37s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 285/285 [45:13<00:00,  9.37s/it]100%|██████████| 285/285 [45:13<00:00,  9.52s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_imbalanced_train/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_imbalanced_train/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_imbalanced_train/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'loss': 0.386, 'learning_rate': 1e-05, 'epoch': 3.51}
{'train_runtime': 2713.4118, 'train_samples_per_second': 6.733, 'train_steps_per_second': 0.105, 'train_loss': 0.27919310352258514, 'epoch': 5.0}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:20,  6.68s/it]  5%|▌         | 2/40 [00:13<04:24,  6.97s/it]  8%|▊         | 3/40 [00:19<04:02,  6.56s/it] 10%|█         | 4/40 [00:26<03:55,  6.54s/it] 12%|█▎        | 5/40 [00:32<03:46,  6.48s/it] 15%|█▌        | 6/40 [00:38<03:34,  6.32s/it] 18%|█▊        | 7/40 [00:44<03:26,  6.26s/it] 20%|██        | 8/40 [00:51<03:23,  6.37s/it] 22%|██▎       | 9/40 [00:58<03:24,  6.60s/it] 25%|██▌       | 10/40 [01:04<03:14,  6.49s/it] 28%|██▊       | 11/40 [01:11<03:06,  6.43s/it] 30%|███       | 12/40 [01:17<02:57,  6.33s/it] 32%|███▎      | 13/40 [01:23<02:51,  6.33s/it] 35%|███▌      | 14/40 [01:30<02:45,  6.38s/it] 38%|███▊      | 15/40 [01:36<02:41,  6.44s/it] 40%|████      | 16/40 [01:43<02:34,  6.45s/it] 42%|████▎     | 17/40 [01:50<02:32,  6.62s/it] 45%|████▌     | 18/40 [01:56<02:26,  6.67s/it] 48%|████▊     | 19/40 [02:03<02:18,  6.58s/it] 50%|█████     | 20/40 [02:09<02:09,  6.48s/it] 52%|█████▎    | 21/40 [02:15<02:02,  6.42s/it] 55%|█████▌    | 22/40 [02:22<01:54,  6.37s/it] 57%|█████▊    | 23/40 [02:28<01:47,  6.30s/it] 60%|██████    | 24/40 [02:34<01:42,  6.38s/it] 62%|██████▎   | 25/40 [02:42<01:39,  6.64s/it] 65%|██████▌   | 26/40 [02:48<01:31,  6.54s/it] 68%|██████▊   | 27/40 [02:54<01:24,  6.50s/it] 70%|███████   | 28/40 [03:00<01:16,  6.39s/it] 72%|███████▎  | 29/40 [03:07<01:09,  6.36s/it] 75%|███████▌  | 30/40 [03:13<01:03,  6.35s/it] 78%|███████▊  | 31/40 [03:19<00:56,  6.28s/it] 80%|████████  | 32/40 [03:25<00:49,  6.22s/it] 82%|████████▎ | 33/40 [03:33<00:46,  6.59s/it] 85%|████████▌ | 34/40 [03:40<00:40,  6.72s/it] 88%|████████▊ | 35/40 [03:47<00:34,  6.81s/it] 90%|█████████ | 36/40 [03:54<00:27,  6.85s/it] 92%|█████████▎| 37/40 [04:00<00:20,  6.74s/it] 95%|█████████▌| 38/40 [04:07<00:13,  6.79s/it] 98%|█████████▊| 39/40 [04:14<00:06,  6.78s/it]100%|██████████| 40/40 [04:20<00:00,  6.59s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:20<00:00,  6.59s/it]100%|██████████| 40/40 [04:20<00:00,  6.51s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 260.5029, 'train_samples_per_second': 10.0, 'train_steps_per_second': 0.154, 'train_loss': 0.6989192962646484, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:05<03:53,  5.98s/it]  5%|▌         | 2/40 [00:12<04:00,  6.33s/it]  8%|▊         | 3/40 [00:18<03:48,  6.18s/it] 10%|█         | 4/40 [00:24<03:45,  6.27s/it] 12%|█▎        | 5/40 [00:31<03:39,  6.27s/it] 15%|█▌        | 6/40 [00:37<03:29,  6.15s/it] 18%|█▊        | 7/40 [00:43<03:21,  6.11s/it] 20%|██        | 8/40 [00:49<03:19,  6.24s/it] 22%|██▎       | 9/40 [00:56<03:20,  6.47s/it] 25%|██▌       | 10/40 [01:02<03:10,  6.35s/it] 28%|██▊       | 11/40 [01:08<03:02,  6.30s/it] 30%|███       | 12/40 [01:14<02:53,  6.21s/it] 32%|███▎      | 13/40 [01:21<02:48,  6.22s/it] 35%|███▌      | 14/40 [01:27<02:42,  6.27s/it] 38%|███▊      | 15/40 [01:34<02:39,  6.36s/it] 40%|████      | 16/40 [01:40<02:34,  6.42s/it] 42%|████▎     | 17/40 [01:47<02:32,  6.61s/it] 45%|████▌     | 18/40 [01:54<02:25,  6.61s/it] 48%|████▊     | 19/40 [02:00<02:15,  6.46s/it] 50%|█████     | 20/40 [02:06<02:08,  6.40s/it] 52%|█████▎    | 21/40 [02:13<02:01,  6.37s/it] 55%|█████▌    | 22/40 [02:19<01:54,  6.34s/it] 57%|█████▊    | 23/40 [02:25<01:46,  6.29s/it] 60%|██████    | 24/40 [02:32<01:41,  6.37s/it] 62%|██████▎   | 25/40 [02:39<01:39,  6.65s/it] 65%|██████▌   | 26/40 [02:45<01:31,  6.55s/it] 68%|██████▊   | 27/40 [02:52<01:24,  6.51s/it] 70%|███████   | 28/40 [02:58<01:16,  6.39s/it] 72%|███████▎  | 29/40 [03:04<01:09,  6.35s/it] 75%|███████▌  | 30/40 [03:10<01:03,  6.33s/it] 78%|███████▊  | 31/40 [03:16<00:56,  6.27s/it] 80%|████████  | 32/40 [03:22<00:49,  6.21s/it] 82%|████████▎ | 33/40 [03:30<00:45,  6.54s/it] 85%|████████▌ | 34/40 [03:36<00:39,  6.57s/it] 88%|████████▊ | 35/40 [03:43<00:32,  6.51s/it] 90%|█████████ | 36/40 [03:49<00:25,  6.42s/it] 92%|█████████▎| 37/40 [03:55<00:18,  6.25s/it] 95%|█████████▌| 38/40 [04:01<00:12,  6.31s/it] 98%|█████████▊| 39/40 [04:08<00:06,  6.30s/it]100%|██████████| 40/40 [04:14<00:00,  6.22s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:14<00:00,  6.22s/it]100%|██████████| 40/40 [04:14<00:00,  6.35s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 254.1037, 'train_samples_per_second': 10.252, 'train_steps_per_second': 0.157, 'train_loss': 0.7158740997314453, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:37,  8.65s/it]  5%|▌         | 2/40 [00:18<05:49,  9.20s/it]  8%|▊         | 3/40 [00:26<05:30,  8.95s/it] 10%|█         | 4/40 [00:36<05:28,  9.12s/it] 12%|█▎        | 5/40 [00:45<05:20,  9.15s/it] 15%|█▌        | 6/40 [00:54<05:05,  8.99s/it] 18%|█▊        | 7/40 [01:02<04:54,  8.93s/it] 20%|██        | 8/40 [01:12<04:53,  9.17s/it] 22%|██▎       | 9/40 [01:23<04:59,  9.65s/it] 25%|██▌       | 10/40 [01:32<04:42,  9.42s/it] 28%|██▊       | 11/40 [01:41<04:28,  9.25s/it] 30%|███       | 12/40 [01:49<04:13,  9.05s/it] 32%|███▎      | 13/40 [01:58<04:03,  9.03s/it] 35%|███▌      | 14/40 [02:07<03:55,  9.07s/it] 38%|███▊      | 15/40 [02:17<03:50,  9.21s/it] 40%|████      | 16/40 [02:26<03:42,  9.25s/it] 42%|████▎     | 17/40 [02:36<03:38,  9.48s/it] 45%|████▌     | 18/40 [02:46<03:28,  9.46s/it] 48%|████▊     | 19/40 [02:54<03:14,  9.26s/it] 50%|█████     | 20/40 [03:03<03:02,  9.14s/it] 52%|█████▎    | 21/40 [03:12<02:53,  9.11s/it] 55%|█████▌    | 22/40 [03:21<02:43,  9.06s/it] 57%|█████▊    | 23/40 [03:30<02:32,  8.99s/it] 60%|██████    | 24/40 [03:40<02:26,  9.15s/it] 62%|██████▎   | 25/40 [03:50<02:23,  9.58s/it] 65%|██████▌   | 26/40 [03:59<02:11,  9.43s/it] 68%|██████▊   | 27/40 [04:09<02:01,  9.37s/it] 70%|███████   | 28/40 [04:17<01:50,  9.22s/it] 72%|███████▎  | 29/40 [04:26<01:40,  9.18s/it] 75%|███████▌  | 30/40 [04:36<01:31,  9.15s/it] 78%|███████▊  | 31/40 [04:44<01:21,  9.05s/it] 80%|████████  | 32/40 [04:53<01:11,  8.97s/it] 82%|████████▎ | 33/40 [05:04<01:05,  9.41s/it] 85%|████████▌ | 34/40 [05:13<00:56,  9.47s/it] 88%|████████▊ | 35/40 [05:22<00:46,  9.36s/it] 90%|█████████ | 36/40 [05:32<00:37,  9.37s/it] 92%|█████████▎| 37/40 [05:41<00:27,  9.24s/it] 95%|█████████▌| 38/40 [05:50<00:18,  9.33s/it] 98%|█████████▊| 39/40 [06:00<00:09,  9.36s/it]100%|██████████| 40/40 [06:09<00:00,  9.29s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:09<00:00,  9.29s/it]100%|██████████| 40/40 [06:09<00:00,  9.23s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-1b_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 521
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 369.2562, 'train_samples_per_second': 7.055, 'train_steps_per_second': 0.108, 'train_loss': 0.6986259460449219, 'epoch': 4.98}
Dataset:  ionchannels_membraneproteins_balanced_train_4.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:29,  9.98s/it]  5%|▌         | 2/40 [00:20<06:35, 10.41s/it]  8%|▊         | 3/40 [00:30<06:13, 10.09s/it] 10%|█         | 4/40 [00:40<06:07, 10.22s/it] 12%|█▎        | 5/40 [00:51<05:58, 10.24s/it] 15%|█▌        | 6/40 [01:00<05:42, 10.09s/it] 18%|█▊        | 7/40 [01:10<05:31, 10.03s/it] 20%|██        | 8/40 [01:21<05:26, 10.21s/it] 22%|██▎       | 9/40 [01:33<05:30, 10.67s/it] 25%|██▌       | 10/40 [01:43<05:21, 10.72s/it] 28%|██▊       | 11/40 [01:54<05:08, 10.62s/it] 30%|███       | 12/40 [02:04<04:52, 10.44s/it] 32%|███▎      | 13/40 [02:14<04:43, 10.51s/it] 35%|███▌      | 14/40 [02:25<04:34, 10.57s/it] 38%|███▊      | 15/40 [02:36<04:26, 10.65s/it] 40%|████      | 16/40 [02:47<04:16, 10.71s/it] 42%|████▎     | 17/40 [02:59<04:13, 11.01s/it] 45%|████▌     | 18/40 [03:09<04:00, 10.94s/it] 48%|████▊     | 19/40 [03:20<03:44, 10.71s/it] 50%|█████     | 20/40 [03:30<03:31, 10.56s/it] 52%|█████▎    | 21/40 [03:40<03:19, 10.49s/it] 55%|█████▌    | 22/40 [03:50<03:07, 10.43s/it] 57%|█████▊    | 23/40 [04:01<02:56, 10.36s/it] 60%|██████    | 24/40 [04:11<02:47, 10.45s/it] 62%|██████▎   | 25/40 [04:23<02:43, 10.89s/it] 65%|██████▌   | 26/40 [04:33<02:29, 10.66s/it] 68%|██████▊   | 27/40 [04:44<02:17, 10.55s/it] 70%|███████   | 28/40 [04:53<02:04, 10.35s/it] 72%|███████▎  | 29/40 [05:04<01:53, 10.28s/it] 75%|███████▌  | 30/40 [05:14<01:43, 10.34s/it] 78%|███████▊  | 31/40 [05:24<01:32, 10.31s/it] 80%|████████  | 32/40 [05:35<01:22, 10.29s/it] 82%|████████▎ | 33/40 [05:47<01:15, 10.83s/it] 85%|████████▌ | 34/40 [05:58<01:05, 10.90s/it] 88%|████████▊ | 35/40 [06:08<00:54, 10.84s/it] 90%|█████████ | 36/40 [06:19<00:42, 10.66s/it] 92%|█████████▎| 37/40 [06:28<00:31, 10.40s/it] 95%|█████████▌| 38/40 [06:39<00:20, 10.40s/it] 98%|█████████▊| 39/40 [06:49<00:10, 10.32s/it]100%|██████████| 40/40 [06:59<00:00, 10.25s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:59<00:00, 10.25s/it]100%|██████████| 40/40 [06:59<00:00, 10.49s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_4/
Configuration saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_4/config.json
Model weights saved in ./finetuned_models/ESM-2_ionchannels_membraneproteins_balanced_train_4/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert/snapshots/3d05bf06e79014892defacad82e0efd06e977ff6/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 419.5895, 'train_samples_per_second': 6.208, 'train_steps_per_second': 0.095, 'train_loss': 0.7555611610412598, 'epoch': 4.98}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ProtBERT
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<04:06,  6.31s/it]  5%|▌         | 2/40 [00:12<04:01,  6.36s/it]  8%|▊         | 3/40 [00:18<03:52,  6.30s/it] 10%|█         | 4/40 [00:25<03:48,  6.35s/it] 12%|█▎        | 5/40 [00:31<03:41,  6.34s/it] 15%|█▌        | 6/40 [00:38<03:37,  6.39s/it] 18%|█▊        | 7/40 [00:44<03:28,  6.32s/it] 20%|██        | 8/40 [00:50<03:23,  6.36s/it] 22%|██▎       | 9/40 [01:01<04:00,  7.75s/it] 25%|██▌       | 10/40 [01:07<03:37,  7.24s/it] 28%|██▊       | 11/40 [01:13<03:20,  6.93s/it] 30%|███       | 12/40 [01:20<03:09,  6.76s/it] 32%|███▎      | 13/40 [01:26<02:56,  6.53s/it] 35%|███▌      | 14/40 [01:32<02:48,  6.49s/it] 38%|███▊      | 15/40 [01:38<02:40,  6.43s/it] 40%|████      | 16/40 [01:45<02:35,  6.48s/it] 42%|████▎     | 17/40 [01:56<03:01,  7.91s/it] 45%|████▌     | 18/40 [02:02<02:41,  7.35s/it] 48%|████▊     | 19/40 [02:09<02:27,  7.02s/it] 50%|█████     | 20/40 [02:15<02:16,  6.84s/it] 52%|█████▎    | 21/40 [02:21<02:04,  6.57s/it] 55%|█████▌    | 22/40 [02:27<01:57,  6.53s/it] 57%|█████▊    | 23/40 [02:34<01:49,  6.45s/it] 60%|██████    | 24/40 [02:40<01:41,  6.34s/it] 62%|██████▎   | 25/40 [02:51<01:58,  7.91s/it] 65%|██████▌   | 26/40 [02:58<01:44,  7.49s/it] 68%|██████▊   | 27/40 [03:05<01:34,  7.27s/it] 70%|███████   | 28/40 [03:11<01:24,  7.08s/it] 72%|███████▎  | 29/40 [03:18<01:16,  6.98s/it] 75%|███████▌  | 30/40 [03:25<01:08,  6.86s/it] 78%|███████▊  | 31/40 [03:31<01:00,  6.67s/it] 80%|████████  | 32/40 [03:37<00:53,  6.63s/it] 82%|████████▎ | 33/40 [03:48<00:54,  7.85s/it] 85%|████████▌ | 34/40 [03:54<00:44,  7.38s/it] 88%|████████▊ | 35/40 [04:01<00:35,  7.12s/it] 90%|█████████ | 36/40 [04:07<00:27,  6.98s/it] 92%|█████████▎| 37/40 [04:13<00:20,  6.70s/it] 95%|█████████▌| 38/40 [04:20<00:13,  6.55s/it] 98%|█████████▊| 39/40 [04:26<00:06,  6.57s/it]100%|██████████| 40/40 [04:33<00:00,  6.53s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:33<00:00,  6.53s/it]100%|██████████| 40/40 [04:33<00:00,  6.83s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading file vocab.txt from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json
Model config BertConfig {
  "_name_or_path": "Rostlab/prot_bert_bfd",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 40000,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 30,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.26.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/pytorch_model.bin
Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 419933186
{'train_runtime': 273.2737, 'train_samples_per_second': 10.264, 'train_steps_per_second': 0.146, 'train_loss': 0.7465215682983398, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ProtBERT-BFD
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:06<03:56,  6.06s/it]  5%|▌         | 2/40 [00:12<03:51,  6.08s/it]  8%|▊         | 3/40 [00:18<03:41,  6.00s/it] 10%|█         | 4/40 [00:24<03:37,  6.04s/it] 12%|█▎        | 5/40 [00:30<03:34,  6.13s/it] 15%|█▌        | 6/40 [00:37<03:33,  6.28s/it] 18%|█▊        | 7/40 [00:43<03:29,  6.35s/it] 20%|██        | 8/40 [00:49<03:23,  6.37s/it] 22%|██▎       | 9/40 [01:00<04:00,  7.76s/it] 25%|██▌       | 10/40 [01:06<03:37,  7.26s/it] 28%|██▊       | 11/40 [01:13<03:25,  7.07s/it] 30%|███       | 12/40 [01:19<03:12,  6.87s/it] 32%|███▎      | 13/40 [01:25<02:56,  6.53s/it] 35%|███▌      | 14/40 [01:31<02:45,  6.38s/it] 38%|███▊      | 15/40 [01:37<02:34,  6.18s/it] 40%|████      | 16/40 [01:43<02:29,  6.23s/it] 42%|████▎     | 17/40 [01:55<02:58,  7.77s/it] 45%|████▌     | 18/40 [02:01<02:40,  7.28s/it] 48%|████▊     | 19/40 [02:07<02:26,  6.98s/it] 50%|█████     | 20/40 [02:13<02:15,  6.79s/it] 52%|█████▎    | 21/40 [02:19<02:02,  6.46s/it] 55%|█████▌    | 22/40 [02:25<01:54,  6.38s/it] 57%|█████▊    | 23/40 [02:31<01:46,  6.24s/it] 60%|██████    | 24/40 [02:37<01:37,  6.11s/it] 62%|██████▎   | 25/40 [02:48<01:51,  7.43s/it] 65%|██████▌   | 26/40 [02:53<01:37,  6.95s/it] 68%|██████▊   | 27/40 [02:59<01:27,  6.70s/it] 70%|███████   | 28/40 [03:05<01:16,  6.42s/it] 72%|███████▎  | 29/40 [03:11<01:09,  6.33s/it] 75%|███████▌  | 30/40 [03:17<01:02,  6.21s/it] 78%|███████▊  | 31/40 [03:23<00:54,  6.08s/it] 80%|████████  | 32/40 [03:29<00:49,  6.15s/it] 82%|████████▎ | 33/40 [03:40<00:51,  7.37s/it] 85%|████████▌ | 34/40 [03:45<00:41,  6.92s/it] 88%|████████▊ | 35/40 [03:51<00:33,  6.64s/it] 90%|█████████ | 36/40 [03:57<00:25,  6.45s/it] 92%|█████████▎| 37/40 [04:03<00:18,  6.24s/it] 95%|█████████▌| 38/40 [04:09<00:12,  6.15s/it] 98%|█████████▊| 39/40 [04:16<00:06,  6.22s/it]100%|██████████| 40/40 [04:21<00:00,  6.10s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [04:21<00:00,  6.10s/it]100%|██████████| 40/40 [04:21<00:00,  6.55s/it]
Saving model checkpoint to ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ProtBERT-BFD_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm1b_t33_650M_UR50S",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": true,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm1b_t33_650M_UR50S/snapshots/7b37824baec4d3658e1df7479222a7c79b465b76/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm1b_t33_650M_UR50S were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652359063
{'train_runtime': 261.858, 'train_samples_per_second': 10.712, 'train_steps_per_second': 0.153, 'train_loss': 0.7505893707275391, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ESM-1b
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:08<05:28,  8.42s/it]  5%|▌         | 2/40 [00:16<05:19,  8.41s/it]  8%|▊         | 3/40 [00:24<05:06,  8.27s/it] 10%|█         | 4/40 [00:33<05:01,  8.37s/it] 12%|█▎        | 5/40 [00:41<04:54,  8.40s/it] 15%|█▌        | 6/40 [00:50<04:50,  8.56s/it] 18%|█▊        | 7/40 [00:59<04:39,  8.46s/it] 20%|██        | 8/40 [01:07<04:31,  8.47s/it] 22%|██▎       | 9/40 [01:21<05:19, 10.31s/it] 25%|██▌       | 10/40 [01:30<04:49,  9.65s/it] 28%|██▊       | 11/40 [01:38<04:28,  9.27s/it] 30%|███       | 12/40 [01:47<04:15,  9.13s/it] 32%|███▎      | 13/40 [01:55<03:58,  8.82s/it] 35%|███▌      | 14/40 [02:03<03:47,  8.74s/it] 38%|███▊      | 15/40 [02:11<03:33,  8.53s/it] 40%|████      | 16/40 [02:20<03:26,  8.62s/it] 42%|████▎     | 17/40 [02:36<04:06, 10.71s/it] 45%|████▌     | 18/40 [02:44<03:39,  9.97s/it] 48%|████▊     | 19/40 [02:53<03:20,  9.54s/it] 50%|█████     | 20/40 [03:01<03:06,  9.32s/it] 52%|█████▎    | 21/40 [03:09<02:48,  8.89s/it] 55%|█████▌    | 22/40 [03:18<02:39,  8.85s/it] 57%|█████▊    | 23/40 [03:26<02:27,  8.71s/it] 60%|██████    | 24/40 [03:35<02:16,  8.51s/it] 62%|██████▎   | 25/40 [03:49<02:34, 10.28s/it] 65%|██████▌   | 26/40 [03:57<02:15,  9.66s/it] 68%|██████▊   | 27/40 [04:06<02:01,  9.33s/it] 70%|███████   | 28/40 [04:14<01:47,  8.93s/it] 72%|███████▎  | 29/40 [04:23<01:39,  9.00s/it] 75%|███████▌  | 30/40 [04:32<01:29,  8.97s/it] 78%|███████▊  | 31/40 [04:40<01:19,  8.78s/it] 80%|████████  | 32/40 [04:50<01:11,  8.96s/it] 82%|████████▎ | 33/40 [05:04<01:14, 10.70s/it] 85%|████████▌ | 34/40 [05:13<00:59,  9.97s/it] 88%|████████▊ | 35/40 [05:21<00:47,  9.55s/it] 90%|█████████ | 36/40 [05:30<00:37,  9.25s/it] 92%|█████████▎| 37/40 [05:38<00:26,  8.94s/it] 95%|█████████▌| 38/40 [05:46<00:17,  8.81s/it] 98%|█████████▊| 39/40 [05:56<00:08,  8.92s/it]100%|██████████| 40/40 [06:04<00:00,  8.74s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:04<00:00,  8.74s/it]100%|██████████| 40/40 [06:04<00:00,  9.11s/it]
Saving model checkpoint to ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-1b_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
loading file vocab.txt from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/vocab.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/tokenizer_config.json
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading configuration file config.json from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/config.json
Model config EsmConfig {
  "_name_or_path": "facebook/esm2_t33_650M_UR50D",
  "architectures": [
    "EsmForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "classifier_dropout": null,
  "emb_layer_norm_before": false,
  "esmfold_config": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "is_folding_model": false,
  "layer_norm_eps": 1e-05,
  "mask_token_id": 32,
  "max_position_embeddings": 1026,
  "model_type": "esm",
  "num_attention_heads": 20,
  "num_hidden_layers": 33,
  "pad_token_id": 1,
  "position_embedding_type": "rotary",
  "token_dropout": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.1",
  "use_cache": true,
  "vocab_list": null,
  "vocab_size": 33
}

loading weights file pytorch_model.bin from cache at /home/h_ghazik/tmp/models--facebook--esm2_t33_650M_UR50D/snapshots/6123bfaf29ccd6aa250ac8861acc109ad5e7fb5d/pytorch_model.bin
Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using cuda_amp half precision backend
***** Running training *****
  Num examples = 561
  Num Epochs = 5
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 64
  Total optimization steps = 40
  Number of trainable parameters = 652356503
{'train_runtime': 364.3666, 'train_samples_per_second': 7.698, 'train_steps_per_second': 0.11, 'train_loss': 0.7581926345825195, 'epoch': 4.91}
Dataset:  iontransporters_membraneproteins_balanced_train_9.csv
Model:  ESM-2
--------------------------------------------------
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:09<06:12,  9.54s/it]  5%|▌         | 2/40 [00:19<06:06,  9.63s/it]  8%|▊         | 3/40 [00:28<05:48,  9.41s/it] 10%|█         | 4/40 [00:37<05:38,  9.40s/it] 12%|█▎        | 5/40 [00:47<05:28,  9.38s/it] 15%|█▌        | 6/40 [00:56<05:22,  9.49s/it] 18%|█▊        | 7/40 [01:06<05:10,  9.40s/it] 20%|██        | 8/40 [01:15<05:02,  9.44s/it] 22%|██▎       | 9/40 [01:31<05:58, 11.55s/it] 25%|██▌       | 10/40 [01:41<05:25, 10.85s/it] 28%|██▊       | 11/40 [01:50<05:02, 10.44s/it] 30%|███       | 12/40 [02:00<04:47, 10.27s/it] 32%|███▎      | 13/40 [02:09<04:28,  9.96s/it] 35%|███▌      | 14/40 [02:19<04:16,  9.86s/it] 38%|███▊      | 15/40 [02:28<04:01,  9.66s/it] 40%|████      | 16/40 [02:38<03:53,  9.73s/it] 42%|████▎     | 17/40 [02:55<04:36, 12.01s/it] 45%|████▌     | 18/40 [03:04<04:06, 11.19s/it] 48%|████▊     | 19/40 [03:14<03:44, 10.71s/it] 50%|█████     | 20/40 [03:24<03:28, 10.43s/it] 52%|█████▎    | 21/40 [03:33<03:09, 10.00s/it] 55%|█████▌    | 22/40 [03:43<02:59,  9.95s/it] 57%|█████▊    | 23/40 [03:52<02:46,  9.78s/it] 60%|██████    | 24/40 [04:01<02:33,  9.57s/it] 62%|██████▎   | 25/40 [04:17<02:52, 11.53s/it] 65%|██████▌   | 26/40 [04:26<02:31, 10.83s/it] 68%|██████▊   | 27/40 [04:36<02:16, 10.47s/it] 70%|███████   | 28/40 [04:45<02:00, 10.03s/it] 72%|███████▎  | 29/40 [04:55<01:49,  9.94s/it] 75%|███████▌  | 30/40 [05:04<01:37,  9.76s/it] 78%|███████▊  | 31/40 [05:13<01:25,  9.54s/it] 80%|████████  | 32/40 [05:23<01:17,  9.67s/it] 82%|████████▎ | 33/40 [05:39<01:21, 11.64s/it] 85%|████████▌ | 34/40 [05:49<01:05, 10.91s/it] 88%|████████▊ | 35/40 [05:58<00:52, 10.49s/it] 90%|█████████ | 36/40 [06:08<00:40, 10.23s/it] 92%|█████████▎| 37/40 [06:17<00:29,  9.95s/it] 95%|█████████▌| 38/40 [06:27<00:19,  9.81s/it] 98%|█████████▊| 39/40 [06:37<00:09,  9.91s/it]100%|██████████| 40/40 [06:46<00:00,  9.73s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 40/40 [06:46<00:00,  9.73s/it]100%|██████████| 40/40 [06:46<00:00, 10.16s/it]
Saving model checkpoint to ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_9/
Configuration saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_9/config.json
Model weights saved in ./finetuned_models/ESM-2_iontransporters_membraneproteins_balanced_train_9/pytorch_model.bin
{'train_runtime': 406.467, 'train_samples_per_second': 6.901, 'train_steps_per_second': 0.098, 'train_loss': 0.7830400466918945, 'epoch': 4.91}
